---
phase: 15-caf-core
plan: 03
type: execute
wave: 3
depends_on: ["15-02"]
files_modified:
  - src/jobforge/ingestion/caf.py
  - data/gold/dim_caf_occupation.parquet
  - data/gold/dim_caf_job_family.parquet
  - data/catalog/tables/dim_caf_occupation.json
  - data/catalog/tables/dim_caf_job_family.json
  - tests/ingestion/test_caf.py
autonomous: true

must_haves:
  truths:
    - "User can query all 107 CAF occupations from gold table"
    - "User can query CAF job families from gold table"
    - "Both tables have full provenance columns"
    - "Bilingual content queryable (title_en, title_fr columns)"
  artifacts:
    - path: "src/jobforge/ingestion/caf.py"
      provides: "Medallion pipeline for CAF gold tables"
      exports: ["ingest_dim_caf_occupation", "ingest_dim_caf_job_family"]
    - path: "data/gold/dim_caf_occupation.parquet"
      provides: "107 CAF occupations dimension table"
    - path: "data/gold/dim_caf_job_family.parquet"
      provides: "CAF job families dimension table"
    - path: "data/catalog/tables/dim_caf_occupation.json"
      provides: "Catalog metadata for CAF occupation table"
  key_links:
    - from: "src/jobforge/ingestion/caf.py"
      to: "data/caf/occupations.json"
      via: "Source data from scraper"
      pattern: "occupations\\.json"
    - from: "data/gold/dim_caf_occupation.parquet"
      to: "data/gold/dim_caf_job_family.parquet"
      via: "job_family_id foreign key"
      pattern: "job_family_id"
---

<objective>
Create gold tables for CAF occupations and job families.

Purpose: Transform scraped CAF data into queryable gold tables following JobForge's medallion pipeline pattern. This enables users to query CAF occupations with full provenance and bilingual content.

Output: dim_caf_occupation.parquet (~107 rows), dim_caf_job_family.parquet, catalog metadata files, ingestion module.
</objective>

<execution_context>
@C:\Users\Administrator\.claude/get-shit-done/workflows/execute-plan.md
@C:\Users\Administrator\.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/15-caf-core/15-CONTEXT.md
@.planning/phases/15-caf-core/15-RESEARCH.md

# Prior plan output
@.planning/phases/15-caf-core/15-02-SUMMARY.md

# Existing patterns
@src/jobforge/ingestion/noc.py
@src/jobforge/ingestion/og.py
@src/jobforge/pipeline/engine.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create dim_caf_occupation ingestion pipeline</name>
  <files>
    src/jobforge/ingestion/caf.py
    data/gold/dim_caf_occupation.parquet
  </files>
  <action>
Create `src/jobforge/ingestion/caf.py` following the OG ingestion pattern:

```python
"""CAF gold table ingestion pipelines.

Transforms scraped CAF data into gold tables following medallion pattern.
"""
import json
from pathlib import Path
from typing import Optional

import polars as pl
import structlog

from jobforge.pipeline.config import PipelineConfig
from jobforge.pipeline.engine import PipelineEngine

logger = structlog.get_logger(__name__)


def _load_caf_occupations_json(source_path: Path) -> pl.DataFrame:
    """Load CAF occupations from nested JSON structure.

    The JSON has structure: {occupations: [{...}, ...], occupation_count: N}
    """
    with open(source_path, 'r', encoding='utf-8') as f:
        data = json.load(f)

    occupations = data.get('occupations', [])
    if not occupations:
        raise ValueError(f"No occupations found in {source_path}")

    # Flatten nested structure to DataFrame
    rows = []
    for occ in occupations:
        row = {
            'occupation_id': occ.get('occupation_id'),
            'title_en': occ.get('title_en'),
            'title_fr': occ.get('title_fr'),
            'overview_en': occ.get('overview_en'),
            'overview_fr': occ.get('overview_fr'),
            'responsibilities_en': occ.get('responsibilities_en'),
            'responsibilities_fr': occ.get('responsibilities_fr'),
            'training_en': occ.get('training_en'),
            'training_fr': occ.get('training_fr'),
            'entry_plans_en': occ.get('entry_plans_en'),
            'entry_plans_fr': occ.get('entry_plans_fr'),
            'requirements_en': occ.get('requirements_en'),
            'requirements_fr': occ.get('requirements_fr'),
            'related_civilian_occupations': json.dumps(occ.get('related_civilian_occupations', [])),
            'military_branch': occ.get('military_branch'),
            'job_family_id': occ.get('job_family'),
            'source_url_en': occ.get('source_url_en'),
            'source_url_fr': occ.get('source_url_fr'),
            'scraped_at': occ.get('scraped_at'),
            'content_hash_en': occ.get('content_hash_en'),
            'content_hash_fr': occ.get('content_hash_fr'),
        }
        rows.append(row)

    return pl.DataFrame(rows)


def ingest_dim_caf_occupation(
    source_path: Path | None = None,
    config: Optional[PipelineConfig] = None,
    table_name: str = "dim_caf_occupation",
) -> dict:
    """Ingest CAF occupations JSON to gold layer as DIM CAF Occupation.

    Transforms applied:
    - Bronze: Load JSON, rename columns for provenance tracking
    - Silver: Validate required fields, normalize IDs
    - Gold: Final column selection with provenance

    Args:
        source_path: Path to occupations.json. Defaults to data/caf/occupations.json.
        config: Pipeline configuration.
        table_name: Output table name.

    Returns:
        Dict with gold_path, batch_id, and row counts.
    """
    if source_path is None:
        source_path = Path("data/caf/occupations.json")

    engine = PipelineEngine(config=config)

    # Load from JSON (custom loader for nested structure)
    df = _load_caf_occupations_json(source_path)

    # Bronze: Add provenance columns
    def add_bronze_provenance(df: pl.LazyFrame) -> pl.LazyFrame:
        """Add bronze layer provenance columns."""
        return df.with_columns([
            pl.lit(str(source_path)).alias("_source_file"),
            pl.lit("caf").alias("_domain"),
        ])

    # Silver: Validate and normalize
    def validate_required_fields(df: pl.LazyFrame) -> pl.LazyFrame:
        """Ensure required fields are not null."""
        return df.filter(
            pl.col("occupation_id").is_not_null() &
            pl.col("title_en").is_not_null()
        )

    def normalize_occupation_ids(df: pl.LazyFrame) -> pl.LazyFrame:
        """Normalize occupation IDs (lowercase, trim)."""
        return df.with_columns([
            pl.col("occupation_id").str.to_lowercase().str.strip_chars().alias("occupation_id"),
        ])

    # Gold: Select final columns
    def select_gold_columns(df: pl.LazyFrame) -> pl.LazyFrame:
        """Select final schema for gold table."""
        return df.select([
            # Primary key
            "occupation_id",
            # Bilingual content
            "title_en", "title_fr",
            "overview_en", "overview_fr",
            "responsibilities_en", "responsibilities_fr",
            "training_en", "training_fr",
            "entry_plans_en", "entry_plans_fr",
            "requirements_en", "requirements_fr",
            # Relationships
            "related_civilian_occupations",  # JSON array as string
            "military_branch",
            "job_family_id",  # FK to dim_caf_job_family
            # Provenance
            "source_url_en", "source_url_fr",
            "scraped_at",
            "content_hash_en", "content_hash_fr",
            "_source_file",
        ])

    # Run transforms manually since source is JSON not CSV
    lazy_df = df.lazy()
    lazy_df = add_bronze_provenance(lazy_df)
    lazy_df = validate_required_fields(lazy_df)
    lazy_df = normalize_occupation_ids(lazy_df)
    lazy_df = select_gold_columns(lazy_df)

    # Write to gold
    gold_df = lazy_df.collect()
    gold_path = Path(config.gold_dir if config else "data/gold") / f"{table_name}.parquet"
    gold_path.parent.mkdir(parents=True, exist_ok=True)
    gold_df.write_parquet(gold_path)

    logger.info(
        "ingested_dim_caf_occupation",
        table_name=table_name,
        row_count=len(gold_df),
        gold_path=str(gold_path),
    )

    return {
        "gold_path": gold_path,
        "row_count": len(gold_df),
        "table_name": table_name,
    }


def _load_caf_job_families_json(source_path: Path) -> pl.DataFrame:
    """Load CAF job families from JSON."""
    with open(source_path, 'r', encoding='utf-8') as f:
        data = json.load(f)

    families = data.get('families', [])
    rows = []
    for fam in families:
        row = {
            'job_family_id': fam.get('job_family_id'),
            'name_en': fam.get('name_en'),
            'name_fr': fam.get('name_fr'),
            'description_en': fam.get('description_en'),
            'description_fr': fam.get('description_fr'),
            'career_count': fam.get('career_count', 0),
            'source_url': fam.get('source_url'),
            'scraped_at': fam.get('scraped_at'),
        }
        rows.append(row)

    return pl.DataFrame(rows)


def ingest_dim_caf_job_family(
    source_path: Path | None = None,
    config: Optional[PipelineConfig] = None,
    table_name: str = "dim_caf_job_family",
) -> dict:
    """Ingest CAF job families JSON to gold layer.

    Args:
        source_path: Path to job_families.json.
        config: Pipeline configuration.
        table_name: Output table name.

    Returns:
        Dict with gold_path, batch_id, and row counts.
    """
    if source_path is None:
        source_path = Path("data/caf/job_families.json")

    df = _load_caf_job_families_json(source_path)

    # Add provenance
    df = df.with_columns([
        pl.lit(str(source_path)).alias("_source_file"),
    ])

    # Write to gold
    gold_path = Path(config.gold_dir if config else "data/gold") / f"{table_name}.parquet"
    gold_path.parent.mkdir(parents=True, exist_ok=True)
    df.write_parquet(gold_path)

    logger.info(
        "ingested_dim_caf_job_family",
        table_name=table_name,
        row_count=len(df),
        gold_path=str(gold_path),
    )

    return {
        "gold_path": gold_path,
        "row_count": len(df),
        "table_name": table_name,
    }


if __name__ == "__main__":
    result = ingest_dim_caf_occupation()
    print(f"Occupations: {result['row_count']} rows -> {result['gold_path']}")

    result = ingest_dim_caf_job_family()
    print(f"Job Families: {result['row_count']} rows -> {result['gold_path']}")
```
  </action>
  <verify>
    `python -c "from jobforge.ingestion.caf import ingest_dim_caf_occupation; r = ingest_dim_caf_occupation(); print(f'Rows: {r[\"row_count\"]}')"` - shows ~107
    `python -c "import polars as pl; print(pl.read_parquet('data/gold/dim_caf_occupation.parquet').columns)"` - shows expected columns
  </verify>
  <done>ingest_dim_caf_occupation creates dim_caf_occupation.parquet with ~107 CAF occupations and bilingual content</done>
</task>

<task type="auto">
  <name>Task 2: Create catalog metadata files</name>
  <files>
    data/catalog/tables/dim_caf_occupation.json
    data/catalog/tables/dim_caf_job_family.json
  </files>
  <action>
Create catalog metadata files following existing patterns:

1. **Create `data/catalog/tables/dim_caf_occupation.json`**:
```json
{
  "table_name": "dim_caf_occupation",
  "domain": "caf",
  "description": "Canadian Armed Forces occupations - 107 military careers with bilingual content and provenance",
  "source": "Canadian Armed Forces",
  "source_url": "https://forces.ca/en/careers/",
  "columns": {
    "occupation_id": {"type": "VARCHAR", "description": "Unique occupation identifier (URL slug)", "primary_key": true},
    "title_en": {"type": "VARCHAR", "description": "Career title in English"},
    "title_fr": {"type": "VARCHAR", "description": "Career title in French"},
    "overview_en": {"type": "VARCHAR", "description": "Career overview/description in English"},
    "overview_fr": {"type": "VARCHAR", "description": "Career overview/description in French"},
    "responsibilities_en": {"type": "VARCHAR", "description": "Job responsibilities in English"},
    "responsibilities_fr": {"type": "VARCHAR", "description": "Job responsibilities in French"},
    "training_en": {"type": "VARCHAR", "description": "Training requirements in English"},
    "training_fr": {"type": "VARCHAR", "description": "Training requirements in French"},
    "entry_plans_en": {"type": "VARCHAR", "description": "Entry plans/paths in English"},
    "entry_plans_fr": {"type": "VARCHAR", "description": "Entry plans/paths in French"},
    "requirements_en": {"type": "VARCHAR", "description": "Qualification requirements in English"},
    "requirements_fr": {"type": "VARCHAR", "description": "Qualification requirements in French"},
    "related_civilian_occupations": {"type": "VARCHAR", "description": "JSON array of related civilian occupation titles"},
    "military_branch": {"type": "VARCHAR", "description": "Military branch: Army, Navy, Air Force, or Common"},
    "job_family_id": {"type": "VARCHAR", "description": "CAF job family identifier", "foreign_key": "dim_caf_job_family.job_family_id"},
    "source_url_en": {"type": "VARCHAR", "description": "Source URL for English page (provenance)"},
    "source_url_fr": {"type": "VARCHAR", "description": "Source URL for French page (provenance)"},
    "scraped_at": {"type": "TIMESTAMP", "description": "UTC timestamp when data was scraped"},
    "content_hash_en": {"type": "VARCHAR", "description": "SHA-256 hash of English page HTML"},
    "content_hash_fr": {"type": "VARCHAR", "description": "SHA-256 hash of French page HTML"}
  },
  "relationships": [
    {"table": "dim_caf_job_family", "join": "dim_caf_occupation.job_family_id = dim_caf_job_family.job_family_id", "type": "many-to-one"},
    {"table": "bridge_caf_noc", "join": "dim_caf_occupation.occupation_id = bridge_caf_noc.caf_occupation_id", "type": "one-to-many"},
    {"table": "bridge_caf_ja", "join": "dim_caf_occupation.occupation_id = bridge_caf_ja.caf_occupation_id", "type": "one-to-many"}
  ]
}
```

2. **Create `data/catalog/tables/dim_caf_job_family.json`**:
```json
{
  "table_name": "dim_caf_job_family",
  "domain": "caf",
  "description": "Canadian Armed Forces job families - career groupings inferred from military branch",
  "source": "Canadian Armed Forces",
  "source_url": "https://forces.ca/en/careers/",
  "columns": {
    "job_family_id": {"type": "VARCHAR", "description": "Unique job family identifier (e.g., CAF-FAM-01)", "primary_key": true},
    "name_en": {"type": "VARCHAR", "description": "Job family name in English"},
    "name_fr": {"type": "VARCHAR", "description": "Job family name in French"},
    "description_en": {"type": "VARCHAR", "description": "Job family description in English"},
    "description_fr": {"type": "VARCHAR", "description": "Job family description in French"},
    "career_count": {"type": "INTEGER", "description": "Number of careers in this family"},
    "source_url": {"type": "VARCHAR", "description": "Source URL (provenance)"},
    "scraped_at": {"type": "TIMESTAMP", "description": "UTC timestamp when data was generated"}
  },
  "relationships": [
    {"table": "dim_caf_occupation", "join": "dim_caf_job_family.job_family_id = dim_caf_occupation.job_family_id", "type": "one-to-many"}
  ]
}
```
  </action>
  <verify>
    `cat data/catalog/tables/dim_caf_occupation.json | python -m json.tool > /dev/null && echo "Valid JSON"`
    `cat data/catalog/tables/dim_caf_job_family.json | python -m json.tool > /dev/null && echo "Valid JSON"`
    `ls -la data/catalog/tables/dim_caf*.json` - both files exist
  </verify>
  <done>Catalog metadata files created for dim_caf_occupation and dim_caf_job_family with relationships defined</done>
</task>

<task type="auto">
  <name>Task 3: Create tests for CAF ingestion</name>
  <files>
    tests/ingestion/test_caf.py
  </files>
  <action>
Create `tests/ingestion/test_caf.py`:

```python
"""Tests for CAF gold table ingestion."""
from pathlib import Path

import polars as pl
import pytest

from jobforge.ingestion.caf import (
    ingest_dim_caf_occupation,
    ingest_dim_caf_job_family,
    _load_caf_occupations_json,
    _load_caf_job_families_json,
)


class TestCAFOccupationIngestion:
    """Tests for dim_caf_occupation ingestion."""

    @pytest.fixture
    def occupation_table(self):
        """Load or create occupation table."""
        gold_path = Path("data/gold/dim_caf_occupation.parquet")
        if not gold_path.exists():
            result = ingest_dim_caf_occupation()
            return pl.read_parquet(result["gold_path"])
        return pl.read_parquet(gold_path)

    def test_occupation_count(self, occupation_table):
        """Test expected number of occupations."""
        assert len(occupation_table) >= 50  # Expect ~107

    def test_occupation_has_bilingual_titles(self, occupation_table):
        """Test occupations have both EN and FR titles."""
        assert "title_en" in occupation_table.columns
        assert "title_fr" in occupation_table.columns

        # Check most have both (some might be missing FR)
        en_count = occupation_table.filter(pl.col("title_en").is_not_null()).height
        assert en_count == len(occupation_table)  # All should have EN

    def test_occupation_has_provenance(self, occupation_table):
        """Test occupations have provenance columns."""
        required_provenance = [
            "source_url_en",
            "source_url_fr",
            "scraped_at",
            "content_hash_en",
            "content_hash_fr",
            "_source_file",
        ]
        for col in required_provenance:
            assert col in occupation_table.columns, f"Missing provenance column: {col}"

    def test_occupation_ids_unique(self, occupation_table):
        """Test occupation IDs are unique."""
        ids = occupation_table["occupation_id"].to_list()
        assert len(ids) == len(set(ids))

    def test_no_duplicate_rows_per_language(self, occupation_table):
        """Test bilingual content in same row (per CONTEXT.md)."""
        # Should have same count of EN and FR columns populated
        # Not separate rows for each language
        assert len(occupation_table) < 200  # If 107 careers, should not be 214 rows


class TestCAFJobFamilyIngestion:
    """Tests for dim_caf_job_family ingestion."""

    @pytest.fixture
    def family_table(self):
        """Load or create job family table."""
        gold_path = Path("data/gold/dim_caf_job_family.parquet")
        if not gold_path.exists():
            result = ingest_dim_caf_job_family()
            return pl.read_parquet(result["gold_path"])
        return pl.read_parquet(gold_path)

    def test_family_count(self, family_table):
        """Test job families exist."""
        assert len(family_table) >= 2  # At least Army, Navy, Air Force

    def test_family_has_name(self, family_table):
        """Test families have names."""
        assert "name_en" in family_table.columns
        names = family_table["name_en"].to_list()
        assert all(n is not None for n in names)

    def test_family_has_career_count(self, family_table):
        """Test families track career counts."""
        assert "career_count" in family_table.columns


class TestCAFForeignKeys:
    """Tests for FK relationships between CAF tables."""

    @pytest.fixture
    def tables(self):
        """Load both tables."""
        occ_path = Path("data/gold/dim_caf_occupation.parquet")
        fam_path = Path("data/gold/dim_caf_job_family.parquet")

        if not occ_path.exists():
            ingest_dim_caf_occupation()
        if not fam_path.exists():
            ingest_dim_caf_job_family()

        return {
            "occupation": pl.read_parquet(occ_path),
            "job_family": pl.read_parquet(fam_path),
        }

    def test_job_family_fk_valid(self, tables):
        """Test occupation job_family_id references valid families."""
        occ_families = set(
            tables["occupation"]
            .filter(pl.col("job_family_id").is_not_null())
            ["job_family_id"].to_list()
        )
        valid_families = set(tables["job_family"]["job_family_id"].to_list())

        invalid = occ_families - valid_families
        assert len(invalid) == 0, f"Invalid job_family_ids: {invalid}"
```
  </action>
  <verify>
    `pytest tests/ingestion/test_caf.py -v` - all tests pass
    `python -c "import polars as pl; print(pl.read_parquet('data/gold/dim_caf_occupation.parquet').shape)"` - shows row count
  </verify>
  <done>CAF ingestion tests pass; FK relationships validated; bilingual storage confirmed</done>
</task>

</tasks>

<verification>
1. `python -c "from jobforge.ingestion.caf import ingest_dim_caf_occupation, ingest_dim_caf_job_family"` - imports work
2. `pytest tests/ingestion/test_caf.py -v` - all tests pass
3. `ls -la data/gold/dim_caf*.parquet` - both files exist
4. `python -c "import polars as pl; df=pl.read_parquet('data/gold/dim_caf_occupation.parquet'); print(f'Rows: {len(df)}, Cols: {len(df.columns)}')"` - ~107 rows
5. `cat data/catalog/tables/dim_caf_occupation.json | python -m json.tool > /dev/null` - valid JSON
</verification>

<success_criteria>
- ingest_dim_caf_occupation creates gold table with ~107 occupations
- ingest_dim_caf_job_family creates gold table with job families
- Both tables have bilingual content (EN/FR columns)
- Both tables have provenance columns (source URLs, scraped_at, content hashes)
- FK relationship valid (occupation.job_family_id -> job_family.job_family_id)
- Catalog metadata files created for both tables
- Tests pass
</success_criteria>

<output>
After completion, create `.planning/phases/15-caf-core/15-03-SUMMARY.md`
</output>
