---
phase: 15-caf-core
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/jobforge/external/caf/__init__.py
  - src/jobforge/external/caf/models.py
  - src/jobforge/external/caf/scraper.py
  - src/jobforge/external/caf/parser.py
  - data/caf/careers_en.json
  - data/caf/careers_fr.json
  - tests/external/test_caf_scraper.py
autonomous: true

must_haves:
  truths:
    - "CAF careers page can be scraped for both EN and FR"
    - "Scraped data includes career URLs for individual page fetching"
    - "All scraped data carries full provenance metadata"
  artifacts:
    - path: "src/jobforge/external/caf/models.py"
      provides: "Pydantic models for CAF data with provenance"
      exports: ["CAFCareerListing", "CAFScrapedPage", "CAFProvenance"]
    - path: "src/jobforge/external/caf/scraper.py"
      provides: "Main scraper class following TBS pattern"
      exports: ["CAFScraper"]
    - path: "data/caf/careers_en.json"
      provides: "107 scraped career listings (EN)"
    - path: "data/caf/careers_fr.json"
      provides: "107 scraped career listings (FR)"
  key_links:
    - from: "src/jobforge/external/caf/scraper.py"
      to: "src/jobforge/external/caf/models.py"
      via: "Pydantic model instantiation"
      pattern: "CAFCareerListing|CAFScrapedPage"
    - from: "src/jobforge/external/caf/scraper.py"
      to: "src/jobforge/external/caf/parser.py"
      via: "HTML parsing functions"
      pattern: "parse_careers_listing"
---

<objective>
Create CAF scraper module with Pydantic models and listing page scraper.

Purpose: Establish the foundation for CAF data extraction by scraping the forces.ca careers listing pages to get all career URLs. This enables Plan 02 to fetch individual career details.

Output: CAF external module with models, scraper class, parser functions, and initial scraped career listings for both languages.
</objective>

<execution_context>
@C:\Users\Administrator\.claude/get-shit-done/workflows/execute-plan.md
@C:\Users\Administrator\.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/15-caf-core/15-CONTEXT.md
@.planning/phases/15-caf-core/15-RESEARCH.md

# Existing patterns to follow
@src/jobforge/external/tbs/models.py
@src/jobforge/external/tbs/scraper.py
@src/jobforge/external/tbs/parser.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create CAF Pydantic models with provenance</name>
  <files>
    src/jobforge/external/caf/__init__.py
    src/jobforge/external/caf/models.py
  </files>
  <action>
Create `src/jobforge/external/caf/` package:

1. **Create `__init__.py`** with exports:
   ```python
   from .models import CAFCareerListing, CAFScrapedPage, CAFProvenance, CAFOccupation, CAFJobFamily
   from .scraper import CAFScraper
   ```

2. **Create `models.py`** following TBS models pattern:

   ```python
   """Pydantic models for CAF scraped data with full provenance.

   Per CONTEXT.md: Every scraped value carries full provenance metadata.
   """
   from datetime import datetime
   from typing import Literal
   from pydantic import BaseModel, Field
   import hashlib

   class CAFProvenance(BaseModel):
       """Provenance for scraped CAF data."""
       source_url: str = Field(description="URL from which data was scraped")
       scraped_at: datetime = Field(description="UTC timestamp when scraping occurred")
       extraction_method: str = Field(description="How value extracted: 'css_selector', 'link_href', 'text_content'")
       page_title: str = Field(default="", description="Title of source page")
       content_hash: str = Field(description="SHA-256 of raw HTML for integrity")
       scraper_version: str = Field(default="CAFScraper-1.0.0", description="Scraper version")

   class CAFCareerListing(BaseModel):
       """Single career from forces.ca listing page."""
       career_slug: str = Field(description="URL slug e.g., 'pilot', 'infantry-soldier'")
       title_en: str | None = Field(default=None, description="Career title in English")
       title_fr: str | None = Field(default=None, description="Career title in French")
       career_url_en: str | None = Field(default=None, description="Full URL to EN career page")
       career_url_fr: str | None = Field(default=None, description="Full URL to FR career page")
       military_branch: str | None = Field(default=None, description="Army, Navy, Air Force, or Common")
       provenance: CAFProvenance

   class CAFScrapedPage(BaseModel):
       """Container for complete listing page scrape result."""
       url: str = Field(description="URL that was scraped")
       language: str = Field(description="Language code: 'en' or 'fr'")
       title: str = Field(description="Page title")
       scraped_at: datetime
       careers: list[CAFCareerListing] = Field(description="Extracted career listings")
       career_count: int = Field(description="Number of careers found")
       content_hash: str = Field(description="SHA-256 of page HTML")

   class CAFOccupation(BaseModel):
       """Complete CAF occupation with bilingual content and provenance.

       Per CONTEXT.md decision: Use separate columns for EN/FR, NOT separate rows.
       """
       occupation_id: str = Field(description="Canonical ID from URL slug")
       title_en: str = Field(description="Career title in English")
       title_fr: str = Field(description="Career title in French")
       overview_en: str | None = Field(default=None, description="Career overview (EN)")
       overview_fr: str | None = Field(default=None, description="Career overview (FR)")
       responsibilities_en: str | None = Field(default=None)
       responsibilities_fr: str | None = Field(default=None)
       training_en: str | None = Field(default=None)
       training_fr: str | None = Field(default=None)
       entry_plans_en: str | None = Field(default=None)
       entry_plans_fr: str | None = Field(default=None)
       requirements_en: str | None = Field(default=None)
       requirements_fr: str | None = Field(default=None)
       related_civilian_occupations: list[str] = Field(default_factory=list, description="Civilian equivalent occupations mentioned")
       related_careers: list[str] = Field(default_factory=list, description="Related CAF careers (slugs)")
       military_branch: str | None = Field(default=None, description="Army/Navy/Air Force/Common")
       job_family: str | None = Field(default=None, description="CAF job family if identifiable")

       # Provenance
       source_url_en: str
       source_url_fr: str
       scraped_at: datetime
       content_hash_en: str
       content_hash_fr: str
       scraper_version: str = "CAFScraper-1.0.0"

   class CAFJobFamily(BaseModel):
       """CAF job family (inferred from career groupings)."""
       job_family_id: str = Field(description="Job family identifier")
       name_en: str
       name_fr: str | None = None
       description_en: str | None = None
       description_fr: str | None = None
       career_count: int = Field(default=0, description="Number of careers in this family")
       source_url: str | None = None
       scraped_at: datetime | None = None
   ```

Follow the TBS models.py pattern exactly for provenance tracking.
  </action>
  <verify>
    `python -c "from jobforge.external.caf.models import CAFCareerListing, CAFOccupation, CAFProvenance; print('Models imported')"`
    `python -c "from jobforge.external.caf import CAFScraper; print('Package exports work')"` (will fail until scraper created)
  </verify>
  <done>CAF models.py created with CAFCareerListing, CAFOccupation, CAFJobFamily, CAFProvenance models following TBS pattern</done>
</task>

<task type="auto">
  <name>Task 2: Create CAF scraper and parser for listing pages</name>
  <files>
    src/jobforge/external/caf/scraper.py
    src/jobforge/external/caf/parser.py
  </files>
  <action>
1. **Create `parser.py`** for HTML parsing:

   ```python
   """HTML parsing functions for forces.ca pages."""
   from datetime import datetime
   from bs4 import BeautifulSoup
   import hashlib
   from .models import CAFCareerListing, CAFProvenance

   def compute_content_hash(html: str) -> str:
       """Compute SHA-256 hash of HTML content."""
       return hashlib.sha256(html.encode('utf-8')).hexdigest()

   def parse_careers_listing(html: str, url: str, scraped_at: datetime, language: str) -> list[CAFCareerListing]:
       """Parse forces.ca careers listing page.

       Forces.ca structure: Career cards/links at /en/career/{slug}/ pattern.
       Extract all career URLs from the listing page.
       """
       content_hash = compute_content_hash(html)
       soup = BeautifulSoup(html, 'lxml')
       careers = []

       # Look for career links - forces.ca uses /en/career/{slug}/ or /fr/carriere/{slug}/
       career_pattern = f"/{language}/career/" if language == "en" else f"/{language}/carriere/"

       for link in soup.find_all('a', href=True):
           href = link['href']
           if career_pattern in href:
               # Extract slug from URL
               parts = href.rstrip('/').split('/')
               slug = parts[-1] if parts else None

               if slug and slug not in [c.career_slug for c in careers]:
                   title = link.get_text(strip=True) or slug.replace('-', ' ').title()

                   provenance = CAFProvenance(
                       source_url=url,
                       scraped_at=scraped_at,
                       extraction_method="link_href",
                       page_title=soup.title.string if soup.title else "",
                       content_hash=content_hash,
                   )

                   career = CAFCareerListing(
                       career_slug=slug,
                       title_en=title if language == "en" else None,
                       title_fr=title if language == "fr" else None,
                       career_url_en=href if language == "en" else None,
                       career_url_fr=href if language == "fr" else None,
                       provenance=provenance,
                   )
                   careers.append(career)

       return careers
   ```

2. **Create `scraper.py`** following TBS pattern:

   ```python
   """CAF Careers scraper with provenance tracking.

   Scrapes forces.ca career listings following the TBS scraper pattern.
   Per CONTEXT.md: Rate limit 1.5s between requests, capture ALL fields.
   """
   import json
   import time
   from datetime import datetime, timezone
   from pathlib import Path

   import httpx
   import structlog
   from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type

   from .models import CAFCareerListing, CAFScrapedPage
   from .parser import parse_careers_listing, compute_content_hash

   logger = structlog.get_logger(__name__)

   # forces.ca career listing URLs
   CAF_URLS = {
       "en": "https://forces.ca/en/careers/",
       "fr": "https://forces.ca/fr/carrieres/",
   }

   REQUEST_DELAY_SECONDS = 1.5  # Polite scraping per RESEARCH.md

   class CAFScraper:
       """CAF Careers scraper with provenance tracking.

       Follows TBS scraper pattern with bilingual support.
       """

       def __init__(self, output_dir: str | Path = "data/caf"):
           self.output_dir = Path(output_dir)
           self.output_dir.mkdir(parents=True, exist_ok=True)
           self.client = httpx.Client(
               timeout=30.0,
               follow_redirects=True,
               limits=httpx.Limits(max_connections=5),
           )

       @retry(
           stop=stop_after_attempt(3),
           wait=wait_exponential(multiplier=1.5, min=2, max=10),
           retry=retry_if_exception_type((httpx.HTTPStatusError, httpx.TimeoutException)),
       )
       def _fetch(self, url: str) -> str:
           """Fetch URL with retry and exponential backoff."""
           response = self.client.get(url)
           response.raise_for_status()
           return response.text

       def scrape_careers_listing(self, language: str = "en") -> CAFScrapedPage:
           """Scrape careers listing page for given language."""
           url = CAF_URLS.get(language)
           if not url:
               raise ValueError(f"Unsupported language: {language}")

           logger.info("scraping_caf_careers_listing", url=url, language=language)

           html = self._fetch(url)
           scraped_at = datetime.now(timezone.utc)
           content_hash = compute_content_hash(html)

           careers = parse_careers_listing(html, url, scraped_at, language)

           logger.info(
               "scraped_caf_careers_listing_success",
               language=language,
               career_count=len(careers),
           )

           time.sleep(REQUEST_DELAY_SECONDS)  # Rate limit

           return CAFScrapedPage(
               url=url,
               language=language,
               title=f"CAF Careers ({language.upper()})",
               scraped_at=scraped_at,
               careers=careers,
               career_count=len(careers),
               content_hash=content_hash,
           )

       def scrape_both_languages(self) -> dict[str, CAFScrapedPage]:
           """Scrape both EN and FR listing pages."""
           return {
               "en": self.scrape_careers_listing("en"),
               "fr": self.scrape_careers_listing("fr"),
           }

       def save_to_json(self, page: CAFScrapedPage) -> Path:
           """Save scraped page to JSON with provenance."""
           filename = f"careers_{page.language}.json"
           filepath = self.output_dir / filename

           data = page.model_dump(mode="json")

           with open(filepath, "w", encoding="utf-8") as f:
               json.dump(data, f, indent=2, ensure_ascii=False)

           logger.info("saved_caf_careers", filepath=str(filepath), count=page.career_count)
           return filepath

       def scrape_and_save(self) -> dict[str, Path]:
           """Scrape both languages and save to JSON files."""
           pages = self.scrape_both_languages()
           return {lang: self.save_to_json(page) for lang, page in pages.items()}


   def scrape_caf_careers(output_dir: str | Path = "data/caf") -> dict[str, Path]:
       """Convenience function to scrape CAF careers listing."""
       scraper = CAFScraper(output_dir)
       return scraper.scrape_and_save()
   ```

3. **Update `__init__.py`** to export scraper:
   ```python
   from .scraper import CAFScraper, scrape_caf_careers
   ```
  </action>
  <verify>
    `python -c "from jobforge.external.caf import CAFScraper; s = CAFScraper(); print('Scraper instantiated')"`
    `python -c "from jobforge.external.caf import scrape_caf_careers; print('Function exported')"`
  </verify>
  <done>CAF scraper and parser created following TBS pattern with rate limiting and retry logic</done>
</task>

<task type="auto">
  <name>Task 3: Run initial scrape and create tests</name>
  <files>
    data/caf/careers_en.json
    data/caf/careers_fr.json
    tests/external/test_caf_scraper.py
  </files>
  <action>
1. **Run the scraper** to produce initial data files:
   ```python
   from jobforge.external.caf import scrape_caf_careers
   paths = scrape_caf_careers()
   print(f"EN: {paths['en']}, FR: {paths['fr']}")
   ```

2. **Create tests** in `tests/external/test_caf_scraper.py`:
   ```python
   """Tests for CAF careers scraper."""
   import json
   from pathlib import Path
   from datetime import datetime

   import pytest

   from jobforge.external.caf.models import CAFCareerListing, CAFScrapedPage, CAFProvenance
   from jobforge.external.caf.parser import parse_careers_listing, compute_content_hash
   from jobforge.external.caf import CAFScraper


   class TestCAFModels:
       """Test CAF Pydantic models."""

       def test_caf_provenance_creation(self):
           """Test CAFProvenance model instantiation."""
           prov = CAFProvenance(
               source_url="https://forces.ca/en/careers/",
               scraped_at=datetime.now(),
               extraction_method="link_href",
               content_hash="abc123",
           )
           assert prov.source_url == "https://forces.ca/en/careers/"
           assert prov.scraper_version == "CAFScraper-1.0.0"

       def test_caf_career_listing_creation(self):
           """Test CAFCareerListing model."""
           prov = CAFProvenance(
               source_url="https://forces.ca/en/careers/",
               scraped_at=datetime.now(),
               extraction_method="link_href",
               content_hash="abc123",
           )
           career = CAFCareerListing(
               career_slug="pilot",
               title_en="Pilot",
               career_url_en="https://forces.ca/en/career/pilot/",
               provenance=prov,
           )
           assert career.career_slug == "pilot"
           assert career.title_en == "Pilot"


   class TestCAFParser:
       """Test HTML parsing functions."""

       def test_compute_content_hash(self):
           """Test SHA-256 hash computation."""
           html = "<html><body>Test</body></html>"
           hash1 = compute_content_hash(html)
           hash2 = compute_content_hash(html)
           assert hash1 == hash2
           assert len(hash1) == 64  # SHA-256 hex length

       def test_parse_careers_listing_extracts_careers(self):
           """Test career extraction from sample HTML."""
           sample_html = '''
           <html><head><title>Careers</title></head><body>
           <a href="/en/career/pilot/">Pilot</a>
           <a href="/en/career/infantry-soldier/">Infantry Soldier</a>
           <a href="/en/about/">About</a>
           </body></html>
           '''
           careers = parse_careers_listing(
               sample_html,
               "https://forces.ca/en/careers/",
               datetime.now(),
               "en"
           )
           assert len(careers) == 2
           slugs = [c.career_slug for c in careers]
           assert "pilot" in slugs
           assert "infantry-soldier" in slugs


   class TestCAFScraper:
       """Test CAF scraper class."""

       def test_scraper_instantiation(self):
           """Test scraper can be instantiated."""
           scraper = CAFScraper()
           assert scraper.output_dir.exists() or True  # May not exist yet

       def test_scraper_urls_defined(self):
           """Test scraper has URL definitions."""
           from jobforge.external.caf.scraper import CAF_URLS
           assert "en" in CAF_URLS
           assert "fr" in CAF_URLS
           assert "forces.ca" in CAF_URLS["en"]


   @pytest.mark.integration
   class TestCAFScraperIntegration:
       """Integration tests requiring network access."""

       def test_scrape_en_careers(self):
           """Test actual scrape of EN careers page."""
           scraper = CAFScraper()
           page = scraper.scrape_careers_listing("en")
           assert page.career_count > 50  # Expect ~107
           assert page.language == "en"
           assert page.content_hash  # Has hash
   ```

3. **Verify scraped data files exist** and contain expected content:
   - careers_en.json should have 100+ careers
   - careers_fr.json should have 100+ careers (same count)
   - Each career should have provenance (source_url, scraped_at, content_hash)
  </action>
  <verify>
    `pytest tests/external/test_caf_scraper.py -v -k "not integration"` - unit tests pass
    `ls -la data/caf/careers_*.json` - both files exist
    `python -c "import json; d=json.load(open('data/caf/careers_en.json')); print(f'Careers: {d[\"career_count\"]}')"` - shows 100+ careers
  </verify>
  <done>Initial scrape complete with careers_en.json and careers_fr.json; unit tests pass; each career has provenance metadata</done>
</task>

</tasks>

<verification>
1. `python -c "from jobforge.external.caf import CAFScraper, scrape_caf_careers"` - imports work
2. `pytest tests/external/test_caf_scraper.py -v -k "not integration"` - unit tests pass
3. `ls -la data/caf/careers_*.json` - both EN and FR files exist
4. `python -c "import json; d=json.load(open('data/caf/careers_en.json')); print(len(d['careers']))"` - shows 100+ careers
5. Content hash present in scraped data (provenance tracking)
</verification>

<success_criteria>
- CAF external package created at src/jobforge/external/caf/
- Pydantic models with provenance (CAFCareerListing, CAFOccupation, CAFProvenance)
- Scraper follows TBS pattern with retry, rate limiting, bilingual support
- careers_en.json and careers_fr.json exist with 100+ careers each
- Each career has provenance (source_url, scraped_at, content_hash)
- Unit tests pass
</success_criteria>

<output>
After completion, create `.planning/phases/15-caf-core/15-01-SUMMARY.md`
</output>
