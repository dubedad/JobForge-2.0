---
phase: 14-og-core
plan: 04
type: execute
wave: 2
depends_on: ["14-01"]
files_modified:
  - src/jobforge/external/tbs/pay_rates_scraper.py
  - src/jobforge/ingestion/og_pay_rates.py
  - data/tbs/og_pay_rates_en.json
  - data/gold/fact_og_pay_rates.parquet
  - data/catalog/tables/fact_og_pay_rates.json
  - tests/ingestion/test_og_pay_rates.py
autonomous: true

must_haves:
  truths:
    - "User can query rates of pay for each OG subgroup"
    - "Pay rates include all steps within each level"
    - "Both represented and unrepresented pay scales are captured"
    - "Pay rates have provenance (source URL, scrape timestamp)"
  artifacts:
    - path: "src/jobforge/external/tbs/pay_rates_scraper.py"
      provides: "Scraper for TBS rates of pay pages"
      contains: "def scrape_pay_rates"
    - path: "src/jobforge/ingestion/og_pay_rates.py"
      provides: "Medallion pipeline for fact_og_pay_rates"
      exports: ["ingest_fact_og_pay_rates"]
    - path: "data/gold/fact_og_pay_rates.parquet"
      provides: "Pay rates fact table"
    - path: "data/catalog/tables/fact_og_pay_rates.json"
      provides: "Catalog metadata for pay rates"
  key_links:
    - from: "data/gold/fact_og_pay_rates.parquet"
      to: "data/gold/dim_og_subgroup.parquet"
      via: "og_subgroup_code foreign key"
      pattern: "og_subgroup_code"
---

<objective>
Scrape TBS rates of pay pages and create fact_og_pay_rates gold table.

Purpose: Per CONTEXT.md, capture full pay scales with all steps within each level for both represented and unrepresented employees. This enables pay-related queries in JD Builder Lite.

Output: Pay rates scraper, scraped JSON data, fact_og_pay_rates parquet table, catalog metadata.
</objective>

<execution_context>
@C:\Users\Administrator\.claude/get-shit-done/workflows/execute-plan.md
@C:\Users\Administrator\.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/14-og-core/14-CONTEXT.md
@.planning/phases/14-og-core/14-RESEARCH.md

# Existing patterns
@src/jobforge/external/tbs/scraper.py
@src/jobforge/external/tbs/parser.py
@src/jobforge/ingestion/noc.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create pay rates scraper</name>
  <files>
    src/jobforge/external/tbs/pay_rates_scraper.py
    data/tbs/og_pay_rates_en.json
  </files>
  <action>
Create `src/jobforge/external/tbs/pay_rates_scraper.py`:

1. **PayRateRow** Pydantic model:
   - `og_code`: str (parent group)
   - `og_subgroup_code`: str (e.g., "AS-01")
   - `classification_level`: str (e.g., "AS-01", "AS-02" or level within subgroup)
   - `step`: int (step within level, typically 1-10)
   - `annual_rate`: Decimal | None
   - `hourly_rate`: Decimal | None
   - `effective_date`: date | None
   - `represented`: bool (True for unionized, False for excluded/unrepresented)
   - `collective_agreement`: str | None (e.g., "PA Group", "EC Group")
   - `source_url`: str
   - `scraped_at`: datetime

2. **scrape_pay_rates()** function:
   - Accept rates_of_pay_url from subgroup data
   - Parse TBS pay rates table structure:
     - Find tables with pay rate data (usually structured as Level/Step matrix)
     - Extract annual rates, effective dates
     - Identify represented vs unrepresented sections
   - Return list of PayRateRow objects
   - Add 1.5 second delay between page requests

3. **scrape_all_pay_rates()** function:
   - Load og_subgroups_en.json to get rates_of_pay_urls
   - Scrape each unique URL
   - Handle pages with multiple subgroup pay tables
   - Save to og_pay_rates_en.json

TBS pay rate pages vary in structure. Common patterns:
- Single table with Level columns and Step rows
- Multiple tables (one per subgroup or per effective date)
- Separate sections for represented vs unrepresented

Implement defensive parsing: log warnings for unexpected structures, extract what you can, continue on errors.
  </action>
  <verify>
    `python -c "from jobforge.external.tbs.pay_rates_scraper import scrape_pay_rates; print('Function imported')"`
    `python -c "import json; d=json.load(open('data/tbs/og_pay_rates_en.json')); print(f'Pay rate rows: {len(d)}')"` shows extracted rows
  </verify>
  <done>Pay rates scraper extracts pay scale data from TBS pages; og_pay_rates_en.json created with provenance</done>
</task>

<task type="auto">
  <name>Task 2: Create fact_og_pay_rates ingestion pipeline</name>
  <files>
    src/jobforge/ingestion/og_pay_rates.py
    data/gold/fact_og_pay_rates.parquet
  </files>
  <action>
Create `src/jobforge/ingestion/og_pay_rates.py` with `ingest_fact_og_pay_rates()`:

1. **Bronze schema**:
   ```python
   bronze_schema = {
       "rename": {
           "og_code": "og_code",
           "og_subgroup_code": "og_subgroup_code",
           "classification_level": "classification_level",
           "step": "step",
           "annual_rate": "annual_rate",
           "hourly_rate": "hourly_rate",
           "effective_date": "effective_date",
           "represented": "is_represented",
           "collective_agreement": "collective_agreement",
           "source_url": "_source_url",
           "scraped_at": "_scraped_at",
       },
       "cast": {
           "og_code": pl.Utf8,
           "og_subgroup_code": pl.Utf8,
           "classification_level": pl.Utf8,
           "step": pl.Int32,
           "annual_rate": pl.Float64,  # Store as float for now, format on display
           "hourly_rate": pl.Float64,
           "is_represented": pl.Boolean,
       }
   }
   ```

2. **Silver transforms**:
   - `normalize_codes`: Uppercase codes
   - `validate_rates`: Ensure annual_rate > 0 where not null
   - `dedupe_rates`: Remove exact duplicates

3. **Gold transforms**:
   - `select_fact_columns`: Final schema:
     - og_subgroup_code (FK to dim_og_subgroup)
     - og_code (FK to dim_og)
     - classification_level
     - step
     - annual_rate
     - hourly_rate
     - effective_date
     - is_represented
     - collective_agreement
     - _source_url, _scraped_at, _source_file, _ingested_at, _batch_id, _layer

This is a fact table (not dimension), so no single PK - composite key on (og_subgroup_code, classification_level, step, effective_date).
  </action>
  <verify>
    `python -c "from jobforge.ingestion.og_pay_rates import ingest_fact_og_pay_rates; print('Function imported')"`
    `python -c "from jobforge.ingestion.og_pay_rates import ingest_fact_og_pay_rates; r = ingest_fact_og_pay_rates(); print(f'Rows: {r[\"row_count\"]}')"` shows row count
  </verify>
  <done>ingest_fact_og_pay_rates creates fact_og_pay_rates.parquet with pay scales and provenance</done>
</task>

<task type="auto">
  <name>Task 3: Create catalog metadata and tests</name>
  <files>
    data/catalog/tables/fact_og_pay_rates.json
    tests/ingestion/test_og_pay_rates.py
  </files>
  <action>
**Create catalog metadata** `data/catalog/tables/fact_og_pay_rates.json`:
```json
{
  "table_name": "fact_og_pay_rates",
  "domain": "occupational_groups",
  "description": "TBS Rates of Pay - federal public service pay scales by occupational group, level, and step",
  "source": "Treasury Board Secretariat",
  "source_url": "https://www.canada.ca/en/treasury-board-secretariat/services/pay.html",
  "columns": {
    "og_subgroup_code": {"type": "VARCHAR", "description": "Occupational group subgroup code", "foreign_key": "dim_og_subgroup.og_subgroup_code"},
    "og_code": {"type": "VARCHAR", "description": "Parent occupational group code", "foreign_key": "dim_og.og_code"},
    "classification_level": {"type": "VARCHAR", "description": "Classification level within subgroup"},
    "step": {"type": "INTEGER", "description": "Pay step within level (typically 1-10)"},
    "annual_rate": {"type": "DOUBLE", "description": "Annual salary rate in CAD"},
    "hourly_rate": {"type": "DOUBLE", "description": "Hourly rate in CAD (if available)"},
    "effective_date": {"type": "DATE", "description": "Date pay rate became effective"},
    "is_represented": {"type": "BOOLEAN", "description": "True if unionized/represented, False if excluded"},
    "collective_agreement": {"type": "VARCHAR", "description": "Collective agreement name if represented"}
  },
  "relationships": [
    {"table": "dim_og_subgroup", "join": "fact_og_pay_rates.og_subgroup_code = dim_og_subgroup.og_subgroup_code", "type": "many-to-one"},
    {"table": "dim_og", "join": "fact_og_pay_rates.og_code = dim_og.og_code", "type": "many-to-one"}
  ]
}
```

**Add tests** in `tests/ingestion/test_og_pay_rates.py`:
- Test PayRateRow model validation
- Test scrape_pay_rates with sample HTML (mock response)
- Test ingest_fact_og_pay_rates produces rows
- Test FK integrity (og_subgroup_codes exist in dim_og_subgroup)
- Test provenance columns exist
- Test annual_rate values are positive where not null
  </action>
  <verify>
    `pytest tests/ingestion/test_og_pay_rates.py -v` passes
    `ls -la data/catalog/tables/fact_og_pay_rates.json` shows file exists
    `python -c "import polars as pl; df=pl.read_parquet('data/gold/fact_og_pay_rates.parquet'); print(df.columns)"` shows expected columns
  </verify>
  <done>Catalog metadata created; tests pass; fact_og_pay_rates queryable with FK relationships</done>
</task>

</tasks>

<verification>
1. `pytest tests/ingestion/test_og_pay_rates.py -v` - all tests pass
2. `python -c "import polars as pl; print(pl.read_parquet('data/gold/fact_og_pay_rates.parquet').shape)"` - shows row count
3. Parquet file has provenance columns
4. Catalog JSON file is valid
</verification>

<success_criteria>
- Pay rates scraper extracts pay scales from TBS pages
- og_pay_rates_en.json contains scraped data with provenance
- fact_og_pay_rates.parquet created with pay scale rows
- FK relationships to dim_og and dim_og_subgroup validated
- Both represented and unrepresented pay scales captured
- Catalog metadata file created
- Tests pass
</success_criteria>

<output>
After completion, create `.planning/phases/14-og-core/14-04-SUMMARY.md`
</output>
