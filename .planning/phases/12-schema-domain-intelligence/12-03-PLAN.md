---
phase: 12-schema-domain-intelligence
plan: 03
type: execute
wave: 3
depends_on: ["12-02"]
files_modified:
  - src/jobforge/api/data_query.py
  - orbit/retrievers/duckdb.py
  - tests/test_data_query.py
autonomous: true

must_haves:
  truths:
    - "DataQueryResult includes source_tables list showing tables used in query"
    - "DataQueryResult includes source_attribution string with provenance info"
    - "System prompt includes workforce domain patterns for gap calculations"
    - "Orbit DuckDBRetriever uses same enhanced DDL and prompts"
  artifacts:
    - path: "src/jobforge/api/data_query.py"
      provides: "Enhanced query service with source attribution"
      contains: "source_attribution"
    - path: "orbit/retrievers/duckdb.py"
      provides: "Enhanced Orbit retriever"
      contains: "WORKFORCE"
  key_links:
    - from: "src/jobforge/api/data_query.py"
      to: "src/jobforge/api/schema_ddl.py"
      via: "uses enhanced DDL"
      pattern: "generate_schema_ddl"
    - from: "orbit/retrievers/duckdb.py"
      to: "data/catalog/tables/*.json"
      via: "reads catalog for attribution"
      pattern: "catalog"
---

<objective>
Enhance query services with workforce domain patterns and source attribution

Purpose: Enable Claude to understand workforce intelligence queries and provide provenance in results
Output: Updated DataQueryResult with source attribution, enhanced system prompts for text-to-SQL
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/12-schema-domain-intelligence/12-CONTEXT.md
@.planning/phases/12-schema-domain-intelligence/12-RESEARCH.md
@.planning/phases/12-schema-domain-intelligence/12-01-SUMMARY.md
@.planning/phases/12-schema-domain-intelligence/12-02-SUMMARY.md
@src/jobforge/api/data_query.py
@orbit/retrievers/duckdb.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add source attribution to DataQueryResult</name>
  <files>src/jobforge/api/data_query.py</files>
  <action>
Modify `src/jobforge/api/data_query.py`:

1. Update `DataQueryResult` model to add source fields:
   ```python
   class DataQueryResult(BaseModel):
       question: str
       sql: str
       explanation: str
       results: list[dict]
       row_count: int
       error: str | None = None
       source_tables: list[str] = Field(default_factory=list, description="Tables used in query")
       source_attribution: str = Field(default="", description="Human-readable source attribution")
   ```

2. Add helper method to `DataQueryService`:
   ```python
   def _build_source_attribution(self, tables: list[str]) -> str:
       """Build source attribution string from table names."""
       if not tables:
           return ""

       # Load catalog to get domain info
       catalog_path = self.config.catalog_tables_path()
       attributions = []

       for table in tables:
           json_path = catalog_path / f"{table}.json"
           if json_path.exists():
               metadata = json.loads(json_path.read_text())
               domain = metadata.get("domain", "WiQ")
               # Map domain to friendly name
               source_map = {
                   "forecasting": "COPS Open Canada",
                   "noc": "Statistics Canada NOC",
                   "oasis": "OaSIS",
                   "job_architecture": "TBS Job Architecture",
               }
               source = source_map.get(domain, domain)
               attributions.append(f"{table} ({source})")
           else:
               attributions.append(table)

       return "Source: " + ", ".join(attributions)
   ```

3. Update `query()` method to populate source fields:
   ```python
   return DataQueryResult(
       question=question,
       sql=sql_result.sql,
       explanation=sql_result.explanation,
       results=results,
       row_count=len(results),
       source_tables=sql_result.tables_used,
       source_attribution=self._build_source_attribution(sql_result.tables_used),
   )
   ```

4. Update error return paths to include empty source fields (already handled by defaults).

5. Add `import json` at top if not present.
  </action>
  <verify>
    python -c "from jobforge.api.data_query import DataQueryResult; r=DataQueryResult(question='test', sql='SELECT 1', explanation='test', results=[], row_count=0, source_tables=['cops_employment'], source_attribution='Source: cops_employment (COPS)'); print(r.source_attribution)"
  </verify>
  <done>
    DataQueryResult has source_tables and source_attribution fields
  </done>
</task>

<task type="auto">
  <name>Task 2: Enhance system prompts with workforce domain patterns</name>
  <files>src/jobforge/api/data_query.py, orbit/retrievers/duckdb.py</files>
  <action>
1. Update `DataQueryService.SYSTEM_PROMPT` in `src/jobforge/api/data_query.py`:

```python
SYSTEM_PROMPT = """You are a SQL expert for the WiQ (Workforce Intelligence) database.
Given a schema and question, generate a DuckDB-compatible SELECT query.
The database contains Canadian occupational data:
- dim_noc: National Occupational Classification hierarchy (NOC 2021)
- dim_occupations: Occupational groups with TBS metadata
- cops_*: Canadian Occupational Projection System forecasts (2023-2033)
- oasis_*: Occupational attributes (skills, abilities, knowledge)
- element_*: NOC element data (titles, duties, requirements)
- job_architecture: Internal job architecture hierarchy

WORKFORCE INTELLIGENCE PATTERNS:
- demand tables: cops_employment, cops_employment_growth, cops_retirements, cops_retirement_rates, cops_other_replacement
- supply tables: cops_immigration, cops_school_leavers, cops_other_seekers
- For "shortage" or "gap" queries: compare demand vs supply
- NOC codes are 5-digit strings (e.g., '21232' for Software Engineers)
- Year columns MUST be quoted: SELECT "2025" FROM cops_employment

ENTITY RECOGNITION:
- NOC codes: 5-digit numbers like 21232, 41200, 00010
- Occupation names: "Software Engineers", "Financial Managers", etc.
- Years: 2023-2033 (projection period)

IMPORTANT:
- Only generate SELECT queries (never INSERT, UPDATE, DELETE, DROP)
- Use DuckDB SQL syntax
- Quote year columns with double quotes: SELECT "2025"
- Keep queries simple and focused on answering the question
- Limit results to 100 rows unless explicitly asked for more"""
```

2. Update `DuckDBRetriever.SYSTEM_PROMPT` in `orbit/retrievers/duckdb.py` to match (same content).
  </action>
  <verify>
    python -c "from jobforge.api.data_query import DataQueryService; print('WORKFORCE' in DataQueryService.SYSTEM_PROMPT)"
  </verify>
  <done>
    System prompts include WORKFORCE INTELLIGENCE PATTERNS and ENTITY RECOGNITION sections
  </done>
</task>

<task type="auto">
  <name>Task 3: Enhance Orbit DuckDBRetriever with catalog-based DDL</name>
  <files>orbit/retrievers/duckdb.py</files>
  <action>
Update `orbit/retrievers/duckdb.py` to use the enhanced DDL generator:

1. Add imports at top:
   ```python
   import sys
   from pathlib import Path

   # Add project root to path for jobforge imports
   sys.path.insert(0, str(Path(__file__).parent.parent.parent))

   from jobforge.api.schema_ddl import generate_schema_ddl
   from jobforge.pipeline.config import PipelineConfig
   ```

2. Update `initialize()` method to use `generate_schema_ddl()`:
   ```python
   def initialize(self) -> None:
       """Initialize DuckDB connection and generate enhanced schema DDL."""
       self._conn = duckdb.connect(":memory:")
       self._client = anthropic.Anthropic()

       # Register parquet files as views
       for parquet in sorted(self.parquet_path.glob("*.parquet")):
           table_name = parquet.stem
           abs_path = str(parquet.resolve()).replace("\\", "/")
           self._conn.execute(
               f"CREATE VIEW {table_name} AS SELECT * FROM '{abs_path}'"
           )

       # Use enhanced DDL generator (reads enriched catalog)
       try:
           config = PipelineConfig()
           self._schema_ddl = generate_schema_ddl(config)
       except Exception:
           # Fallback to basic DDL if jobforge not available
           self._schema_ddl = self._generate_basic_ddl()

   def _generate_basic_ddl(self) -> str:
       """Generate basic DDL as fallback."""
       ddl_parts = []
       for parquet in sorted(self.parquet_path.glob("*.parquet")):
           table_name = parquet.stem
           cols = self._conn.execute(f"DESCRIBE {table_name}").fetchall()
           col_defs = [f"  {col[0]} {col[1]}" for col in cols]
           ddl_parts.append(f"CREATE TABLE {table_name} (\n" + ",\n".join(col_defs) + "\n);")
       return "\n\n".join(ddl_parts)
   ```

3. The SYSTEM_PROMPT was already updated in Task 2.

This ensures Orbit uses the same enriched DDL as the JobForge API.
  </action>
  <verify>
    python -c "import sys; sys.path.insert(0, '.'); from orbit.retrievers.duckdb import DuckDBRetriever; r=DuckDBRetriever({'parquet_path': 'data/gold'}); r.initialize(); print('COMMENT' in r._schema_ddl or 'RELATIONSHIPS' in r._schema_ddl)"
  </verify>
  <done>
    - Orbit DuckDBRetriever uses generate_schema_ddl() for enhanced DDL
    - Fallback to basic DDL works if jobforge import fails
  </done>
</task>

</tasks>

<verification>
After all tasks:

1. Test DataQueryResult source attribution:
   ```python
   python -c "
   from jobforge.api.data_query import DataQueryService, DataQueryResult
   # Test model fields exist
   r = DataQueryResult(
       question='test', sql='SELECT 1', explanation='test',
       results=[], row_count=0,
       source_tables=['cops_employment'],
       source_attribution='Source: cops_employment (COPS Open Canada)'
   )
   print('source_tables:', r.source_tables)
   print('source_attribution:', r.source_attribution)
   "
   ```

2. Test system prompt content:
   ```python
   python -c "
   from jobforge.api.data_query import DataQueryService
   prompt = DataQueryService.SYSTEM_PROMPT
   assert 'WORKFORCE' in prompt, 'Missing workforce patterns'
   assert 'demand tables' in prompt, 'Missing demand tables'
   assert 'NOC codes' in prompt, 'Missing NOC entity hints'
   print('System prompt validated')
   "
   ```

3. Test Orbit retriever DDL:
   ```python
   python -c "
   import sys; sys.path.insert(0, '.')
   from orbit.retrievers.duckdb import DuckDBRetriever
   r = DuckDBRetriever({'parquet_path': 'data/gold'})
   r.initialize()
   print('DDL has comments:', 'COMMENT' in r._schema_ddl)
   print('DDL has relationships:', 'RELATIONSHIPS' in r._schema_ddl)
   r.close()
   "
   ```

4. Run existing tests to ensure no regressions:
   ```
   pytest tests/test_data_query.py tests/test_schema_ddl.py -v
   ```
</verification>

<success_criteria>
- [ ] DataQueryResult has source_tables and source_attribution fields
- [ ] Query results include source attribution (e.g., "Source: cops_employment (COPS Open Canada)")
- [ ] System prompts include WORKFORCE INTELLIGENCE PATTERNS section
- [ ] System prompts include ENTITY RECOGNITION hints
- [ ] Orbit DuckDBRetriever uses enhanced DDL from jobforge
- [ ] Fallback to basic DDL works if catalog unavailable
- [ ] All existing tests pass
</success_criteria>

<output>
After completion, create `.planning/phases/12-schema-domain-intelligence/12-03-SUMMARY.md`
</output>
