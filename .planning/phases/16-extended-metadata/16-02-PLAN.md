---
phase: 16-extended-metadata
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - src/jobforge/external/tbs/evaluation_scraper.py
  - src/jobforge/ingestion/og_evaluation.py
  - data/tbs/og_evaluation_standards.json
  - data/gold/dim_og_job_evaluation_standard.parquet
  - data/catalog/tables/dim_og_job_evaluation_standard.json
  - tests/external/tbs/test_evaluation_scraper.py
  - tests/ingestion/test_og_evaluation.py
autonomous: true

must_haves:
  truths:
    - "User can query job evaluation standards per occupational group"
    - "User can query evaluation factors with numeric points where available"
    - "User can see per-record source URLs to TBS authority"
    - "Historical versions preserved if TBS provides them"
  artifacts:
    - path: "src/jobforge/external/tbs/evaluation_scraper.py"
      provides: "Job evaluation standards scraper"
      exports: ["scrape_evaluation_standards", "EvaluationStandard"]
    - path: "src/jobforge/ingestion/og_evaluation.py"
      provides: "Ingestion pipeline for dim_og_job_evaluation_standard"
      exports: ["ingest_dim_og_job_evaluation_standard"]
    - path: "data/gold/dim_og_job_evaluation_standard.parquet"
      provides: "Gold table with evaluation standards"
      min_rows: 20
    - path: "data/catalog/tables/dim_og_job_evaluation_standard.json"
      provides: "Catalog metadata"
      contains: "evaluation_factor"
  key_links:
    - from: "src/jobforge/ingestion/og_evaluation.py"
      to: "src/jobforge/external/tbs/evaluation_scraper.py"
      via: "import scrape_evaluation_standards"
      pattern: "from jobforge.external.tbs.evaluation_scraper import"
    - from: "data/gold/dim_og_job_evaluation_standard.parquet"
      to: "data/gold/dim_og.parquet"
      via: "FK og_code"
      pattern: "og_code"
---

<objective>
Create job evaluation standards scraper and gold table for TBS classification standards.

Purpose: Enable querying job evaluation factors and points for classification decisions. Per CONTEXT.md: "Both classification standard definitions AND evaluation factors with numeric points where available + descriptive text."
Output: dim_og_job_evaluation_standard gold table with evaluation factors, points, and descriptive text.
</objective>

<execution_context>
@C:\Users\Administrator\.claude/get-shit-done/workflows/execute-plan.md
@C:\Users\Administrator\.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/16-extended-metadata/16-CONTEXT.md
@.planning/phases/16-extended-metadata/16-RESEARCH.md
@src/jobforge/external/tbs/scraper.py
@src/jobforge/external/tbs/pay_rates_scraper.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create job evaluation standards scraper</name>
  <files>
    src/jobforge/external/tbs/evaluation_scraper.py
    data/tbs/og_evaluation_standards.json
    tests/external/tbs/test_evaluation_scraper.py
  </files>
  <action>
Create `src/jobforge/external/tbs/evaluation_scraper.py` following existing TBS scraper patterns:

1. **First: Manual inspection of TBS page structure**
   - Navigate to https://www.canada.ca/en/treasury-board-secretariat/services/staffing/qualification-standards.html
   - Identify links to classification/evaluation standards (may be separate from qualification standards)
   - Check if content is HTML or PDF (per RESEARCH.md Open Question 1)
   - Adapt scraper approach based on actual format

2. `EvaluationStandard` Pydantic model:
   ```python
   class EvaluationStandard(BaseModel):
       og_code: str  # FK to dim_og
       og_subgroup_code: str | None = None
       standard_name: str  # e.g., "Computer Systems (CS) Job Evaluation Standard"
       standard_type: str  # 'classification_standard' or 'evaluation_factor'

       # Evaluation factor details (when applicable)
       factor_name: str | None = None  # e.g., "Knowledge", "Decision Making"
       factor_description: str | None = None
       factor_points: int | None = None  # Numeric points where available
       factor_level: str | None = None  # e.g., "Level 1", "Level 2"
       level_description: str | None = None

       # Full text for search
       full_text: str

       # Temporal tracking (CONTEXT.md)
       effective_date: str | None = None
       version: str | None = None  # e.g., "2018 version"

       # Provenance
       source_url: str
       scraped_at: str
   ```

3. `scrape_evaluation_standards(delay: float = 1.5) -> list[EvaluationStandard]`:
   - Follow existing scraper pattern (1.5s delay between requests)
   - Use tenacity retry decorator for resilience
   - Parse HTML tables with BeautifulSoup (or PDF with pdfplumber if needed)
   - Extract evaluation factors with points where structure allows
   - Log warnings for pages that can't be parsed (per RESEARCH.md Pitfall 2)

4. `scrape_all_evaluation_standards(output_dir: Path) -> Path`:
   - Orchestrate scraping all OG evaluation standard pages
   - Save to data/tbs/og_evaluation_standards.json with provenance

Create tests in `tests/external/tbs/test_evaluation_scraper.py`:
- Test EvaluationStandard model validation
- Test scraper handles missing pages gracefully
- Test factor points extraction (mock HTML)
- Test effective_date extraction
- Note: Integration test may be skipped if TBS structure unclear
  </action>
  <verify>
Run: `pytest tests/external/tbs/test_evaluation_scraper.py -v`
Tests pass. If integration test skipped, document why in test docstring.
Check scraped data: `python -c "import json; data = json.load(open('data/tbs/og_evaluation_standards.json')); print(f'Records: {len(data)}')"` (if scraping ran)
  </verify>
  <done>
Evaluation scraper module exists with EvaluationStandard model. Scraper handles TBS page structure (HTML or PDF). Tests pass for model and parsing logic.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create evaluation standards ingestion pipeline</name>
  <files>
    src/jobforge/ingestion/og_evaluation.py
    data/gold/dim_og_job_evaluation_standard.parquet
    tests/ingestion/test_og_evaluation.py
  </files>
  <action>
Create `src/jobforge/ingestion/og_evaluation.py` following existing ingestion patterns:

1. `ingest_dim_og_job_evaluation_standard()`:
   - Load from data/tbs/og_evaluation_standards.json
   - Bronze: Parse JSON, validate required fields
   - Silver: Normalize og_codes, validate FK to dim_og (soft validation)
   - Gold: Write parquet with full schema

2. Schema for dim_og_job_evaluation_standard:
   ```python
   columns = [
       "og_code",              # PK/FK to dim_og
       "og_subgroup_code",     # Optional subgroup
       "standard_name",        # Classification standard name
       "standard_type",        # 'classification_standard' or 'evaluation_factor'
       "factor_name",          # Evaluation factor name
       "factor_description",   # Factor description
       "factor_points",        # Numeric points (nullable)
       "factor_level",         # Level within factor
       "level_description",    # Level description
       "full_text",            # Complete text for search
       "effective_date",       # When standard became effective
       "version",              # Version identifier
       "_source_url",          # Per-record provenance (CONTEXT.md)
       "_scraped_at",
       "_ingested_at",
       "_batch_id",
       "_layer",
   ]
   ```

3. Output: data/gold/dim_og_job_evaluation_standard.parquet

4. Handle sparsity gracefully:
   - Not all OGs have formal evaluation standards
   - Store what's available, don't fail on missing data
   - Log summary of coverage

Create tests in `tests/ingestion/test_og_evaluation.py`:
- Test pipeline produces parquet with expected columns
- Test og_code normalization
- Test provenance fields present
- Test handles empty/minimal source data
  </action>
  <verify>
Run: `pytest tests/ingestion/test_og_evaluation.py -v`
Tests pass.
Check output: `python -c "import polars as pl; df = pl.read_parquet('data/gold/dim_og_job_evaluation_standard.parquet'); print(f'Rows: {len(df)}, Cols: {len(df.columns)}')"` (if data exists)
  </verify>
  <done>
dim_og_job_evaluation_standard.parquet created with evaluation factors and points where available. Pipeline handles sparse TBS data gracefully.
  </done>
</task>

<task type="auto">
  <name>Task 3: Create catalog metadata for evaluation standards</name>
  <files>
    data/catalog/tables/dim_og_job_evaluation_standard.json
  </files>
  <action>
Create `data/catalog/tables/dim_og_job_evaluation_standard.json`:

1. Table metadata:
   ```json
   {
     "name": "dim_og_job_evaluation_standard",
     "description": "TBS Job Evaluation Standards per occupational group with evaluation factors, points, and level descriptions",
     "source": "Treasury Board Secretariat Classification Standards",
     "source_url": "https://www.canada.ca/en/treasury-board-secretariat/services/staffing/qualification-standards.html",
     "domain": "occupational_groups",
     "layer": "gold",
     "columns": [...]
   }
   ```

2. Column descriptions:
   - og_code: "Primary key - TBS occupational group code (FK to dim_og)"
   - standard_name: "Name of the classification or evaluation standard"
   - standard_type: "Type: 'classification_standard' or 'evaluation_factor'"
   - factor_name: "Evaluation factor name (e.g., Knowledge, Decision Making)"
   - factor_points: "Numeric point value for factor (where available)"
   - factor_level: "Level within evaluation factor"
   - level_description: "Description of what this level entails"
   - effective_date: "Date standard became effective"
   - _source_url: "Per-record URL to authoritative TBS page"

3. Relationships:
   ```json
   "relationships": [
     {
       "to_table": "dim_og",
       "from_column": "og_code",
       "to_column": "og_code",
       "cardinality": "many-to-one"
     }
   ]
   ```
  </action>
  <verify>
Run: `python -c "import json; data = json.load(open('data/catalog/tables/dim_og_job_evaluation_standard.json')); print(data['name']); print(len(data['columns']))"`
Output shows table name and 15+ columns.
  </verify>
  <done>
Catalog metadata created for dim_og_job_evaluation_standard with column descriptions and FK relationship.
  </done>
</task>

</tasks>

<verification>
All tasks complete when:
1. `pytest tests/external/tbs/test_evaluation_scraper.py -v` - Scraper tests pass
2. `pytest tests/ingestion/test_og_evaluation.py -v` - Ingestion tests pass
3. Evaluation standards JSON exists: `ls data/tbs/og_evaluation_standards.json`
4. Gold table exists: `ls data/gold/dim_og_job_evaluation_standard.parquet`
5. Catalog metadata exists: `ls data/catalog/tables/dim_og_job_evaluation_standard.json`
</verification>

<success_criteria>
- Scraper module handles TBS evaluation standard page structure (HTML or PDF)
- dim_og_job_evaluation_standard.parquet exists (row count depends on TBS data availability)
- Evaluation factors with numeric points extracted where TBS provides them
- Per-record source URLs preserved
- Catalog metadata complete
- Tests passing
</success_criteria>

<output>
After completion, create `.planning/phases/16-extended-metadata/16-02-SUMMARY.md`
</output>
