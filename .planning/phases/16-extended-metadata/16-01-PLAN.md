---
phase: 16-extended-metadata
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/jobforge/external/tbs/qualification_parser.py
  - src/jobforge/ingestion/og_qualification_standards.py
  - data/gold/dim_og_qualification_standard.parquet
  - data/catalog/tables/dim_og_qualification_standard.json
  - tests/external/tbs/test_qualification_parser.py
  - tests/ingestion/test_og_qualification_standards.py
autonomous: true

must_haves:
  truths:
    - "User can query essential vs asset qualifications separately"
    - "User can query structured bilingual requirements (reading, writing, oral levels)"
    - "User can query security clearance levels per OG"
    - "User can query minimum years of experience as numeric filter"
    - "User can query conditions of employment flags (travel, shift work)"
  artifacts:
    - path: "src/jobforge/external/tbs/qualification_parser.py"
      provides: "Enhanced qualification text parsing with CONTEXT.md structured fields"
      exports: ["parse_enhanced_qualification", "extract_bilingual_levels", "extract_security_clearance"]
    - path: "src/jobforge/ingestion/og_qualification_standards.py"
      provides: "Enhanced ingestion pipeline for dim_og_qualification_standard"
      exports: ["ingest_dim_og_qualification_standard"]
    - path: "data/gold/dim_og_qualification_standard.parquet"
      provides: "Gold table with structured qualification fields"
      min_rows: 70
    - path: "data/catalog/tables/dim_og_qualification_standard.json"
      provides: "Catalog metadata with FK relationships"
      contains: "essential_qualification"
  key_links:
    - from: "src/jobforge/ingestion/og_qualification_standards.py"
      to: "src/jobforge/external/tbs/qualification_parser.py"
      via: "import parse_enhanced_qualification"
      pattern: "from jobforge.external.tbs.qualification_parser import"
    - from: "data/gold/dim_og_qualification_standard.parquet"
      to: "data/gold/dim_og.parquet"
      via: "FK og_code"
      pattern: "og_code"
---

<objective>
Create enhanced qualification standards parser and ingestion pipeline with CONTEXT.md structured fields.

Purpose: Enable JD Builder to query structured qualification requirements (essential vs asset, bilingual levels, security clearance, experience years) instead of parsing raw text.
Output: dim_og_qualification_standard gold table with 15+ structured fields, replacing basic dim_og_qualifications.
</objective>

<execution_context>
@C:\Users\Administrator\.claude/get-shit-done/workflows/execute-plan.md
@C:\Users\Administrator\.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/16-extended-metadata/16-CONTEXT.md
@.planning/phases/16-extended-metadata/16-RESEARCH.md
@src/jobforge/ingestion/og_qualifications.py
@src/jobforge/external/tbs/pdf_extractor.py
@data/tbs/og_qualification_text.json
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create enhanced qualification parser module</name>
  <files>
    src/jobforge/external/tbs/qualification_parser.py
    tests/external/tbs/test_qualification_parser.py
  </files>
  <action>
Create `src/jobforge/external/tbs/qualification_parser.py` with functions to extract CONTEXT.md structured fields from qualification text:

1. `parse_enhanced_qualification(full_text: str) -> EnhancedQualification`:
   - Returns Pydantic model with ALL structured fields from CONTEXT.md
   - Use permissive regex patterns with fallbacks (per RESEARCH.md Pitfall 1)
   - Always preserve original text alongside parsed values

2. Structured extraction functions (called by main parser):
   - `extract_education_level(text) -> tuple[str | None, str]`: Returns (standardized_level, original_text)
     - Levels: 'high_school', 'certificate', 'diploma', 'bachelors', 'masters', 'phd', 'professional_degree'
   - `extract_experience_years(text) -> tuple[int | None, str]`: Returns (min_years, original_text)
     - Patterns: "minimum of N years", "at least N years", "N+ years of"
   - `extract_bilingual_levels(text) -> dict`: Returns {reading: str|None, writing: str|None, oral: str|None}
     - Patterns: "BBB", "CBC", "CCC" proficiency levels
   - `extract_security_clearance(text) -> str | None`: Returns 'Reliability', 'Secret', 'Top Secret', or None
   - `extract_equivalency(text) -> tuple[bool, str | None]`: Returns (has_equivalency, statement)

3. `EnhancedQualification` Pydantic model with CONTEXT.md fields:
   ```python
   class EnhancedQualification(BaseModel):
       # Education: structured + original
       education_level: str | None
       education_requirement_text: str

       # Experience: numeric + original
       min_years_experience: int | None
       experience_requirement_text: str

       # Essential vs Asset qualifications (CONTEXT.md requirement)
       essential_qualification_text: str | None
       asset_qualification_text: str | None

       # Equivalency
       has_equivalency: bool = False
       equivalency_statement: str | None = None

       # Bilingual requirements: structured levels
       bilingual_reading_level: str | None  # BBB/CBC levels
       bilingual_writing_level: str | None
       bilingual_oral_level: str | None

       # Security clearance: structured enum
       security_clearance: str | None  # 'Reliability', 'Secret', 'Top Secret'

       # Conditions of employment (CONTEXT.md)
       requires_travel: bool = False
       shift_work: bool = False
       physical_demands: bool = False

       # Operational requirements (CONTEXT.md)
       overtime_required: bool = False
       on_call_required: bool = False
       deployments_required: bool = False

       # Certification (preserve existing)
       certification_requirement: str | None = None

       # Full text for search
       full_text: str
   ```

Create tests in `tests/external/tbs/test_qualification_parser.py`:
- Test education level extraction (each level type)
- Test experience years extraction (various patterns)
- Test bilingual level extraction (BBB, CBC patterns)
- Test security clearance extraction
- Test equivalency detection
- Test full parse_enhanced_qualification with real TBS text sample
- Test fallback when patterns don't match (returns None, preserves text)
  </action>
  <verify>
Run: `pytest tests/external/tbs/test_qualification_parser.py -v`
All tests pass. Parser extracts structured fields from test cases.
  </verify>
  <done>
Parser module exists with EnhancedQualification model. All structured field extractors work with fallbacks to original text. Tests pass.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create enhanced qualification ingestion pipeline</name>
  <files>
    src/jobforge/ingestion/og_qualification_standards.py
    data/gold/dim_og_qualification_standard.parquet
    tests/ingestion/test_og_qualification_standards.py
  </files>
  <action>
Create `src/jobforge/ingestion/og_qualification_standards.py` replacing the basic og_qualifications.py:

1. `ingest_dim_og_qualification_standard()`:
   - Load from data/tbs/og_qualification_text.json (same source as existing)
   - Apply parse_enhanced_qualification to each record
   - Bronze: Load JSON, parse structured fields
   - Silver: Normalize codes, validate FK to dim_og (soft validation per existing pattern)
   - Gold: Write parquet with ALL EnhancedQualification fields plus provenance

2. Schema (matches EnhancedQualification + provenance):
   ```python
   columns = [
       "og_code",                       # PK/FK to dim_og
       "og_subgroup_code",              # Optional subgroup
       "education_level",               # Standardized
       "education_requirement_text",    # Original
       "min_years_experience",          # Numeric
       "experience_requirement_text",   # Original
       "essential_qualification_text",  # NEW
       "asset_qualification_text",      # NEW
       "has_equivalency",
       "equivalency_statement",
       "bilingual_reading_level",       # NEW
       "bilingual_writing_level",       # NEW
       "bilingual_oral_level",          # NEW
       "security_clearance",            # NEW
       "requires_travel",               # NEW
       "shift_work",                    # NEW
       "physical_demands",              # NEW
       "overtime_required",             # NEW
       "on_call_required",              # NEW
       "deployments_required",          # NEW
       "certification_requirement",
       "full_text",
       "_source_url",
       "_extracted_at",
       "_ingested_at",
       "_batch_id",
       "_layer",
   ]
   ```

3. Output: data/gold/dim_og_qualification_standard.parquet

Create tests in `tests/ingestion/test_og_qualification_standards.py`:
- Test pipeline produces parquet with expected columns
- Test structured fields are populated (at least 50% of records have education_level)
- Test FK validation logs warnings for orphan og_codes
- Test provenance fields present
  </action>
  <verify>
Run: `pytest tests/ingestion/test_og_qualification_standards.py -v`
All tests pass. Parquet file exists with enhanced columns.
Check: `python -c "import polars as pl; df = pl.read_parquet('data/gold/dim_og_qualification_standard.parquet'); print(df.columns); print(len(df))"`
Output shows 25+ columns and 70+ rows.
  </verify>
  <done>
dim_og_qualification_standard.parquet created with all CONTEXT.md structured fields. Pipeline extracts education_level, min_years_experience, bilingual levels, security clearance from qualification text.
  </done>
</task>

<task type="auto">
  <name>Task 3: Create catalog metadata for enhanced qualifications</name>
  <files>
    data/catalog/tables/dim_og_qualification_standard.json
  </files>
  <action>
Create `data/catalog/tables/dim_og_qualification_standard.json` with:

1. Table metadata:
   ```json
   {
     "name": "dim_og_qualification_standard",
     "description": "TBS Qualification Standards per occupational group with structured fields for education, experience, bilingual requirements, and security clearance",
     "source": "Treasury Board Secretariat Qualification Standards",
     "source_url": "https://www.canada.ca/en/treasury-board-secretariat/services/staffing/qualification-standards/core.html",
     "domain": "occupational_groups",
     "layer": "gold",
     "columns": [...]
   }
   ```

2. Column descriptions for each field:
   - og_code: "Primary key - TBS occupational group code (FK to dim_og)"
   - education_level: "Standardized education level: high_school, certificate, diploma, bachelors, masters, phd"
   - min_years_experience: "Minimum years of experience required (numeric for filtering)"
   - essential_qualification_text: "Essential/mandatory qualification requirements"
   - asset_qualification_text: "Asset/preferred qualification requirements"
   - bilingual_reading_level: "Official language reading proficiency level (BBB, CBC, etc.)"
   - security_clearance: "Required security clearance level: Reliability, Secret, or Top Secret"
   - requires_travel: "Position requires travel (boolean)"
   - full_text: "Complete qualification standard text for full-text search"

3. Relationships:
   ```json
   "relationships": [
     {
       "to_table": "dim_og",
       "from_column": "og_code",
       "to_column": "og_code",
       "cardinality": "many-to-one"
     }
   ]
   ```

4. Quality and governance metadata (per CONTEXT.md):
   - No DMBOK tags yet (added in Plan 06)
  </action>
  <verify>
Run: `python -c "import json; data = json.load(open('data/catalog/tables/dim_og_qualification_standard.json')); print(len(data['columns'])); print([c['name'] for c in data['columns'][:5]])"`
Output shows 25+ columns with expected names.
  </verify>
  <done>
Catalog metadata created with column descriptions for all enhanced qualification fields. FK relationship to dim_og documented.
  </done>
</task>

</tasks>

<verification>
All tasks complete when:
1. `pytest tests/external/tbs/test_qualification_parser.py -v` - All parser tests pass
2. `pytest tests/ingestion/test_og_qualification_standards.py -v` - All ingestion tests pass
3. `python -c "import polars as pl; df = pl.read_parquet('data/gold/dim_og_qualification_standard.parquet'); print(f'Rows: {len(df)}, Cols: {len(df.columns)}')"` - Shows 70+ rows, 25+ columns
4. Enhanced columns present: education_level, min_years_experience, bilingual_reading_level, security_clearance
</verification>

<success_criteria>
- dim_og_qualification_standard.parquet exists with 70+ rows and 25+ columns
- Parser extracts structured fields from TBS qualification text
- Structured fields populated for majority of records (where data exists)
- Original text preserved alongside structured fields
- Catalog metadata complete with column descriptions
- All tests passing
</success_criteria>

<output>
After completion, create `.planning/phases/16-extended-metadata/16-01-SUMMARY.md`
</output>
