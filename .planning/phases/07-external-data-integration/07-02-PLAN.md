---
phase: 07-external-data-integration
plan: 02
type: execute
wave: 2
depends_on: [07-01]
files_modified:
  - src/jobforge/external/llm/__init__.py
  - src/jobforge/external/llm/client.py
  - src/jobforge/external/llm/service.py
  - src/jobforge/external/llm/prompts.py
  - src/jobforge/external/models.py
  - tests/test_llm_imputation.py
autonomous: true
user_setup:
  - service: openai
    why: "LLM imputation requires OpenAI API credentials"
    env_vars:
      - name: OPENAI_API_KEY
        source: "https://platform.openai.com/api-keys -> Create new secret key"

must_haves:
  truths:
    - "LLM can impute attribute values for cells empty after hierarchical inheritance"
    - "LLM returns structured response with confidence and rationale"
    - "Imputed values carry provenance with source_type='LLM'"
    - "All LLM answers accepted regardless of confidence (stored for downstream filtering)"
  artifacts:
    - path: "src/jobforge/external/llm/client.py"
      provides: "OpenAI client wrapper"
      exports: ["LLMClient"]
    - path: "src/jobforge/external/llm/service.py"
      provides: "Attribute imputation orchestration"
      exports: ["AttributeImputationService", "impute_missing_attributes"]
    - path: "src/jobforge/external/llm/prompts.py"
      provides: "Prompt templates for imputation"
      exports: ["IMPUTATION_SYSTEM_PROMPT", "build_imputation_prompt"]
    - path: "src/jobforge/external/models.py"
      provides: "Pydantic models for LLM responses"
      exports: ["ImputedAttributeValue", "ImputationResponse"]
  key_links:
    - from: "src/jobforge/external/llm/service.py"
      to: "src/jobforge/external/llm/client.py"
      via: "LLMClient.parse for structured output"
      pattern: "client\\.parse.*ImputationResponse"
    - from: "src/jobforge/external/llm/service.py"
      to: "src/jobforge/imputation/provenance.py"
      via: "Creates imputation provenance"
      pattern: "source_type.*LLM"
---

<objective>
Create LLM-powered attribute imputation service for filling gaps after hierarchical inheritance and O*NET fallback.

Purpose: When authoritative sources (hierarchy) and O*NET have no data for an attribute, use GPT-4o to intelligently impute values. Per CONTEXT.md: accept all answers regardless of confidence, store rationale for downstream filtering.

Output: Working `jobforge.external.llm` package with OpenAI client, prompt templates, and imputation service returning structured Pydantic models.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/07-external-data-integration/07-RESEARCH.md
@.planning/phases/07-external-data-integration/07-CONTEXT.md
@.planning/phases/07-external-data-integration/07-01-SUMMARY.md
@src/jobforge/imputation/models.py
@src/jobforge/imputation/provenance.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create LLM client and response models</name>
  <files>
    - src/jobforge/external/llm/__init__.py
    - src/jobforge/external/llm/client.py
    - src/jobforge/external/models.py
    - pyproject.toml
  </files>
  <action>
1. **Add openai dependency to pyproject.toml**:
   - openai>=1.52.0 (Structured Outputs support)

2. **Extend external/models.py** with LLM response models:
   ```python
   from pydantic import BaseModel, Field

   class ImputedAttributeValue(BaseModel):
       """Single imputed attribute with confidence and rationale."""
       attribute_name: str = Field(description="Name of the attribute being imputed")
       value: str = Field(description="The imputed attribute value")
       confidence: float = Field(ge=0.0, le=1.0, description="LLM's confidence 0-1")
       rationale: str = Field(description="Explanation for the imputation")

   class ImputationResponse(BaseModel):
       """LLM response for attribute imputation batch."""
       imputations: list[ImputedAttributeValue]
       context_used: str = Field(description="Summary of context that influenced the answer")

   class LLMImputedAttribute(BaseModel):
       """Imputed attribute with full provenance for storage."""
       attribute_name: str
       value: str
       confidence: float
       rationale: str
       source_type: Literal["LLM"] = "LLM"
       model_used: str  # e.g., "gpt-4o-2024-08-06"
       imputed_at: datetime
   ```

3. **Create llm/client.py** - OpenAI client wrapper:
   ```python
   from openai import OpenAI

   LLM_IMPUTATION_MODEL = "gpt-4o-2024-08-06"  # Structured Outputs support

   class LLMClient:
       def __init__(self, api_key: str | None = None, model: str = LLM_IMPUTATION_MODEL):
           # Use OPENAI_API_KEY env var if not provided
           self.client = OpenAI(api_key=api_key or os.getenv("OPENAI_API_KEY"))
           self.model = model

       def parse(
           self,
           messages: list[dict],
           response_format: type[BaseModel],
           max_tokens: int = 1000,
       ) -> BaseModel:
           """Parse LLM response into Pydantic model using Structured Outputs."""
           completion = self.client.beta.chat.completions.parse(
               model=self.model,
               messages=messages,
               response_format=response_format,
               max_tokens=max_tokens,
           )
           return completion.choices[0].message.parsed

       def is_available(self) -> bool:
           return bool(os.getenv("OPENAI_API_KEY"))
   ```

4. **Package exports**:
   - llm/__init__.py: Export LLMClient
   - Update external/__init__.py: Export new models
  </action>
  <verify>
```bash
pip install "openai>=1.52.0"
python -c "from jobforge.external.llm import LLMClient; from jobforge.external.models import ImputationResponse"
```
  </verify>
  <done>LLMClient wraps OpenAI with Structured Outputs, response models defined with Pydantic</done>
</task>

<task type="auto">
  <name>Task 2: Create imputation prompts and service</name>
  <files>
    - src/jobforge/external/llm/prompts.py
    - src/jobforge/external/llm/service.py
    - src/jobforge/external/llm/__init__.py
  </files>
  <action>
1. **Create llm/prompts.py** - Prompt templates:
   ```python
   IMPUTATION_SYSTEM_PROMPT = """You are an expert workforce analyst helping to complete occupational attribute data.

   You will be given context about a job title including its job family, job function, and any known attributes.
   Your task is to impute values for missing attributes based on the occupational context.

   Guidelines:
   - Use the job title, family, and function to understand the occupational domain
   - Reference known attributes to maintain consistency
   - Provide a confidence score (0.0-1.0) reflecting your certainty
   - Include a brief rationale explaining your reasoning
   - If highly uncertain, still provide your best estimate with low confidence

   Your responses will be used to supplement authoritative data, so accuracy is important."""

   def build_imputation_prompt(
       job_title: str,
       job_family: str | None,
       job_function: str | None,
       unit_group: str | None,
       known_attributes: dict[str, str],
       missing_attributes: list[str],
   ) -> str:
       """Build the user prompt for attribute imputation."""
       context_parts = [f"Job Title: {job_title}"]
       if job_family:
           context_parts.append(f"Job Family: {job_family}")
       if job_function:
           context_parts.append(f"Job Function: {job_function}")
       if unit_group:
           context_parts.append(f"NOC Unit Group: {unit_group}")

       context = "\n".join(context_parts)

       known_str = "\n".join(f"- {k}: {v}" for k, v in known_attributes.items()) if known_attributes else "None"
       missing_str = ", ".join(missing_attributes)

       return f"""
   {context}

   Known Attributes:
   {known_str}

   Please impute values for these missing attributes: {missing_str}

   For each attribute, provide:
   - The imputed value
   - Your confidence (0.0-1.0)
   - A brief rationale
   """
   ```

2. **Create llm/service.py** - Imputation orchestration:
   ```python
   from datetime import datetime, timezone

   LLM_SOURCE_PRECEDENCE = 1  # Lowest precedence (per CONTEXT.md)

   class AttributeImputationService:
       def __init__(self, client: LLMClient | None = None):
           self.client = client or LLMClient()

       def impute_attributes(
           self,
           job_title: str,
           missing_attributes: list[str],
           job_family: str | None = None,
           job_function: str | None = None,
           unit_group: str | None = None,
           known_attributes: dict[str, str] | None = None,
       ) -> list[LLMImputedAttribute]:
           """Impute missing attributes using LLM.

           Per CONTEXT.md:
           - Accept ALL answers regardless of confidence
           - Store confidence + rationale for downstream filtering
           - Mark source_type='LLM' for provenance
           """
           if not missing_attributes:
               return []

           prompt = build_imputation_prompt(
               job_title=job_title,
               job_family=job_family,
               job_function=job_function,
               unit_group=unit_group,
               known_attributes=known_attributes or {},
               missing_attributes=missing_attributes,
           )

           messages = [
               {"role": "system", "content": IMPUTATION_SYSTEM_PROMPT},
               {"role": "user", "content": prompt},
           ]

           response = self.client.parse(
               messages=messages,
               response_format=ImputationResponse,
           )

           # Convert to LLMImputedAttribute with full provenance
           now = datetime.now(timezone.utc)
           return [
               LLMImputedAttribute(
                   attribute_name=imp.attribute_name,
                   value=imp.value,
                   confidence=imp.confidence,
                   rationale=imp.rationale,
                   model_used=self.client.model,
                   imputed_at=now,
               )
               for imp in response.imputations
           ]

   # Convenience function
   def impute_missing_attributes(
       job_title: str,
       missing_attributes: list[str],
       **kwargs,
   ) -> list[LLMImputedAttribute]:
       """One-liner for simple usage."""
       service = AttributeImputationService()
       return service.impute_attributes(job_title, missing_attributes, **kwargs)
   ```

3. **Update llm/__init__.py** exports:
   - Export: LLMClient, AttributeImputationService, impute_missing_attributes
   - Export: IMPUTATION_SYSTEM_PROMPT, build_imputation_prompt
  </action>
  <verify>
```python
from jobforge.external.llm import AttributeImputationService, build_imputation_prompt
prompt = build_imputation_prompt("Data Analyst", None, None, "21211", {"skill1": "Python"}, ["leadership", "communication"])
assert "Data Analyst" in prompt
assert "leadership" in prompt
```
  </verify>
  <done>Service accepts all LLM responses with confidence + rationale, marks source_type='LLM'</done>
</task>

<task type="auto">
  <name>Task 3: Create LLM imputation tests</name>
  <files>
    - tests/test_llm_imputation.py
  </files>
  <action>
Create comprehensive tests for LLM imputation:

1. **Prompt building tests** (no API key needed):
   - test_build_prompt_minimal: Just job title and missing attrs
   - test_build_prompt_full_context: All context fields populated
   - test_build_prompt_known_attributes: Known attrs included
   - test_build_prompt_empty_missing: Returns minimal prompt

2. **Client tests** (mock or skip):
   - test_client_is_available_no_key: Returns False
   - test_client_is_available_with_key: Returns True (mocked env)
   - Use pytest.mark.skipif for real API tests

3. **Service tests** (mock API responses):
   - test_service_imputes_single_attribute: Returns LLMImputedAttribute
   - test_service_imputes_multiple_attributes: Handles batch
   - test_service_empty_missing_returns_empty: [] for no missing
   - test_service_sets_provenance: source_type='LLM', model_used set
   - test_service_preserves_low_confidence: Doesn't filter by confidence
   - test_service_includes_rationale: Rationale field populated

4. **Response model tests**:
   - test_imputation_response_parses: Valid JSON -> ImputationResponse
   - test_imputed_attribute_confidence_bounds: 0.0-1.0 enforced
   - test_llm_imputed_attribute_timestamp: imputed_at set

5. **Mock fixtures**:
   ```python
   @pytest.fixture
   def mock_imputation_response():
       return ImputationResponse(
           imputations=[
               ImputedAttributeValue(
                   attribute_name="leadership",
                   value="Strong leadership skills required",
                   confidence=0.75,
                   rationale="Data analysts often lead cross-functional projects"
               )
           ],
           context_used="Job title and family context"
       )
   ```

Use unittest.mock.patch to mock LLMClient.parse for service tests.
  </action>
  <verify>
```bash
pytest tests/test_llm_imputation.py -v
```
All tests pass. Tests skip gracefully without API key.
  </verify>
  <done>15+ tests covering prompts, client, service, and models. CI-safe with mocks.</done>
</task>

</tasks>

<verification>
1. **Package structure**: `from jobforge.external.llm import LLMClient, AttributeImputationService, impute_missing_attributes`
2. **Response models**: `from jobforge.external.models import ImputationResponse, LLMImputedAttribute`
3. **Prompt building**: `build_imputation_prompt()` includes all context fields
4. **Test suite**: `pytest tests/test_llm_imputation.py` passes
5. **Full suite**: `pytest` passes (no regressions)
</verification>

<success_criteria>
- LLMClient uses OpenAI Structured Outputs with gpt-4o-2024-08-06
- AttributeImputationService accepts ALL responses regardless of confidence
- Every imputed value has confidence + rationale stored
- source_type='LLM' set on all imputed attributes
- model_used and imputed_at timestamp tracked
- Tests pass without requiring real API key (mocked)
</success_criteria>

<output>
After completion, create `.planning/phases/07-external-data-integration/07-02-SUMMARY.md`
</output>
