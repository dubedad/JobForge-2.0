---
phase: 07-external-data-integration
plan: 03
type: execute
wave: 1
depends_on: []
files_modified:
  - src/jobforge/external/tbs/__init__.py
  - src/jobforge/external/tbs/scraper.py
  - src/jobforge/external/tbs/parser.py
  - src/jobforge/external/tbs/models.py
  - src/jobforge/external/tbs/link_fetcher.py
  - src/jobforge/semantic/schema.py
  - tests/test_tbs_scraper.py
  - data/tbs/occupational_groups_en.json
  - data/tbs/occupational_groups_fr.json
  - data/tbs/linked_metadata_en.json
  - data/tbs/linked_metadata_fr.json
autonomous: true

must_haves:
  truths:
    - "TBS Occupational Groups page scraped with full provenance"
    - "Embedded links followed to retrieve occupational group metadata"
    - "Both English and French content scraped into separate files"
    - "Scraper fails loudly on page structure changes"
    - "DIM Occupations schema extended with scraped fields"
  artifacts:
    - path: "src/jobforge/external/tbs/scraper.py"
      provides: "TBS page scraping with provenance"
      exports: ["TBSScraper", "scrape_occupational_groups"]
    - path: "src/jobforge/external/tbs/parser.py"
      provides: "HTML table parsing"
      exports: ["parse_occupational_groups_table", "extract_embedded_links"]
    - path: "src/jobforge/external/tbs/link_fetcher.py"
      provides: "Embedded link traversal and metadata extraction"
      exports: ["LinkMetadataFetcher", "fetch_linked_metadata"]
    - path: "src/jobforge/external/tbs/models.py"
      provides: "Pydantic models for scraped data"
      exports: ["OccupationalGroupRow", "ScrapedPage", "ScrapedProvenance", "LinkedPageMetadata"]
    - path: "data/tbs/occupational_groups_en.json"
      provides: "Scraped English occupational groups data"
      contains: "group_code, group_name, definition_url fields"
    - path: "data/tbs/occupational_groups_fr.json"
      provides: "Scraped French occupational groups data"
      contains: "group_code, group_name, definition_url fields"
    - path: "data/tbs/linked_metadata_en.json"
      provides: "Metadata fetched from embedded links (EN)"
      contains: "definition_content, qualification_content, job_eval_content"
    - path: "data/tbs/linked_metadata_fr.json"
      provides: "Metadata fetched from embedded links (FR)"
      contains: "definition_content, qualification_content, job_eval_content"
  key_links:
    - from: "src/jobforge/external/tbs/scraper.py"
      to: "src/jobforge/external/tbs/parser.py"
      via: "parse_occupational_groups_table"
      pattern: "parser\\.parse_occupational_groups_table"
    - from: "src/jobforge/external/tbs/link_fetcher.py"
      to: "src/jobforge/external/tbs/parser.py"
      via: "extract_embedded_links"
      pattern: "extract_embedded_links"
    - from: "src/jobforge/semantic/schema.py"
      to: "data/tbs/occupational_groups_en.json"
      via: "DIM_Occupations extended fields"
      pattern: "tbs_group_code|tbs_definition_url"
---

<objective>
Scrape TBS Occupational Groups page with embedded link traversal and extend DIM Occupations schema.

Purpose: Enrich WiQ with Government of Canada occupational group metadata including definitions, evaluation standards, and qualification standards. All scraped data must carry full provenance (URL, timestamp, extraction method). Per CONTEXT.md: traverse two levels deep from main page.

Output: Working `jobforge.external.tbs` package with scraper, parser, link fetcher, and models. Scraped data saved to JSON files including metadata from linked pages. DIM Occupations schema extended with TBS fields.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/07-external-data-integration/07-RESEARCH.md
@.planning/phases/07-external-data-integration/07-CONTEXT.md
@src/jobforge/semantic/schema.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create TBS models and HTML parser</name>
  <files>
    - src/jobforge/external/tbs/__init__.py
    - src/jobforge/external/tbs/models.py
    - src/jobforge/external/tbs/parser.py
    - pyproject.toml
  </files>
  <action>
1. **Add dependencies to pyproject.toml**:
   - beautifulsoup4>=4.12.0 (HTML parsing)
   - lxml>=4.9.0 (fast HTML parser backend)
   - requests>=2.31.0 (sync HTTP - already likely present)

2. **Create tbs/models.py** - Pydantic models for scraped data:
   ```python
   from datetime import datetime
   from pydantic import BaseModel, Field, HttpUrl

   class ScrapedProvenance(BaseModel):
       """Provenance for any scraped value."""
       source_url: str
       scraped_at: datetime
       extraction_method: str  # "table_cell", "link_href", "link_text"
       page_title: str

   class OccupationalGroupRow(BaseModel):
       """Single row from TBS occupational groups table."""
       group_abbrev: str = Field(description="Group abbreviation (e.g., 'AI')")
       group_code: str = Field(description="Numeric code")
       group_name: str = Field(description="Full occupational group name")
       subgroup: str | None = Field(default=None, description="Subgroup if applicable")
       definition_url: str | None = Field(default=None, description="Link to definition page")
       job_eval_standard_url: str | None = Field(default=None, description="Link to job evaluation standard")
       qualification_standard_url: str | None = Field(default=None, description="Link to qualification standard")
       provenance: ScrapedProvenance

   class ScrapedPage(BaseModel):
       """Container for full page scrape result."""
       url: str
       language: str  # "en" or "fr"
       title: str
       scraped_at: datetime
       rows: list[OccupationalGroupRow]
       link_count: int = Field(description="Number of embedded links found")
       row_count: int = Field(description="Number of data rows extracted")
   ```

3. **Create tbs/parser.py** - HTML parsing logic:
   ```python
   from bs4 import BeautifulSoup

   TBS_EXPECTED_COLUMNS = [
       "Group abbreviation", "Code", "Occupational Group",
       "Group", "Subgroup", "Definition", "Job evaluation standard",
       "Qualification standard"
   ]  # First 8 columns - ignore pay/agreement columns

   def validate_table_structure(table: Tag) -> bool:
       """Validate table has expected columns. Raises if structure changed."""
       headers = [th.text.strip() for th in table.find_all("th")]
       # Check first 6 required columns present
       required = ["Group abbreviation", "Code", "Occupational Group"]
       for col in required:
           if not any(col.lower() in h.lower() for h in headers):
               raise ValueError(f"Missing expected column: {col}. Page structure may have changed.")
       return True

   def parse_occupational_groups_table(
       html: str,
       source_url: str,
       scraped_at: datetime,
   ) -> list[OccupationalGroupRow]:
       """Parse TBS occupational groups HTML table.

       Per CONTEXT.md: Fail loudly on structure changes.
       """
       soup = BeautifulSoup(html, "lxml")
       table = soup.find("table")

       if not table:
           raise ValueError(f"No table found at {source_url} - page structure may have changed")

       validate_table_structure(table)

       page_title = soup.find("title").text.strip() if soup.find("title") else "Unknown"
       rows = []

       for tr in table.find_all("tr")[1:]:  # Skip header row
           cells = tr.find_all("td")
           if len(cells) < 6:
               continue  # Skip malformed rows

           # Extract cell values
           group_abbrev = cells[0].text.strip()
           code = cells[1].text.strip()
           group_name = cells[2].text.strip()
           group = cells[3].text.strip() if len(cells) > 3 else None
           subgroup = cells[4].text.strip() if len(cells) > 4 and cells[4].text.strip() else None

           # Extract links (check for "Yes" links)
           def extract_link(cell_idx: int) -> str | None:
               if len(cells) > cell_idx:
                   link = cells[cell_idx].find("a")
                   if link and link.get("href"):
                       href = link["href"]
                       # Make absolute URL if relative
                       if href.startswith("/"):
                           href = "https://www.canada.ca" + href
                       return href
               return None

           rows.append(OccupationalGroupRow(
               group_abbrev=group_abbrev,
               group_code=code,
               group_name=group_name,
               subgroup=subgroup,
               definition_url=extract_link(5),
               job_eval_standard_url=extract_link(6),
               qualification_standard_url=extract_link(7),
               provenance=ScrapedProvenance(
                   source_url=source_url,
                   scraped_at=scraped_at,
                   extraction_method="table_cell",
                   page_title=page_title,
               ),
           ))

       return rows

   def extract_embedded_links(rows: list[OccupationalGroupRow]) -> dict[str, list[str]]:
       """Extract all embedded links by type for follow-up scraping."""
       return {
           "definitions": [r.definition_url for r in rows if r.definition_url],
           "job_eval_standards": [r.job_eval_standard_url for r in rows if r.job_eval_standard_url],
           "qualification_standards": [r.qualification_standard_url for r in rows if r.qualification_standard_url],
       }
   ```

4. **Package exports in tbs/__init__.py**
  </action>
  <verify>
```python
from jobforge.external.tbs.models import OccupationalGroupRow, ScrapedProvenance
from jobforge.external.tbs.parser import parse_occupational_groups_table, validate_table_structure
```
  </verify>
  <done>Parser validates table structure and fails loudly on changes, extracts all columns with provenance</done>
</task>

<task type="auto">
  <name>Task 2: Create TBS scraper with bilingual support</name>
  <files>
    - src/jobforge/external/tbs/scraper.py
    - data/tbs/occupational_groups_en.json
    - data/tbs/occupational_groups_fr.json
  </files>
  <action>
1. **Create tbs/scraper.py** - Main scraping orchestration:
   ```python
   import json
   from datetime import datetime, timezone
   from pathlib import Path
   import requests
   import structlog

   logger = structlog.get_logger(__name__)

   TBS_URLS = {
       "en": "https://www.canada.ca/en/treasury-board-secretariat/services/collective-agreements/occupational-groups.html",
       "fr": "https://www.canada.ca/fr/secretariat-conseil-tresor/services/conventions-collectives/groupes-professionnels.html",
   }

   class TBSScraper:
       """TBS Occupational Groups scraper with provenance tracking."""

       def __init__(self, output_dir: str | Path = "data/tbs"):
           self.output_dir = Path(output_dir)
           self.output_dir.mkdir(parents=True, exist_ok=True)

       def scrape_page(self, language: str = "en", timeout: int = 30) -> ScrapedPage:
           """Scrape TBS occupational groups page for given language.

           Per CONTEXT.md: Fail loudly on structure changes.
           """
           url = TBS_URLS.get(language)
           if not url:
               raise ValueError(f"Unsupported language: {language}. Use 'en' or 'fr'.")

           logger.info("Scraping TBS page", url=url, language=language)

           response = requests.get(url, timeout=timeout)
           response.raise_for_status()

           scraped_at = datetime.now(timezone.utc)
           rows = parse_occupational_groups_table(response.text, url, scraped_at)

           links = extract_embedded_links(rows)
           link_count = sum(len(v) for v in links.values())

           logger.info(
               "Scraped TBS page successfully",
               language=language,
               row_count=len(rows),
               link_count=link_count,
           )

           return ScrapedPage(
               url=url,
               language=language,
               title=f"TBS Occupational Groups ({language.upper()})",
               scraped_at=scraped_at,
               rows=rows,
               link_count=link_count,
               row_count=len(rows),
           )

       def scrape_both_languages(self) -> dict[str, ScrapedPage]:
           """Scrape both English and French pages per CONTEXT.md."""
           return {
               "en": self.scrape_page("en"),
               "fr": self.scrape_page("fr"),
           }

       def save_to_json(self, page: ScrapedPage) -> Path:
           """Save scraped page to JSON file with provenance."""
           filename = f"occupational_groups_{page.language}.json"
           filepath = self.output_dir / filename

           # Convert to JSON-serializable format
           data = page.model_dump(mode="json")

           with open(filepath, "w", encoding="utf-8") as f:
               json.dump(data, f, indent=2, ensure_ascii=False)

           logger.info("Saved scraped data", filepath=str(filepath))
           return filepath

       def scrape_and_save(self) -> dict[str, Path]:
           """Scrape both languages and save to JSON files."""
           pages = self.scrape_both_languages()
           return {
               lang: self.save_to_json(page)
               for lang, page in pages.items()
           }

   # Convenience function
   def scrape_occupational_groups(
       output_dir: str | Path = "data/tbs",
       languages: list[str] | None = None,
   ) -> dict[str, Path]:
       """Scrape TBS pages and save to JSON. Returns paths to saved files."""
       scraper = TBSScraper(output_dir)
       languages = languages or ["en", "fr"]

       paths = {}
       for lang in languages:
           page = scraper.scrape_page(lang)
           paths[lang] = scraper.save_to_json(page)

       return paths
   ```

2. **Create initial data files** by running scraper:
   - Run scraper once to create data/tbs/occupational_groups_en.json
   - Run scraper once to create data/tbs/occupational_groups_fr.json
   - Commit the scraped data as baseline

3. **Update tbs/__init__.py** exports:
   - TBSScraper, scrape_occupational_groups
  </action>
  <verify>
```python
from jobforge.external.tbs import TBSScraper, scrape_occupational_groups
# Quick test without network:
from pathlib import Path
assert Path("data/tbs").exists() or True  # Will create on scrape
```

```bash
# Run scraper to create initial data (requires network)
python -c "from jobforge.external.tbs import scrape_occupational_groups; print(scrape_occupational_groups())"
ls data/tbs/*.json
```
  </verify>
  <done>Scraper fetches both EN/FR pages, saves with provenance, fails loudly on structure changes</done>
</task>

<task type="auto">
  <name>Task 3: Create link fetcher to traverse embedded URLs</name>
  <files>
    - src/jobforge/external/tbs/models.py
    - src/jobforge/external/tbs/link_fetcher.py
    - src/jobforge/external/tbs/__init__.py
    - data/tbs/linked_metadata_en.json
    - data/tbs/linked_metadata_fr.json
  </files>
  <action>
Per CONTEXT.md: "Traverse two levels deep from main page" and requirement SRC-02: embedded links must be FOLLOWED, not just extracted.

1. **Extend tbs/models.py** with linked page metadata model:
   ```python
   class LinkedPageContent(BaseModel):
       """Content extracted from a linked page (definition, qual standard, etc.)."""
       title: str = Field(description="Page title")
       main_content: str = Field(description="Primary content text (definition, standard text)")
       effective_date: str | None = Field(default=None, description="Effective date if present")
       last_modified: str | None = Field(default=None, description="Page last modified date")

   class LinkedPageMetadata(BaseModel):
       """Metadata fetched from an embedded link."""
       group_abbrev: str = Field(description="Parent group abbreviation")
       link_type: str = Field(description="'definition' | 'job_eval_standard' | 'qualification_standard'")
       url: str = Field(description="URL that was fetched")
       content: LinkedPageContent | None = Field(default=None, description="Extracted content, None if fetch failed")
       fetch_status: str = Field(description="'success' | 'failed' | 'not_found'")
       error_message: str | None = Field(default=None, description="Error message if fetch failed")
       provenance: ScrapedProvenance

   class LinkedMetadataCollection(BaseModel):
       """Collection of all linked page metadata for a language."""
       language: str
       fetched_at: datetime
       total_links: int
       successful_fetches: int
       failed_fetches: int
       metadata: list[LinkedPageMetadata]
   ```

2. **Create tbs/link_fetcher.py** - Traverse and fetch embedded links:
   ```python
   import json
   import time
   from datetime import datetime, timezone
   from pathlib import Path
   import requests
   from bs4 import BeautifulSoup
   import structlog

   from .models import (
       OccupationalGroupRow, LinkedPageMetadata, LinkedPageContent,
       LinkedMetadataCollection, ScrapedProvenance
   )
   from .parser import extract_embedded_links

   logger = structlog.get_logger(__name__)

   # Polite delay between requests to avoid hammering canada.ca
   REQUEST_DELAY_SECONDS = 1.0

   class LinkMetadataFetcher:
       """Fetches and parses metadata from embedded TBS links."""

       def __init__(self, output_dir: str | Path = "data/tbs"):
           self.output_dir = Path(output_dir)
           self.output_dir.mkdir(parents=True, exist_ok=True)

       def _parse_linked_page(self, html: str, url: str) -> LinkedPageContent:
           """Extract structured content from a linked page.

           TBS pages typically have:
           - h1 for title
           - main content in article or main tag
           - dates in metadata or visible text
           """
           soup = BeautifulSoup(html, "lxml")

           # Extract title
           title_tag = soup.find("h1") or soup.find("title")
           title = title_tag.text.strip() if title_tag else "Unknown"

           # Extract main content - try article, main, or content div
           content_tag = (
               soup.find("article") or
               soup.find("main") or
               soup.find("div", {"class": "mwsgeneric-base-html"}) or
               soup.find("div", {"id": "content"})
           )

           if content_tag:
               # Get text, clean up whitespace
               paragraphs = content_tag.find_all(["p", "li"])
               main_content = "\n\n".join(
                   p.text.strip() for p in paragraphs if p.text.strip()
               )
           else:
               # Fallback: get body text
               main_content = soup.get_text(separator="\n", strip=True)[:5000]

           # Try to find dates
           effective_date = None
           last_modified = None

           # Look for "Date modified" pattern common on canada.ca
           date_dl = soup.find("dl", {"id": "wb-dtmd"})
           if date_dl:
               dd = date_dl.find("dd")
               if dd:
                   last_modified = dd.text.strip()

           # Look for "Effective date" in content
           for text in soup.stripped_strings:
               if "effective" in text.lower() and any(c.isdigit() for c in text):
                   effective_date = text
                   break

           return LinkedPageContent(
               title=title,
               main_content=main_content[:10000],  # Cap content length
               effective_date=effective_date,
               last_modified=last_modified,
           )

       def fetch_single_link(
           self,
           url: str,
           group_abbrev: str,
           link_type: str,
           timeout: int = 30,
       ) -> LinkedPageMetadata:
           """Fetch and parse a single linked page."""
           scraped_at = datetime.now(timezone.utc)
           provenance = ScrapedProvenance(
               source_url=url,
               scraped_at=scraped_at,
               extraction_method="linked_page_content",
               page_title="",  # Will be updated after fetch
           )

           try:
               logger.debug("Fetching linked page", url=url, link_type=link_type)
               response = requests.get(url, timeout=timeout)
               response.raise_for_status()

               content = self._parse_linked_page(response.text, url)
               provenance.page_title = content.title

               return LinkedPageMetadata(
                   group_abbrev=group_abbrev,
                   link_type=link_type,
                   url=url,
                   content=content,
                   fetch_status="success",
                   error_message=None,
                   provenance=provenance,
               )

           except requests.exceptions.HTTPError as e:
               status = e.response.status_code if e.response else "unknown"
               logger.warning("HTTP error fetching link", url=url, status=status)
               return LinkedPageMetadata(
                   group_abbrev=group_abbrev,
                   link_type=link_type,
                   url=url,
                   content=None,
                   fetch_status="not_found" if status == 404 else "failed",
                   error_message=f"HTTP {status}",
                   provenance=provenance,
               )

           except Exception as e:
               logger.warning("Error fetching link", url=url, error=str(e))
               return LinkedPageMetadata(
                   group_abbrev=group_abbrev,
                   link_type=link_type,
                   url=url,
                   content=None,
                   fetch_status="failed",
                   error_message=str(e),
                   provenance=provenance,
               )

       def fetch_all_links(
           self,
           rows: list[OccupationalGroupRow],
           language: str = "en",
       ) -> LinkedMetadataCollection:
           """Fetch metadata from all embedded links in scraped rows.

           Per CONTEXT.md: Traverse two levels deep from main page.
           This is level 2 - following links from the main table.
           """
           logger.info(
               "Fetching linked metadata",
               language=language,
               row_count=len(rows),
           )

           all_metadata: list[LinkedPageMetadata] = []
           successful = 0
           failed = 0

           for row in rows:
               # Fetch each link type if present
               links_to_fetch = [
                   (row.definition_url, "definition"),
                   (row.job_eval_standard_url, "job_eval_standard"),
                   (row.qualification_standard_url, "qualification_standard"),
               ]

               for url, link_type in links_to_fetch:
                   if not url:
                       continue

                   metadata = self.fetch_single_link(
                       url=url,
                       group_abbrev=row.group_abbrev,
                       link_type=link_type,
                   )
                   all_metadata.append(metadata)

                   if metadata.fetch_status == "success":
                       successful += 1
                   else:
                       failed += 1

                   # Polite delay between requests
                   time.sleep(REQUEST_DELAY_SECONDS)

           logger.info(
               "Completed fetching linked metadata",
               language=language,
               total=len(all_metadata),
               successful=successful,
               failed=failed,
           )

           return LinkedMetadataCollection(
               language=language,
               fetched_at=datetime.now(timezone.utc),
               total_links=len(all_metadata),
               successful_fetches=successful,
               failed_fetches=failed,
               metadata=all_metadata,
           )

       def save_to_json(self, collection: LinkedMetadataCollection) -> Path:
           """Save linked metadata to JSON file."""
           filename = f"linked_metadata_{collection.language}.json"
           filepath = self.output_dir / filename

           data = collection.model_dump(mode="json")

           with open(filepath, "w", encoding="utf-8") as f:
               json.dump(data, f, indent=2, ensure_ascii=False)

           logger.info("Saved linked metadata", filepath=str(filepath))
           return filepath


   # Convenience function
   def fetch_linked_metadata(
       scraped_data_path: str | Path,
       output_dir: str | Path = "data/tbs",
   ) -> Path:
       """Fetch metadata from all links in a scraped occupational groups file.

       Args:
           scraped_data_path: Path to occupational_groups_{lang}.json
           output_dir: Where to save linked_metadata_{lang}.json

       Returns:
           Path to saved linked metadata file
       """
       import json

       with open(scraped_data_path, "r", encoding="utf-8") as f:
           data = json.load(f)

       # Reconstruct rows from JSON
       rows = [OccupationalGroupRow(**r) for r in data["rows"]]
       language = data["language"]

       fetcher = LinkMetadataFetcher(output_dir)
       collection = fetcher.fetch_all_links(rows, language)
       return fetcher.save_to_json(collection)
   ```

3. **Update tbs/__init__.py** exports:
   ```python
   from .link_fetcher import LinkMetadataFetcher, fetch_linked_metadata
   from .models import LinkedPageMetadata, LinkedMetadataCollection
   ```

4. **Run link fetcher to create initial data files**:
   - After scraper creates occupational_groups_en.json, run:
     `fetch_linked_metadata("data/tbs/occupational_groups_en.json")`
   - Repeat for French
   - This creates linked_metadata_en.json and linked_metadata_fr.json
   - Commit as baseline

Note: This will make many HTTP requests to canada.ca (one per link). Use REQUEST_DELAY_SECONDS to be polite. Expect ~50-100 requests per language.
  </action>
  <verify>
```python
from jobforge.external.tbs import LinkMetadataFetcher, fetch_linked_metadata
from jobforge.external.tbs.models import LinkedPageMetadata, LinkedMetadataCollection
```

```bash
# Run link fetcher after scraper has created base files (requires network)
python -c "
from jobforge.external.tbs import fetch_linked_metadata
from pathlib import Path
if Path('data/tbs/occupational_groups_en.json').exists():
    print(fetch_linked_metadata('data/tbs/occupational_groups_en.json'))
"
ls data/tbs/linked_metadata_*.json
```
  </verify>
  <done>Link fetcher traverses embedded URLs, extracts definition/standard content, saves with provenance. Handles failures gracefully with status tracking.</done>
</task>

<task type="auto">
  <name>Task 4: Extend DIM Occupations schema and create tests</name>
  <files>
    - src/jobforge/semantic/schema.py
    - tests/test_tbs_scraper.py
  </files>
  <action>
1. **Extend DIM Occupations in schema.py**:

   Add TBS fields to the existing DIM_Occupations table definition:
   ```python
   # In the DIM_Occupations columns list, add:
   "tbs_group_code": "TEXT",           # TBS occupational group code
   "tbs_group_abbrev": "TEXT",         # Group abbreviation (AI, CR, etc.)
   "tbs_group_name": "TEXT",           # Full group name
   "tbs_definition_url": "TEXT",       # Link to definition page
   "tbs_definition_content": "TEXT",   # Content fetched from definition page
   "tbs_job_eval_standard_url": "TEXT", # Link to job eval standard
   "tbs_job_eval_content": "TEXT",     # Content fetched from eval standard page
   "tbs_qualification_standard_url": "TEXT", # Link to qualification standard
   "tbs_qualification_content": "TEXT", # Content fetched from qual standard page
   "tbs_scraped_at": "TIMESTAMP",      # When TBS data was scraped
   ```

   These fields enable gold layer queries joining occupations to TBS metadata including fetched content.

2. **Create tests/test_tbs_scraper.py**:

   **Parser tests** (no network needed):
   - test_parse_valid_html: Mock HTML parses correctly
   - test_parse_extracts_links: Definition/eval/qual URLs extracted
   - test_parse_fails_on_missing_table: Raises ValueError
   - test_parse_fails_on_wrong_columns: Raises ValueError (structure validation)
   - test_validate_table_structure_valid: Returns True for valid table
   - test_validate_table_structure_invalid: Raises for missing columns
   - test_extract_embedded_links: Returns dict with link lists

   **Model tests**:
   - test_occupational_group_row_validates: Valid data creates model
   - test_scraped_provenance_required_fields: All provenance fields required
   - test_scraped_page_counts: row_count and link_count computed
   - test_linked_page_metadata_success: Success case creates valid model
   - test_linked_page_metadata_failure: Failure case stores error

   **Scraper tests** (mock network):
   - test_scraper_creates_output_dir: Directory created if missing
   - test_scraper_saves_json: JSON file written with correct structure
   - test_scraper_bilingual: Both en/fr URLs used
   - test_scraper_provenance_in_output: scraped_at timestamp in JSON

   **Link fetcher tests** (mock network):
   - test_link_fetcher_parses_content: Extracts title and main_content
   - test_link_fetcher_handles_404: Returns not_found status
   - test_link_fetcher_handles_timeout: Returns failed status with error
   - test_link_fetcher_collection_counts: total/successful/failed counts correct
   - test_fetch_linked_metadata_convenience: Function loads JSON and fetches

   **Integration test** (skip if no network):
   - test_scrape_real_page: Actually scrapes TBS (pytest.mark.integration)
   - test_fetch_real_link: Actually fetches one definition page (pytest.mark.integration)

   **Mock fixtures**:
   ```python
   MOCK_TBS_HTML = """
   <html><head><title>Occupational Groups</title></head>
   <body>
   <table>
   <tr><th>Group abbreviation</th><th>Code</th><th>Occupational Group</th>
       <th>Group</th><th>Subgroup</th><th>Definition</th><th>Job evaluation standard</th></tr>
   <tr><td>AI</td><td>001</td><td>Air Traffic Control</td>
       <td>AI</td><td></td><td><a href="/en/def/ai">Yes</a></td><td><a href="/en/eval/ai">Yes</a></td></tr>
   </table>
   </body></html>
   """

   MOCK_DEFINITION_HTML = """
   <html><head><title>AI Group Definition</title></head>
   <body>
   <h1>Air Traffic Control Group (AI)</h1>
   <article>
   <p>The Air Traffic Control Group comprises positions...</p>
   <p>Effective date: 2023-01-01</p>
   </article>
   <dl id="wb-dtmd"><dd>2023-11-07</dd></dl>
   </body></html>
   """
   ```

3. **Schema test**:
   - test_dim_occupations_has_tbs_fields: Verify new columns in schema including content fields
  </action>
  <verify>
```bash
pytest tests/test_tbs_scraper.py -v
pytest tests/test_semantic.py -v -k "tbs" 2>/dev/null || echo "Schema tests in main test file"
```
All tests pass. Integration tests skip without network.
  </verify>
  <done>DIM Occupations extended with TBS fields including fetched content, 20+ tests covering parser, scraper, link fetcher, and schema</done>
</task>

</tasks>

<verification>
1. **Package structure**: `from jobforge.external.tbs import TBSScraper, scrape_occupational_groups, LinkMetadataFetcher, fetch_linked_metadata`
2. **Data files exist**: `data/tbs/occupational_groups_en.json`, `data/tbs/occupational_groups_fr.json`
3. **Linked metadata files exist**: `data/tbs/linked_metadata_en.json`, `data/tbs/linked_metadata_fr.json`
4. **Provenance in data**: JSON files contain `scraped_at`, `source_url`, `extraction_method`
5. **Linked content**: linked_metadata files contain `content.main_content` from followed URLs
6. **Schema extended**: DIM_Occupations has `tbs_group_code`, `tbs_definition_url`, `tbs_definition_content`, etc.
7. **Test suite**: `pytest tests/test_tbs_scraper.py` passes
8. **Full suite**: `pytest` passes (no regressions)
</verification>

<success_criteria>
- TBSScraper fetches EN and FR pages from canada.ca
- Parser validates table structure and fails loudly on changes
- All scraped values carry provenance (URL, timestamp, method)
- Embedded links extracted (definitions, job eval, qualifications)
- **Embedded links FOLLOWED to fetch actual content (requirement SRC-02)**
- Linked page content parsed (title, main text, dates)
- JSON files saved with full provenance metadata
- DIM_Occupations schema extended with TBS fields including content
- Tests pass with mocked HTML (no network dependency for CI)
</success_criteria>

<output>
After completion, create `.planning/phases/07-external-data-integration/07-03-SUMMARY.md`
</output>
