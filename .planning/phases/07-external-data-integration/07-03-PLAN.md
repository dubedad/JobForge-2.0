---
phase: 07-external-data-integration
plan: 03
type: execute
wave: 1
depends_on: []
files_modified:
  - src/jobforge/external/tbs/__init__.py
  - src/jobforge/external/tbs/scraper.py
  - src/jobforge/external/tbs/parser.py
  - src/jobforge/external/tbs/models.py
  - src/jobforge/semantic/schema.py
  - tests/test_tbs_scraper.py
  - data/tbs/occupational_groups_en.json
  - data/tbs/occupational_groups_fr.json
autonomous: true

must_haves:
  truths:
    - "TBS Occupational Groups page scraped with full provenance"
    - "Embedded links followed to retrieve occupational group metadata"
    - "Both English and French content scraped into separate files"
    - "Scraper fails loudly on page structure changes"
    - "DIM Occupations schema extended with scraped fields"
  artifacts:
    - path: "src/jobforge/external/tbs/scraper.py"
      provides: "TBS page scraping with provenance"
      exports: ["TBSScraper", "scrape_occupational_groups"]
    - path: "src/jobforge/external/tbs/parser.py"
      provides: "HTML table parsing"
      exports: ["parse_occupational_groups_table", "extract_embedded_links"]
    - path: "src/jobforge/external/tbs/models.py"
      provides: "Pydantic models for scraped data"
      exports: ["OccupationalGroupRow", "ScrapedPage", "ScrapedProvenance"]
    - path: "data/tbs/occupational_groups_en.json"
      provides: "Scraped English occupational groups data"
      contains: "group_code, group_name, definition_url fields"
    - path: "data/tbs/occupational_groups_fr.json"
      provides: "Scraped French occupational groups data"
      contains: "group_code, group_name, definition_url fields"
  key_links:
    - from: "src/jobforge/external/tbs/scraper.py"
      to: "src/jobforge/external/tbs/parser.py"
      via: "parse_occupational_groups_table"
      pattern: "parser\\.parse_occupational_groups_table"
    - from: "src/jobforge/semantic/schema.py"
      to: "data/tbs/occupational_groups_en.json"
      via: "DIM_Occupations extended fields"
      pattern: "tbs_group_code|tbs_definition_url"
---

<objective>
Scrape TBS Occupational Groups page with embedded link traversal and extend DIM Occupations schema.

Purpose: Enrich WiQ with Government of Canada occupational group metadata including definitions, evaluation standards, and qualification standards. All scraped data must carry full provenance (URL, timestamp, extraction method).

Output: Working `jobforge.external.tbs` package with scraper, parser, and models. Scraped data saved to JSON files. DIM Occupations schema extended with TBS fields.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/07-external-data-integration/07-RESEARCH.md
@.planning/phases/07-external-data-integration/07-CONTEXT.md
@src/jobforge/semantic/schema.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create TBS models and HTML parser</name>
  <files>
    - src/jobforge/external/tbs/__init__.py
    - src/jobforge/external/tbs/models.py
    - src/jobforge/external/tbs/parser.py
    - pyproject.toml
  </files>
  <action>
1. **Add dependencies to pyproject.toml**:
   - beautifulsoup4>=4.12.0 (HTML parsing)
   - lxml>=4.9.0 (fast HTML parser backend)
   - requests>=2.31.0 (sync HTTP - already likely present)

2. **Create tbs/models.py** - Pydantic models for scraped data:
   ```python
   from datetime import datetime
   from pydantic import BaseModel, Field, HttpUrl

   class ScrapedProvenance(BaseModel):
       """Provenance for any scraped value."""
       source_url: str
       scraped_at: datetime
       extraction_method: str  # "table_cell", "link_href", "link_text"
       page_title: str

   class OccupationalGroupRow(BaseModel):
       """Single row from TBS occupational groups table."""
       group_abbrev: str = Field(description="Group abbreviation (e.g., 'AI')")
       group_code: str = Field(description="Numeric code")
       group_name: str = Field(description="Full occupational group name")
       subgroup: str | None = Field(default=None, description="Subgroup if applicable")
       definition_url: str | None = Field(default=None, description="Link to definition page")
       job_eval_standard_url: str | None = Field(default=None, description="Link to job evaluation standard")
       qualification_standard_url: str | None = Field(default=None, description="Link to qualification standard")
       provenance: ScrapedProvenance

   class ScrapedPage(BaseModel):
       """Container for full page scrape result."""
       url: str
       language: str  # "en" or "fr"
       title: str
       scraped_at: datetime
       rows: list[OccupationalGroupRow]
       link_count: int = Field(description="Number of embedded links found")
       row_count: int = Field(description="Number of data rows extracted")
   ```

3. **Create tbs/parser.py** - HTML parsing logic:
   ```python
   from bs4 import BeautifulSoup

   TBS_EXPECTED_COLUMNS = [
       "Group abbreviation", "Code", "Occupational Group",
       "Group", "Subgroup", "Definition", "Job evaluation standard",
       "Qualification standard"
   ]  # First 8 columns - ignore pay/agreement columns

   def validate_table_structure(table: Tag) -> bool:
       """Validate table has expected columns. Raises if structure changed."""
       headers = [th.text.strip() for th in table.find_all("th")]
       # Check first 6 required columns present
       required = ["Group abbreviation", "Code", "Occupational Group"]
       for col in required:
           if not any(col.lower() in h.lower() for h in headers):
               raise ValueError(f"Missing expected column: {col}. Page structure may have changed.")
       return True

   def parse_occupational_groups_table(
       html: str,
       source_url: str,
       scraped_at: datetime,
   ) -> list[OccupationalGroupRow]:
       """Parse TBS occupational groups HTML table.

       Per CONTEXT.md: Fail loudly on structure changes.
       """
       soup = BeautifulSoup(html, "lxml")
       table = soup.find("table")

       if not table:
           raise ValueError(f"No table found at {source_url} - page structure may have changed")

       validate_table_structure(table)

       page_title = soup.find("title").text.strip() if soup.find("title") else "Unknown"
       rows = []

       for tr in table.find_all("tr")[1:]:  # Skip header row
           cells = tr.find_all("td")
           if len(cells) < 6:
               continue  # Skip malformed rows

           # Extract cell values
           group_abbrev = cells[0].text.strip()
           code = cells[1].text.strip()
           group_name = cells[2].text.strip()
           group = cells[3].text.strip() if len(cells) > 3 else None
           subgroup = cells[4].text.strip() if len(cells) > 4 and cells[4].text.strip() else None

           # Extract links (check for "Yes" links)
           def extract_link(cell_idx: int) -> str | None:
               if len(cells) > cell_idx:
                   link = cells[cell_idx].find("a")
                   if link and link.get("href"):
                       href = link["href"]
                       # Make absolute URL if relative
                       if href.startswith("/"):
                           href = "https://www.canada.ca" + href
                       return href
               return None

           rows.append(OccupationalGroupRow(
               group_abbrev=group_abbrev,
               group_code=code,
               group_name=group_name,
               subgroup=subgroup,
               definition_url=extract_link(5),
               job_eval_standard_url=extract_link(6),
               qualification_standard_url=extract_link(7),
               provenance=ScrapedProvenance(
                   source_url=source_url,
                   scraped_at=scraped_at,
                   extraction_method="table_cell",
                   page_title=page_title,
               ),
           ))

       return rows

   def extract_embedded_links(rows: list[OccupationalGroupRow]) -> dict[str, list[str]]:
       """Extract all embedded links by type for follow-up scraping."""
       return {
           "definitions": [r.definition_url for r in rows if r.definition_url],
           "job_eval_standards": [r.job_eval_standard_url for r in rows if r.job_eval_standard_url],
           "qualification_standards": [r.qualification_standard_url for r in rows if r.qualification_standard_url],
       }
   ```

4. **Package exports in tbs/__init__.py**
  </action>
  <verify>
```python
from jobforge.external.tbs.models import OccupationalGroupRow, ScrapedProvenance
from jobforge.external.tbs.parser import parse_occupational_groups_table, validate_table_structure
```
  </verify>
  <done>Parser validates table structure and fails loudly on changes, extracts all columns with provenance</done>
</task>

<task type="auto">
  <name>Task 2: Create TBS scraper with bilingual support</name>
  <files>
    - src/jobforge/external/tbs/scraper.py
    - data/tbs/occupational_groups_en.json
    - data/tbs/occupational_groups_fr.json
  </files>
  <action>
1. **Create tbs/scraper.py** - Main scraping orchestration:
   ```python
   import json
   from datetime import datetime, timezone
   from pathlib import Path
   import requests
   import structlog

   logger = structlog.get_logger(__name__)

   TBS_URLS = {
       "en": "https://www.canada.ca/en/treasury-board-secretariat/services/collective-agreements/occupational-groups.html",
       "fr": "https://www.canada.ca/fr/secretariat-conseil-tresor/services/conventions-collectives/groupes-professionnels.html",
   }

   class TBSScraper:
       """TBS Occupational Groups scraper with provenance tracking."""

       def __init__(self, output_dir: str | Path = "data/tbs"):
           self.output_dir = Path(output_dir)
           self.output_dir.mkdir(parents=True, exist_ok=True)

       def scrape_page(self, language: str = "en", timeout: int = 30) -> ScrapedPage:
           """Scrape TBS occupational groups page for given language.

           Per CONTEXT.md: Fail loudly on structure changes.
           """
           url = TBS_URLS.get(language)
           if not url:
               raise ValueError(f"Unsupported language: {language}. Use 'en' or 'fr'.")

           logger.info("Scraping TBS page", url=url, language=language)

           response = requests.get(url, timeout=timeout)
           response.raise_for_status()

           scraped_at = datetime.now(timezone.utc)
           rows = parse_occupational_groups_table(response.text, url, scraped_at)

           links = extract_embedded_links(rows)
           link_count = sum(len(v) for v in links.values())

           logger.info(
               "Scraped TBS page successfully",
               language=language,
               row_count=len(rows),
               link_count=link_count,
           )

           return ScrapedPage(
               url=url,
               language=language,
               title=f"TBS Occupational Groups ({language.upper()})",
               scraped_at=scraped_at,
               rows=rows,
               link_count=link_count,
               row_count=len(rows),
           )

       def scrape_both_languages(self) -> dict[str, ScrapedPage]:
           """Scrape both English and French pages per CONTEXT.md."""
           return {
               "en": self.scrape_page("en"),
               "fr": self.scrape_page("fr"),
           }

       def save_to_json(self, page: ScrapedPage) -> Path:
           """Save scraped page to JSON file with provenance."""
           filename = f"occupational_groups_{page.language}.json"
           filepath = self.output_dir / filename

           # Convert to JSON-serializable format
           data = page.model_dump(mode="json")

           with open(filepath, "w", encoding="utf-8") as f:
               json.dump(data, f, indent=2, ensure_ascii=False)

           logger.info("Saved scraped data", filepath=str(filepath))
           return filepath

       def scrape_and_save(self) -> dict[str, Path]:
           """Scrape both languages and save to JSON files."""
           pages = self.scrape_both_languages()
           return {
               lang: self.save_to_json(page)
               for lang, page in pages.items()
           }

   # Convenience function
   def scrape_occupational_groups(
       output_dir: str | Path = "data/tbs",
       languages: list[str] | None = None,
   ) -> dict[str, Path]:
       """Scrape TBS pages and save to JSON. Returns paths to saved files."""
       scraper = TBSScraper(output_dir)
       languages = languages or ["en", "fr"]

       paths = {}
       for lang in languages:
           page = scraper.scrape_page(lang)
           paths[lang] = scraper.save_to_json(page)

       return paths
   ```

2. **Create initial data files** by running scraper:
   - Run scraper once to create data/tbs/occupational_groups_en.json
   - Run scraper once to create data/tbs/occupational_groups_fr.json
   - Commit the scraped data as baseline

3. **Update tbs/__init__.py** exports:
   - TBSScraper, scrape_occupational_groups
  </action>
  <verify>
```python
from jobforge.external.tbs import TBSScraper, scrape_occupational_groups
# Quick test without network:
from pathlib import Path
assert Path("data/tbs").exists() or True  # Will create on scrape
```

```bash
# Run scraper to create initial data (requires network)
python -c "from jobforge.external.tbs import scrape_occupational_groups; print(scrape_occupational_groups())"
ls data/tbs/*.json
```
  </verify>
  <done>Scraper fetches both EN/FR pages, saves with provenance, fails loudly on structure changes</done>
</task>

<task type="auto">
  <name>Task 3: Extend DIM Occupations schema and create tests</name>
  <files>
    - src/jobforge/semantic/schema.py
    - tests/test_tbs_scraper.py
  </files>
  <action>
1. **Extend DIM Occupations in schema.py**:

   Add TBS fields to the existing DIM_Occupations table definition:
   ```python
   # In the DIM_Occupations columns list, add:
   "tbs_group_code": "TEXT",           # TBS occupational group code
   "tbs_group_abbrev": "TEXT",         # Group abbreviation (AI, CR, etc.)
   "tbs_group_name": "TEXT",           # Full group name
   "tbs_definition_url": "TEXT",       # Link to definition page
   "tbs_job_eval_standard_url": "TEXT", # Link to job eval standard
   "tbs_qualification_standard_url": "TEXT", # Link to qualification standard
   "tbs_scraped_at": "TIMESTAMP",      # When TBS data was scraped
   ```

   These fields enable gold layer queries joining occupations to TBS metadata.

2. **Create tests/test_tbs_scraper.py**:

   **Parser tests** (no network needed):
   - test_parse_valid_html: Mock HTML parses correctly
   - test_parse_extracts_links: Definition/eval/qual URLs extracted
   - test_parse_fails_on_missing_table: Raises ValueError
   - test_parse_fails_on_wrong_columns: Raises ValueError (structure validation)
   - test_validate_table_structure_valid: Returns True for valid table
   - test_validate_table_structure_invalid: Raises for missing columns
   - test_extract_embedded_links: Returns dict with link lists

   **Model tests**:
   - test_occupational_group_row_validates: Valid data creates model
   - test_scraped_provenance_required_fields: All provenance fields required
   - test_scraped_page_counts: row_count and link_count computed

   **Scraper tests** (mock network):
   - test_scraper_creates_output_dir: Directory created if missing
   - test_scraper_saves_json: JSON file written with correct structure
   - test_scraper_bilingual: Both en/fr URLs used
   - test_scraper_provenance_in_output: scraped_at timestamp in JSON

   **Integration test** (skip if no network):
   - test_scrape_real_page: Actually scrapes TBS (pytest.mark.integration)

   **Mock fixtures**:
   ```python
   MOCK_TBS_HTML = """
   <html><head><title>Occupational Groups</title></head>
   <body>
   <table>
   <tr><th>Group abbreviation</th><th>Code</th><th>Occupational Group</th>
       <th>Group</th><th>Subgroup</th><th>Definition</th><th>Job evaluation standard</th></tr>
   <tr><td>AI</td><td>001</td><td>Air Traffic Control</td>
       <td>AI</td><td></td><td><a href="/en/def/ai">Yes</a></td><td><a href="/en/eval/ai">Yes</a></td></tr>
   </table>
   </body></html>
   """
   ```

3. **Schema test**:
   - test_dim_occupations_has_tbs_fields: Verify new columns in schema
  </action>
  <verify>
```bash
pytest tests/test_tbs_scraper.py -v
pytest tests/test_semantic.py -v -k "tbs" 2>/dev/null || echo "Schema tests in main test file"
```
All tests pass. Integration tests skip without network.
  </verify>
  <done>DIM Occupations extended with TBS fields, 15+ tests covering parser, scraper, and schema</done>
</task>

</tasks>

<verification>
1. **Package structure**: `from jobforge.external.tbs import TBSScraper, scrape_occupational_groups`
2. **Data files exist**: `data/tbs/occupational_groups_en.json`, `data/tbs/occupational_groups_fr.json`
3. **Provenance in data**: JSON files contain `scraped_at`, `source_url`, `extraction_method`
4. **Schema extended**: DIM_Occupations has `tbs_group_code`, `tbs_definition_url`, etc.
5. **Test suite**: `pytest tests/test_tbs_scraper.py` passes
6. **Full suite**: `pytest` passes (no regressions)
</verification>

<success_criteria>
- TBSScraper fetches EN and FR pages from canada.ca
- Parser validates table structure and fails loudly on changes
- All scraped values carry provenance (URL, timestamp, method)
- Embedded links extracted (definitions, job eval, qualifications)
- JSON files saved with full provenance metadata
- DIM_Occupations schema extended with TBS fields
- Tests pass with mocked HTML (no network dependency for CI)
</success_criteria>

<output>
After completion, create `.planning/phases/07-external-data-integration/07-03-SUMMARY.md`
</output>
