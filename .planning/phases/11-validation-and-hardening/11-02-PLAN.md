---
phase: 11-validation-and-hardening
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - tests/api/test_table_coverage.py
  - tests/api/test_intent_routing.py
  - tests/orbit/test_adapter_config.py
  - tests/orbit/__init__.py
autonomous: true
user_setup: []

must_haves:
  truths:
    - "User can query any of the 24 gold tables via DuckDBRetriever without errors"
    - "User query is classified into correct intent category (data, metadata, compliance, lineage)"
    - "HTTP adapter configuration correctly routes queries to JobForge API endpoints"
  artifacts:
    - path: "tests/api/test_table_coverage.py"
      provides: "Parametrized tests for all 24 gold tables"
      min_lines: 80
    - path: "tests/api/test_intent_routing.py"
      provides: "Intent classification validation tests"
      min_lines: 60
    - path: "tests/orbit/test_adapter_config.py"
      provides: "HTTP adapter configuration validation"
      min_lines: 40
  key_links:
    - from: "tests/api/test_table_coverage.py"
      to: "src/jobforge/api/data_query.py"
      via: "DataQueryService with mocked Claude"
      pattern: "DataQueryService"
    - from: "tests/orbit/test_adapter_config.py"
      to: "orbit/config/adapters/jobforge.yaml"
      via: "YAML validation"
      pattern: "jobforge.yaml"
---

<objective>
Validate DuckDBRetriever works with all 24 gold tables and intent classification routes queries correctly.

Purpose: ORB-03 requires validation that all 24 gold tables are queryable. ORB-02 requires intent classification to route queries to correct endpoints. ORB-01 requires HTTP adapter config validation.

Output: Parametrized table coverage tests, intent routing tests, adapter config validation tests.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md

# Existing implementations
@src/jobforge/api/data_query.py
@orbit/retrievers/duckdb.py
@orbit/config/adapters/jobforge.yaml
@orbit/config/intents/wiq_intents.yaml

# Existing test patterns
@tests/api/test_data_query.py
@tests/api/test_routes.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create parametrized table coverage tests</name>
  <files>tests/api/test_table_coverage.py</files>
  <action>
Create new test file `tests/api/test_table_coverage.py`:

1. Dynamically discover all 24 gold tables:
   ```python
   import pytest
   from pathlib import Path

   def get_gold_tables():
       """Get list of all gold table names from parquet files."""
       gold_path = Path("data/gold")
       return sorted([p.stem for p in gold_path.glob("*.parquet")])

   GOLD_TABLES = get_gold_tables()
   ```

2. Parametrized test for table accessibility:
   ```python
   @pytest.mark.parametrize("table_name", GOLD_TABLES)
   def test_table_accessible_via_data_service(table_name, mock_client):
       """Validate each gold table is queryable via DataQueryService."""
       # Mock Claude to return simple SELECT for this table
       mock_response = create_mock_response(
           sql=f"SELECT * FROM {table_name} LIMIT 1",
           explanation=f"Query {table_name}",
           tables_used=[table_name]
       )
       mock_client.messages.create.return_value = mock_response

       service = DataQueryService(config=config, client=mock_client)
       result = service.query(f"Show one row from {table_name}")

       assert result.error is None, f"Table {table_name} query failed: {result.error}"
       # Table may be empty, but should not error
       assert isinstance(result.results, list)
       service.close()
   ```

3. Test DuckDB connection registers all tables:
   ```python
   def test_duckdb_registers_all_gold_tables():
       """Verify DuckDB connection has views for all gold tables."""
       service = DataQueryService(config=PipelineConfig(), client=MagicMock())
       conn = service.conn

       # Get registered views
       views = conn.execute("SELECT name FROM sqlite_master WHERE type='view'").fetchall()
       view_names = {v[0] for v in views}

       # All gold tables should be registered
       for table in GOLD_TABLES:
           assert table in view_names, f"Table {table} not registered in DuckDB"

       service.close()
   ```

4. Test table count matches expected:
   ```python
   def test_gold_table_count():
       """Verify we have exactly 24 gold tables."""
       assert len(GOLD_TABLES) == 24, f"Expected 24 tables, found {len(GOLD_TABLES)}: {GOLD_TABLES}"
   ```

5. Helper fixtures following test_data_query.py patterns:
   ```python
   @pytest.fixture
   def mock_client():
       """Create mock Anthropic client."""
       client = MagicMock()
       return client

   def create_mock_response(sql: str, explanation: str, tables_used: list[str]):
       """Create mock Claude response."""
       mock = MagicMock()
       mock.content = [MagicMock(text=json.dumps({
           "sql": sql,
           "explanation": explanation,
           "tables_used": tables_used
       }))]
       return mock
   ```

Use existing patterns from tests/api/test_data_query.py for mock structure.
  </action>
  <verify>
Run: `pytest tests/api/test_table_coverage.py -v`
All 24 parametrized tests should pass.
  </verify>
  <done>All 24 gold tables validated as queryable via DataQueryService, DuckDB registration verified</done>
</task>

<task type="auto">
  <name>Task 2: Create intent classification validation tests</name>
  <files>tests/api/test_intent_routing.py</files>
  <action>
Create new test file `tests/api/test_intent_routing.py`:

1. Create IntentClassifier class that mirrors logic from jobforge.yaml:
   ```python
   import re
   from dataclasses import dataclass

   @dataclass
   class IntentMatch:
       intent: str
       confidence: float
       matched_pattern: str | None = None

   class IntentClassifier:
       """Test implementation of intent classification from adapter config."""

       def __init__(self):
           # Patterns from orbit/config/adapters/jobforge.yaml
           self.patterns = [
               # Data query patterns (high confidence)
               (re.compile(r"how many", re.I), "data", 0.9),
               (re.compile(r"count of", re.I), "data", 0.9),
               (re.compile(r"list all", re.I), "data", 0.85),
               (re.compile(r"show me", re.I), "data", 0.8),

               # Metadata patterns (high confidence)
               (re.compile(r"where does.*come from", re.I), "metadata", 0.95),
               (re.compile(r"lineage", re.I), "metadata", 0.95),
               (re.compile(r"what columns", re.I), "metadata", 0.9),
               (re.compile(r"describe table", re.I), "metadata", 0.9),
               (re.compile(r"how many tables", re.I), "metadata", 0.85),

               # Compliance patterns (high confidence)
               (re.compile(r"dadm compliance", re.I), "compliance", 0.95),
               (re.compile(r"dama compliance", re.I), "compliance", 0.95),
               (re.compile(r"governance status", re.I), "compliance", 0.9),
           ]

       def classify(self, query: str) -> IntentMatch:
           """Classify query intent."""
           best = IntentMatch(intent="data", confidence=0.5)  # Default
           for pattern, intent, confidence in self.patterns:
               if pattern.search(query):
                   if confidence > best.confidence:
                       best = IntentMatch(intent, confidence, pattern.pattern)
           return best
   ```

2. Parametrized tests for each intent category:
   ```python
   DATA_QUERIES = [
       ("How many software developers are there?", "data"),
       ("Count of TEER 1 occupations", "data"),
       ("List all NOC unit groups", "data"),
       ("Show me occupations in category 2", "data"),
       ("What is the employment for 21232?", "data"),
   ]

   METADATA_QUERIES = [
       ("Where does dim_noc come from?", "metadata"),
       ("Show lineage for cops_employment", "metadata"),
       ("What columns are in element_labels?", "metadata"),
       ("Describe table dim_occupations", "metadata"),
       ("How many tables are there?", "metadata"),
   ]

   COMPLIANCE_QUERIES = [
       ("Show DADM compliance status", "compliance"),
       ("Is WiQ DAMA compliant?", "compliance"),
       ("Generate governance status report", "compliance"),
   ]

   @pytest.mark.parametrize("query,expected_intent", DATA_QUERIES)
   def test_data_intent_classification(query, expected_intent):
       classifier = IntentClassifier()
       result = classifier.classify(query)
       assert result.intent == expected_intent, f"Query '{query}' classified as {result.intent}, expected {expected_intent}"

   @pytest.mark.parametrize("query,expected_intent", METADATA_QUERIES)
   def test_metadata_intent_classification(query, expected_intent):
       classifier = IntentClassifier()
       result = classifier.classify(query)
       assert result.intent == expected_intent

   @pytest.mark.parametrize("query,expected_intent", COMPLIANCE_QUERIES)
   def test_compliance_intent_classification(query, expected_intent):
       classifier = IntentClassifier()
       result = classifier.classify(query)
       assert result.intent == expected_intent
   ```

3. Test for ambiguous query handling:
   ```python
   def test_ambiguous_query_defaults_to_data():
       """Ambiguous queries should default to data intent."""
       classifier = IntentClassifier()
       result = classifier.classify("Tell me about occupations")
       # Low confidence, defaults to data
       assert result.intent == "data"
       assert result.confidence < 0.7
   ```

4. Test lineage intent (subtype of metadata):
   ```python
   LINEAGE_QUERIES = [
       "Where does dim_noc come from?",
       "What feeds cops_employment?",
       "Show upstream dependencies of job_architecture",
   ]

   @pytest.mark.parametrize("query", LINEAGE_QUERIES)
   def test_lineage_classified_as_metadata(query):
       """Lineage queries should route to metadata endpoint."""
       classifier = IntentClassifier()
       result = classifier.classify(query)
       assert result.intent == "metadata"
   ```
  </action>
  <verify>
Run: `pytest tests/api/test_intent_routing.py -v`
All intent classification tests should pass.
  </verify>
  <done>Intent classification validated for data, metadata, compliance, and lineage query types</done>
</task>

<task type="auto">
  <name>Task 3: Create HTTP adapter configuration tests</name>
  <files>tests/orbit/test_adapter_config.py, tests/orbit/__init__.py</files>
  <action>
Create test directory and files:

1. Create `tests/orbit/__init__.py` (empty file for pytest discovery)

2. Create `tests/orbit/test_adapter_config.py`:

```python
"""Tests for Orbit adapter configuration validity."""

import yaml
from pathlib import Path

import pytest


class TestJobForgeAdapterConfig:
    """Tests for orbit/config/adapters/jobforge.yaml validity."""

    @pytest.fixture
    def adapter_config(self):
        """Load adapter configuration."""
        config_path = Path("orbit/config/adapters/jobforge.yaml")
        assert config_path.exists(), "Adapter config not found"
        with open(config_path) as f:
            return yaml.safe_load(f)

    def test_adapter_has_required_fields(self, adapter_config):
        """Test adapter config has all required fields."""
        required = ["name", "description", "enabled", "type", "http"]
        for field in required:
            assert field in adapter_config, f"Missing required field: {field}"

    def test_adapter_type_is_http(self, adapter_config):
        """Test adapter is HTTP type."""
        assert adapter_config["type"] == "http"

    def test_http_config_has_endpoints(self, adapter_config):
        """Test HTTP config defines all required endpoints."""
        http = adapter_config["http"]
        assert "base_url" in http
        assert "endpoints" in http

        required_endpoints = ["data", "metadata", "compliance"]
        for endpoint in required_endpoints:
            assert endpoint in http["endpoints"], f"Missing endpoint: {endpoint}"

    def test_data_endpoint_config(self, adapter_config):
        """Test data endpoint is correctly configured."""
        data = adapter_config["http"]["endpoints"]["data"]
        assert data["path"] == "/api/query/data"
        assert data["method"] == "POST"
        assert "question" in data["body"]

    def test_metadata_endpoint_config(self, adapter_config):
        """Test metadata endpoint is correctly configured."""
        metadata = adapter_config["http"]["endpoints"]["metadata"]
        assert metadata["path"] == "/api/query/metadata"
        assert metadata["method"] == "POST"

    def test_compliance_endpoint_config(self, adapter_config):
        """Test compliance endpoint is correctly configured."""
        compliance = adapter_config["http"]["endpoints"]["compliance"]
        assert compliance["path"].startswith("/api/compliance/")
        assert compliance["method"] == "GET"

    def test_intents_defined(self, adapter_config):
        """Test intent routing rules are defined."""
        assert "intents" in adapter_config
        intents = adapter_config["intents"]

        # Should have data, metadata, and compliance intents
        intent_names = {i["name"] for i in intents}
        assert "data_query" in intent_names
        assert "metadata_query" in intent_names
        assert "compliance_query" in intent_names

    def test_intent_patterns_are_valid(self, adapter_config):
        """Test all intent patterns are valid strings."""
        import re
        for intent in adapter_config["intents"]:
            for pattern in intent.get("patterns", []):
                # Should be a valid string (not regex, just keyword)
                assert isinstance(pattern, str)
                assert len(pattern) > 0


class TestWiQIntentsConfig:
    """Tests for orbit/config/intents/wiq_intents.yaml validity."""

    @pytest.fixture
    def intents_config(self):
        """Load intents configuration."""
        config_path = Path("orbit/config/intents/wiq_intents.yaml")
        assert config_path.exists(), "Intents config not found"
        with open(config_path) as f:
            return yaml.safe_load(f)

    def test_intents_has_domain(self, intents_config):
        """Test intents config specifies domain."""
        assert intents_config["domain"] == "workforce_intelligence"

    def test_entities_defined(self, intents_config):
        """Test domain entities are defined."""
        entities = intents_config["entities"]
        required_entities = ["noc_code", "teer_level", "broad_category"]
        for entity in required_entities:
            assert entity in entities, f"Missing entity: {entity}"

    def test_entity_patterns_are_valid_regex(self, intents_config):
        """Test entity patterns compile as valid regex."""
        import re
        for entity_name, entity in intents_config["entities"].items():
            for pattern in entity.get("patterns", []):
                try:
                    re.compile(pattern)
                except re.error as e:
                    pytest.fail(f"Invalid regex in {entity_name}: {pattern} - {e}")

    def test_intent_categories_defined(self, intents_config):
        """Test intent categories are defined."""
        categories = intents_config["intent_categories"]
        required = ["occupation_queries", "forecast_queries", "lineage_queries", "compliance_queries"]
        for cat in required:
            assert cat in categories, f"Missing category: {cat}"

    def test_fallback_strategy_defined(self, intents_config):
        """Test fallback behavior is defined."""
        assert "fallback" in intents_config
        assert "strategy" in intents_config["fallback"]
```

These tests validate the YAML configuration files are well-formed and contain all required routing information.
  </action>
  <verify>
Run: `pytest tests/orbit/ -v`
All adapter configuration tests should pass.
  </verify>
  <done>HTTP adapter and intent configuration validated via automated tests</done>
</task>

</tasks>

<verification>
Run complete test suite:
```bash
pytest tests/api/test_table_coverage.py tests/api/test_intent_routing.py tests/orbit/ -v
```

Verify test counts:
- Table coverage: 24+ tests (one per gold table + registration tests)
- Intent routing: 15+ tests (across all intent categories)
- Adapter config: 10+ tests (YAML validation)

Total new tests should be ~50+.
</verification>

<success_criteria>
- [ ] `tests/api/test_table_coverage.py` exists with parametrized tests for all 24 tables
- [ ] `tests/api/test_intent_routing.py` exists with tests for data/metadata/compliance/lineage intents
- [ ] `tests/orbit/test_adapter_config.py` exists with YAML validation tests
- [ ] All 24 gold tables pass queryability test
- [ ] Intent classification matches expected routing for sample queries
- [ ] Adapter config has all required endpoints (data, metadata, compliance)
- [ ] All new tests pass
</success_criteria>

<output>
After completion, create `.planning/phases/11-validation-and-hardening/11-02-SUMMARY.md`
</output>
