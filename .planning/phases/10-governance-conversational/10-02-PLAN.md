---
phase: 10-governance-conversational
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - src/jobforge/api/__init__.py
  - src/jobforge/api/routes.py
  - src/jobforge/api/data_query.py
  - src/jobforge/api/metadata_query.py
  - src/jobforge/api/schema_ddl.py
  - pyproject.toml
  - tests/api/__init__.py
  - tests/api/test_routes.py
  - tests/api/test_data_query.py
  - tests/api/test_metadata_query.py
autonomous: true
user_setup:
  - service: anthropic
    why: "Claude API for text-to-SQL generation"
    env_vars:
      - name: ANTHROPIC_API_KEY
        source: "Anthropic Console -> API Keys"

must_haves:
  truths:
    - "User can POST natural language question to /api/query/data and get SQL + results"
    - "User can POST natural language question to /api/query/metadata and get lineage/provenance answer"
    - "Data query endpoint uses Claude structured outputs for SQL generation"
    - "Metadata query endpoint extends existing LineageQueryEngine patterns"
  artifacts:
    - path: "src/jobforge/api/routes.py"
      provides: "FastAPI router with /query/data, /query/metadata, /compliance/{framework} endpoints"
      exports: ["router", "create_api_app"]
    - path: "src/jobforge/api/data_query.py"
      provides: "DataQueryService using Claude + DuckDB"
      exports: ["DataQueryService", "SQLQuery", "DataQueryResult"]
    - path: "src/jobforge/api/metadata_query.py"
      provides: "MetadataQueryService wrapping LineageQueryEngine"
      exports: ["MetadataQueryService"]
    - path: "src/jobforge/api/schema_ddl.py"
      provides: "DDL generator for gold tables"
      exports: ["generate_schema_ddl"]
  key_links:
    - from: "src/jobforge/api/data_query.py"
      to: "anthropic"
      via: "Claude API client with structured outputs"
      pattern: "anthropic\\.Anthropic"
    - from: "src/jobforge/api/data_query.py"
      to: "duckdb"
      via: "DuckDB connection for SQL execution"
      pattern: "duckdb\\.connect"
    - from: "src/jobforge/api/metadata_query.py"
      to: "src/jobforge/governance/query.py"
      via: "LineageQueryEngine import"
      pattern: "from jobforge.governance.query import LineageQueryEngine"
---

<objective>
Create HTTP API endpoints for conversational data and metadata queries, enabling Orbit integration.

Purpose: Expose JobForge's query capabilities via HTTP so Orbit can route user questions to the appropriate backend (GOV-05, GOV-06). Data queries use Claude structured outputs for text-to-SQL; metadata queries extend the existing rule-based LineageQueryEngine.

Output: FastAPI application with /api/query/data, /api/query/metadata, and /api/compliance/{framework} endpoints.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/10-governance-conversational/10-RESEARCH.md

# Existing infrastructure
@src/jobforge/governance/query.py
@src/jobforge/governance/graph.py
@src/jobforge/demo/app.py
@src/jobforge/pipeline/config.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create Data Query Service with Claude Text-to-SQL</name>
  <files>
    - src/jobforge/api/__init__.py
    - src/jobforge/api/data_query.py
    - src/jobforge/api/schema_ddl.py
    - pyproject.toml
    - tests/api/__init__.py
    - tests/api/test_data_query.py
  </files>
  <action>
Create the API package with data query service using Claude structured outputs.

**Add dependency to pyproject.toml:**
```toml
dependencies = [
    ...
    "anthropic>=0.43.0",
]
```

**schema_ddl.py:**
Create DDL generator that produces CREATE TABLE statements from gold parquet files:
```python
import duckdb
from pathlib import Path
from jobforge.pipeline.config import PipelineConfig

def generate_schema_ddl(config: PipelineConfig | None = None) -> str:
    """Generate DDL for all gold tables."""
    config = config or PipelineConfig()
    conn = duckdb.connect(":memory:")

    ddl_parts = []
    for parquet in sorted(config.gold_path().glob("*.parquet")):
        table_name = parquet.stem
        # Create view to introspect schema
        conn.execute(f"CREATE VIEW {table_name} AS SELECT * FROM '{parquet}'")
        # Get column info
        cols = conn.execute(f"DESCRIBE {table_name}").fetchall()
        col_defs = [f"  {col[0]} {col[1]}" for col in cols]
        ddl_parts.append(f"CREATE TABLE {table_name} (\n" + ",\n".join(col_defs) + "\n);")

    return "\n\n".join(ddl_parts)
```

**data_query.py:**
Create DataQueryService using Claude structured outputs:
```python
import os
import json
import duckdb
import anthropic
from pydantic import BaseModel, Field
from jobforge.pipeline.config import PipelineConfig
from jobforge.api.schema_ddl import generate_schema_ddl

class SQLQuery(BaseModel):
    """Structured output for SQL generation."""
    sql: str = Field(description="DuckDB-compatible SELECT query")
    explanation: str = Field(description="Brief explanation of what the query does")
    tables_used: list[str] = Field(description="Tables referenced in the query")

class DataQueryResult(BaseModel):
    """Result of a data query."""
    question: str
    sql: str
    explanation: str
    results: list[dict]
    row_count: int
    error: str | None = None

class DataQueryService:
    """Service for natural language to SQL queries."""

    SYSTEM_PROMPT = """You are a SQL expert for the WiQ (Workforce Intelligence) database.
Given a schema and question, generate a DuckDB-compatible SELECT query.
The database contains Canadian occupational data:
- dim_noc: National Occupational Classification hierarchy
- dim_occupations: Occupational groups with TBS metadata
- cops_*: Canadian Occupational Projection System forecasts
- oasis_*: Occupational attributes (skills, abilities, knowledge)
- element_*: NOC element data (titles, duties, requirements)
- job_architecture: Internal job architecture hierarchy

IMPORTANT:
- Only generate SELECT queries (never INSERT, UPDATE, DELETE, DROP)
- Use DuckDB SQL syntax
- Keep queries simple and focused on answering the question"""

    def __init__(self, config: PipelineConfig | None = None):
        self.config = config or PipelineConfig()
        self.client = anthropic.Anthropic()
        self._schema_ddl: str | None = None
        self._conn: duckdb.DuckDBPyConnection | None = None

    @property
    def schema_ddl(self) -> str:
        if self._schema_ddl is None:
            self._schema_ddl = generate_schema_ddl(self.config)
        return self._schema_ddl

    @property
    def conn(self) -> duckdb.DuckDBPyConnection:
        if self._conn is None:
            self._conn = duckdb.connect(":memory:")
            # Register all gold tables as views
            for parquet in self.config.gold_path().glob("*.parquet"):
                table_name = parquet.stem
                self._conn.execute(
                    f"CREATE VIEW {table_name} AS SELECT * FROM '{parquet}'"
                )
        return self._conn

    def query(self, question: str) -> DataQueryResult:
        """Generate SQL from question and execute."""
        try:
            # Generate SQL using Claude structured outputs
            response = self.client.messages.create(
                model="claude-sonnet-4-20250514",
                max_tokens=1024,
                system=self.SYSTEM_PROMPT,
                messages=[{
                    "role": "user",
                    "content": f"Schema:\n{self.schema_ddl}\n\nQuestion: {question}"
                }],
                extra_headers={"anthropic-beta": "structured-outputs-2025-11-13"},
                response_format={
                    "type": "json_schema",
                    "json_schema": {
                        "name": "sql_query",
                        "schema": SQLQuery.model_json_schema(),
                    }
                }
            )

            sql_result = SQLQuery.model_validate_json(response.content[0].text)

            # Execute query
            df = self.conn.execute(sql_result.sql).fetchdf()
            results = df.to_dict(orient="records")

            return DataQueryResult(
                question=question,
                sql=sql_result.sql,
                explanation=sql_result.explanation,
                results=results,
                row_count=len(results),
            )

        except Exception as e:
            return DataQueryResult(
                question=question,
                sql="",
                explanation="",
                results=[],
                row_count=0,
                error=str(e),
            )
```

**Tests:**
- Test generate_schema_ddl produces valid DDL for all 24 gold tables
- Test SQLQuery model validation
- Test DataQueryResult model with and without error
- Mock anthropic client for DataQueryService tests
- Test SQL execution on sample queries
  </action>
  <verify>
```bash
pytest tests/api/test_data_query.py -v
```
All tests pass, including DDL generation and mocked query service.
  </verify>
  <done>
DataQueryService generates SQL from natural language using Claude structured outputs and executes against DuckDB. Schema DDL generated from all 24 gold parquet files.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create Metadata Query Service and API Routes</name>
  <files>
    - src/jobforge/api/metadata_query.py
    - src/jobforge/api/routes.py
    - tests/api/test_metadata_query.py
    - tests/api/test_routes.py
  </files>
  <action>
Create metadata query service wrapping LineageQueryEngine, and FastAPI routes.

**metadata_query.py:**
Create MetadataQueryService that extends LineageQueryEngine patterns:
```python
import re
from typing import Callable
from jobforge.governance.graph import LineageGraph
from jobforge.governance.query import LineageQueryEngine
from jobforge.pipeline.config import PipelineConfig

class MetadataQueryService:
    """Service for natural language metadata queries."""

    def __init__(self, config: PipelineConfig | None = None):
        self.config = config or PipelineConfig()
        self.graph = LineageGraph(self.config)
        self.lineage_engine = LineageQueryEngine(self.graph)
        self.patterns: list[tuple[re.Pattern, Callable]] = self._build_patterns()

    def _build_patterns(self) -> list[tuple[re.Pattern, Callable]]:
        """Build additional patterns beyond LineageQueryEngine."""
        return [
            # Catalogue patterns
            (re.compile(r"describe (?:table )?(\w+)", re.I), self._handle_describe),
            (re.compile(r"what columns (?:are )?in (\w+)", re.I), self._handle_columns),
            (re.compile(r"how many (?:gold )?tables", re.I), self._handle_table_count),
            (re.compile(r"list (?:all )?tables", re.I), self._handle_list_tables),
            # Row count patterns
            (re.compile(r"how many rows (?:are )?in (\w+)", re.I), self._handle_row_count),
        ]

    def query(self, question: str) -> str:
        """Process metadata question, return human-readable answer."""
        # Try extended patterns first
        for pattern, handler in self.patterns:
            match = pattern.search(question)
            if match:
                return handler(*match.groups()) if match.groups() else handler()

        # Fall back to lineage engine
        return self.lineage_engine.query(question)

    def _handle_describe(self, table_name: str) -> str:
        """Describe a table from catalogue metadata."""
        # Load table metadata from catalog
        ...

    def _handle_columns(self, table_name: str) -> str:
        """List columns in a table."""
        ...

    def _handle_table_count(self) -> str:
        """Count gold tables."""
        count = len(list(self.config.gold_path().glob("*.parquet")))
        return f"There are {count} tables in the gold layer."

    def _handle_list_tables(self) -> str:
        """List all gold tables."""
        tables = sorted(p.stem for p in self.config.gold_path().glob("*.parquet"))
        return "Gold layer tables:\n" + "\n".join(f"  - {t}" for t in tables)

    def _handle_row_count(self, table_name: str) -> str:
        """Get row count for a table."""
        import duckdb
        path = self.config.gold_path() / f"{table_name.lower()}.parquet"
        if not path.exists():
            return f"Table '{table_name}' not found."
        conn = duckdb.connect(":memory:")
        count = conn.execute(f"SELECT COUNT(*) FROM '{path}'").fetchone()[0]
        return f"Table '{table_name}' has {count:,} rows."
```

**routes.py:**
Create FastAPI application with query endpoints:
```python
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from jobforge.api.data_query import DataQueryService, DataQueryResult
from jobforge.api.metadata_query import MetadataQueryService
from jobforge.pipeline.config import PipelineConfig

class QueryRequest(BaseModel):
    question: str

class MetadataQueryResult(BaseModel):
    question: str
    answer: str

def create_api_app() -> FastAPI:
    """Create FastAPI application for query endpoints."""
    app = FastAPI(
        title="JobForge Query API",
        description="Conversational interface for WiQ data and metadata",
        version="1.0.0",
    )

    config = PipelineConfig()
    data_service = DataQueryService(config)
    metadata_service = MetadataQueryService(config)

    @app.post("/api/query/data", response_model=DataQueryResult)
    async def query_data(request: QueryRequest):
        """Query WiQ data using natural language."""
        result = data_service.query(request.question)
        if result.error:
            raise HTTPException(status_code=400, detail=result.error)
        return result

    @app.post("/api/query/metadata", response_model=MetadataQueryResult)
    async def query_metadata(request: QueryRequest):
        """Query WiQ metadata using natural language."""
        answer = metadata_service.query(request.question)
        return MetadataQueryResult(question=request.question, answer=answer)

    @app.get("/api/compliance/{framework}")
    async def get_compliance(framework: str):
        """Get compliance log for a framework."""
        from jobforge.governance.compliance import (
            DADMComplianceLog,
            DAMAComplianceLog,
            ClassificationComplianceLog,
        )
        generators = {
            "dadm": DADMComplianceLog,
            "dama": DAMAComplianceLog,
            "classification": ClassificationComplianceLog,
        }
        if framework.lower() not in generators:
            raise HTTPException(status_code=404, detail=f"Unknown framework: {framework}")
        generator = generators[framework.lower()](config)
        log = generator.generate()
        return log.model_dump()

    @app.get("/api/health")
    async def health():
        return {"status": "ok"}

    return app

# Export for uvicorn
app = create_api_app()
```

**Tests:**
- Test MetadataQueryService patterns (describe, columns, table_count, list_tables, row_count)
- Test MetadataQueryService fallback to LineageQueryEngine
- Test FastAPI routes with TestClient
- Test /api/query/data endpoint (mocked Claude)
- Test /api/query/metadata endpoint
- Test /api/compliance/{framework} endpoint
- Test error handling (unknown table, invalid framework)
  </action>
  <verify>
```bash
pytest tests/api/test_metadata_query.py tests/api/test_routes.py -v
```
All tests pass, including route tests with FastAPI TestClient.
  </verify>
  <done>
MetadataQueryService extends LineageQueryEngine with catalogue patterns. FastAPI routes expose /api/query/data, /api/query/metadata, and /api/compliance/{framework} endpoints.
  </done>
</task>

<task type="auto">
  <name>Task 3: Add CLI Command and Integration with Demo App</name>
  <files>
    - src/jobforge/cli/commands.py
    - src/jobforge/api/__init__.py
  </files>
  <action>
Add CLI command to start the query API server and update package exports.

**Update api/__init__.py:**
```python
from jobforge.api.routes import create_api_app, app
from jobforge.api.data_query import DataQueryService, SQLQuery, DataQueryResult
from jobforge.api.metadata_query import MetadataQueryService

__all__ = [
    "create_api_app",
    "app",
    "DataQueryService",
    "SQLQuery",
    "DataQueryResult",
    "MetadataQueryService",
]
```

**Add CLI command to commands.py:**
```python
@app.command()
def api(
    host: str = typer.Option("127.0.0.1", "--host", "-h", help="Host to bind to"),
    port: int = typer.Option(8000, "--port", "-p", help="Port to bind to"),
    reload: bool = typer.Option(False, "--reload", "-r", help="Enable auto-reload"),
):
    """Start the JobForge Query API server.

    Provides HTTP endpoints for conversational data and metadata queries:
    - POST /api/query/data - Natural language data queries (uses Claude API)
    - POST /api/query/metadata - Metadata and lineage queries
    - GET /api/compliance/{framework} - Compliance traceability logs

    Prerequisites:
    - Set ANTHROPIC_API_KEY environment variable for data queries
    - Gold layer tables must exist (run 'jobforge run' first)

    Example:
        jobforge api                    # Start on localhost:8000
        jobforge api -p 8080            # Custom port
        jobforge api --reload           # Development mode with auto-reload

    Test with curl:
        curl -X POST http://localhost:8000/api/query/metadata \\
             -H "Content-Type: application/json" \\
             -d '{"question": "how many gold tables?"}'
    """
    import uvicorn
    from jobforge.api import app as api_app

    console.print("[bold]Starting JobForge Query API[/bold]")
    console.print(f"  Host: {host}")
    console.print(f"  Port: {port}")
    console.print(f"  Docs: http://{host}:{port}/docs")
    console.print()

    uvicorn.run(
        "jobforge.api:app" if reload else api_app,
        host=host,
        port=port,
        reload=reload,
    )
```

This integrates with the existing demo app architecture (Phase 9) while providing a separate API server for query endpoints. The demo app (SSE streaming) runs on port 8080; the query API runs on port 8000.
  </action>
  <verify>
```bash
# Test CLI command help
jobforge api --help

# Start server briefly to verify it works
timeout 5 jobforge api &
sleep 2
curl http://localhost:8000/api/health
curl -X POST http://localhost:8000/api/query/metadata \
     -H "Content-Type: application/json" \
     -d '{"question": "how many gold tables?"}'

# Full test suite
pytest tests/api/ -v
```
API server starts and responds to health check and metadata queries.
  </verify>
  <done>
CLI command `jobforge api` starts the query API server. Package exports configured for clean imports. Integration tested with curl commands.
  </done>
</task>

</tasks>

<verification>
**Phase-level verification:**

1. API server starts and responds:
```bash
jobforge api &
API_PID=$!
sleep 3

# Health check
curl http://localhost:8000/api/health

# Metadata query (no Claude API needed)
curl -X POST http://localhost:8000/api/query/metadata \
     -H "Content-Type: application/json" \
     -d '{"question": "where does dim_noc come from?"}'

# Data query (requires ANTHROPIC_API_KEY)
curl -X POST http://localhost:8000/api/query/data \
     -H "Content-Type: application/json" \
     -d '{"question": "how many unit groups are in dim_noc?"}'

# Compliance endpoint
curl http://localhost:8000/api/compliance/dadm

kill $API_PID
```

2. OpenAPI docs available at http://localhost:8000/docs

3. Full test suite passes:
```bash
pytest tests/api/ -v
```
</verification>

<success_criteria>
- GOV-05: POST /api/query/data accepts question, returns SQL + results (or error)
- GOV-06: POST /api/query/metadata accepts question, returns lineage/provenance answer
- Claude structured outputs used for SQL generation (anthropic-beta header)
- DuckDB executes generated SQL against gold parquet files
- MetadataQueryService extends LineageQueryEngine with catalogue patterns
- CLI command `jobforge api` starts server on configurable host:port
- All tests pass with mocked Claude client
</success_criteria>

<output>
After completion, create `.planning/phases/10-governance-conversational/10-02-SUMMARY.md`
</output>
