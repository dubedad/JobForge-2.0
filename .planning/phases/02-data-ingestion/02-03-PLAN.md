---
phase: 02-data-ingestion
plan: 03
type: execute
wave: 2
depends_on: ["02-01"]
files_modified:
  - src/jobforge/ingestion/cops.py
  - src/jobforge/ingestion/job_architecture.py
  - tests/test_cops_ingestion.py
  - tests/test_job_architecture_ingestion.py
  - data/gold/cops_employment.parquet
  - data/gold/job_architecture.parquet
autonomous: true

must_haves:
  truths:
    - "COPS forecasting tables exist in gold layer with NOC foreign keys"
    - "Job Architecture table exists in gold layer with job titles and classifications"
    - "DIM Occupations (Occupational Groups) structure is extractable from Job Architecture"
    - "COPS tables have unit_group_id linking to DIM NOC where applicable"
  artifacts:
    - path: "src/jobforge/ingestion/cops.py"
      provides: "COPS forecasting table ingestion"
      exports: ["ingest_cops_table", "COPS_SUPPLY_TABLES", "COPS_DEMAND_TABLES"]
    - path: "src/jobforge/ingestion/job_architecture.py"
      provides: "Job Architecture and DIM Occupations ingestion"
      exports: ["ingest_job_architecture", "extract_dim_occupations"]
    - path: "data/gold/cops_employment.parquet"
      provides: "COPS employment projections by NOC"
      contains: "unit_group_id, projection years 2023-2033"
    - path: "data/gold/job_architecture.parquet"
      provides: "Job titles with NOC mappings"
      contains: "unit_group_id, job_title, job_family, job_function"
  key_links:
    - from: "data/gold/cops_employment.parquet"
      to: "data/gold/dim_noc.parquet"
      via: "unit_group_id foreign key"
      pattern: "unit_group_id"
    - from: "data/gold/job_architecture.parquet"
      to: "data/gold/dim_noc.parquet"
      via: "unit_group_id derived from 2021_NOC_UID"
      pattern: "2021_NOC_UID"
---

<objective>
Ingest COPS forecasting data, Job Architecture, and DIM Occupations to gold layer.

Purpose: COPS tables provide 10-year workforce supply/demand projections linked to NOC codes. Job Architecture maps organizational job titles to NOC classifications. DIM Occupations (Occupational Groups) provides job family/function hierarchy. These complete the Phase 2 data ingestion requirements.

Output:
- ingest_cops_table() for forecasting facts (supply and demand)
- ingest_job_architecture() for job title mappings
- extract_dim_occupations() to derive occupational groups from Job Architecture
- Tests verifying FK relationships and data integrity
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-data-ingestion/02-RESEARCH.md
@.planning/phases/02-data-ingestion/02-01-SUMMARY.md

# Existing ingestion patterns
@src/jobforge/ingestion/noc.py
@src/jobforge/ingestion/transforms.py
@src/jobforge/pipeline/engine.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create COPS Forecasting Ingestion</name>
  <files>
    src/jobforge/ingestion/cops.py
    data/source/employment_cops_sample.csv
  </files>
  <action>
Create COPS (Canadian Occupational Projection System) table ingestion.

**COPS data format (from 02-RESEARCH.md):**
- Column "Code": NOC code or aggregate code (00000, TEER_0, NOC1_1, 00010, etc.)
- Column "Occupation Name" / "Nom de la profession": Bilingual occupation names
- Columns "2023", "2024", ... "2033": Projection values for each year
- Special codes for aggregates: 00000 (all), TEER_*, NOC1_* (not unit groups)

**COPS table types:**
- Supply tables: school_leavers, immigration, other_seekers, job_seekers_total
- Demand tables: employment, employment_growth, job_openings, retirements, retirement_rates, other_replacement

**src/jobforge/ingestion/cops.py:**
```python
"""COPS forecasting table ingestion."""
from pathlib import Path
from typing import Optional

import polars as pl

from jobforge.pipeline.config import PipelineConfig
from jobforge.pipeline.engine import PipelineEngine


# COPS supply tables (where workers come from)
COPS_SUPPLY_TABLES = [
    "school_leavers",
    "immigration",
    "other_seekers",
    "job_seekers_total",
]

# COPS demand tables (why positions need filling)
COPS_DEMAND_TABLES = [
    "employment",
    "employment_growth",
    "job_openings",
    "retirements",
    "retirement_rates",
    "other_replacement",
]

# Summary/assessment tables
COPS_SUMMARY_TABLES = [
    "summary",
    "flmc",
    "rlmc",
]

# All COPS tables
COPS_TABLES = COPS_SUPPLY_TABLES + COPS_DEMAND_TABLES + COPS_SUMMARY_TABLES


def _is_unit_group_code(code: str) -> bool:
    """Check if a COPS code is a unit group (5-digit NOC code).

    COPS includes aggregate codes:
    - 00000: All occupations
    - TEER_0 through TEER_5: TEER level aggregates
    - NOC1_0 through NOC1_9: Major group aggregates

    Only 5-digit numeric codes are unit groups.
    """
    if code is None or len(code) != 5:
        return False
    return code.isdigit()


def ingest_cops_table(
    source_path: Path,
    table_name: str,
    config: Optional[PipelineConfig] = None,
    filter_to_unit_groups: bool = True,
) -> dict:
    """Ingest a COPS forecasting table to gold layer.

    COPS tables contain 10-year projections for workforce supply/demand.
    Each row represents one occupation (by Code).

    Transforms applied:
    - Bronze: Rename columns to snake_case
    - Silver: Derive unit_group_id, optionally filter to unit groups
    - Gold: Final column ordering

    Args:
        source_path: Path to COPS CSV file
        table_name: Output table name (e.g., "cops_employment")
        config: Pipeline configuration (defaults to PipelineConfig())
        filter_to_unit_groups: If True, filter out aggregate rows (default True)

    Returns:
        Dict with gold_path, batch_id, and row counts
    """
    engine = PipelineEngine(config=config)

    # Bronze: rename columns to snake_case
    bronze_schema = {
        "rename": {
            "Code": "code",
            "Occupation Name": "occupation_name_en",
            "Nom de la profession": "occupation_name_fr",
        },
    }

    # Silver transforms
    def derive_unit_group_id(df: pl.LazyFrame) -> pl.LazyFrame:
        """Derive unit_group_id for valid 5-digit codes only."""
        return df.with_columns(
            pl.when(
                (pl.col("code").str.len_chars() == 5) &
                (pl.col("code").str.contains(r"^\d{5}$"))
            )
            .then(pl.col("code").str.zfill(5))
            .otherwise(pl.lit(None))
            .alias("unit_group_id")
        )

    def filter_unit_groups_only(df: pl.LazyFrame) -> pl.LazyFrame:
        """Filter to unit group rows only (exclude aggregates)."""
        return df.filter(pl.col("unit_group_id").is_not_null())

    silver_transforms = [derive_unit_group_id]
    if filter_to_unit_groups:
        silver_transforms.append(filter_unit_groups_only)

    # Gold: reorder columns
    def reorder_columns(df: pl.LazyFrame) -> pl.LazyFrame:
        """Reorder columns: FK first, then code/names, then years, then provenance."""
        cols = df.collect_schema().names()

        # Priority columns
        priority = ["unit_group_id", "code", "occupation_name_en", "occupation_name_fr"]

        # Provenance columns
        provenance = ["_source_file", "_ingested_at", "_batch_id", "_layer"]

        # Year columns (numeric columns like 2023, 2024, etc.)
        year_cols = [c for c in cols if c not in priority and c not in provenance]

        # Final order
        final_order = priority + year_cols + provenance
        return df.select([c for c in final_order if c in cols])

    gold_transforms = [reorder_columns]

    result = engine.run_full_pipeline(
        source_path=source_path,
        table_name=table_name,
        domain="cops",
        bronze_schema=bronze_schema,
        silver_transforms=silver_transforms,
        gold_transforms=gold_transforms,
    )

    return result


def ingest_all_cops_tables(
    source_dir: Path,
    config: Optional[PipelineConfig] = None,
    filter_to_unit_groups: bool = True,
) -> dict[str, dict]:
    """Ingest all COPS tables from a source directory.

    Args:
        source_dir: Directory containing COPS CSV files
        config: Pipeline configuration
        filter_to_unit_groups: Filter out aggregate rows

    Returns:
        Dict mapping table name to ingestion result
    """
    results = {}
    for table in COPS_TABLES:
        # Try common file patterns
        patterns = [
            f"{table}_*.csv",
            f"*{table}*.csv",
            f"{table.replace('_', '-')}*.csv",
        ]
        for pattern in patterns:
            matches = list(source_dir.glob(pattern))
            if matches:
                source_path = matches[0]
                results[table] = ingest_cops_table(
                    source_path=source_path,
                    table_name=f"cops_{table}",
                    config=config,
                    filter_to_unit_groups=filter_to_unit_groups,
                )
                break
    return results
```

**Create sample COPS CSV for testing:**

**data/source/employment_cops_sample.csv:**
```csv
Code,Occupation Name,Nom de la profession,2023,2024,2025,2026,2027,2028,2029,2030,2031,2032,2033
00000,All occupations,Toutes les professions,20000000,20100000,20200000,20300000,20400000,20500000,20600000,20700000,20800000,20900000,21000000
TEER_0,TEER 0 occupations,Professions FEER 0,500000,510000,520000,530000,540000,550000,560000,570000,580000,590000,600000
00010,Legislators,Législateurs,5000,5100,5150,5200,5250,5300,5350,5400,5450,5500,5550
00011,Senior government managers,Cadres supérieurs gouvernementaux,12000,12100,12200,12300,12400,12500,12600,12700,12800,12900,13000
10010,Financial managers,Directeurs financiers,45000,45500,46000,46500,47000,47500,48000,48500,49000,49500,50000
```
  </action>
  <verify>
1. `python -c "from jobforge.ingestion.cops import ingest_cops_table, COPS_SUPPLY_TABLES, COPS_DEMAND_TABLES; print('Import OK')"`
2. Verify sample CSV: `ls data/source/employment_cops_sample.csv`
3. Quick test: `python -c "from jobforge.ingestion.cops import ingest_cops_table; from pathlib import Path; r = ingest_cops_table(Path('data/source/employment_cops_sample.csv'), 'cops_employment_test'); print(f'Rows: {r}')"` (if sample exists)
  </verify>
  <done>
ingest_cops_table() function created. Filters aggregate rows, derives unit_group_id for valid 5-digit NOC codes. Sample employment CSV available for testing.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create Job Architecture and DIM Occupations Ingestion</name>
  <files>
    src/jobforge/ingestion/job_architecture.py
    data/source/job_architecture_sample.csv
  </files>
  <action>
Create Job Architecture ingestion with DIM Occupations extraction.

**Job Architecture data format (from 02-RESEARCH.md):**
- JT_ID: Job title unique identifier
- Job_Title / Titre_de_poste: Bilingual job titles
- Job_Function / Fonction_professionnelle: Function name
- Job_Family / Famille_d'emplois: Family name
- Managerial_Level / Niveau_de_gestion: Level (Employee, Director, etc.)
- 2021_NOC_UID: NOC 2021 code for FK relationship
- Additional columns for classification and matching

**Hierarchy:** Job Title (L7) -> Job Family (L6) -> Job Function

**src/jobforge/ingestion/job_architecture.py:**
```python
"""Job Architecture and DIM Occupations ingestion."""
from pathlib import Path
from typing import Optional

import polars as pl

from jobforge.pipeline.config import PipelineConfig
from jobforge.pipeline.engine import PipelineEngine


def ingest_job_architecture(
    source_path: Path,
    config: Optional[PipelineConfig] = None,
    table_name: str = "job_architecture",
) -> dict:
    """Ingest Job Architecture CSV to gold layer.

    Job Architecture maps organizational job titles to NOC codes.
    Each row is one job title (L7 level).

    Transforms applied:
    - Bronze: Clean column names (remove special chars)
    - Silver: Derive unit_group_id from 2021_NOC_UID
    - Gold: Select key columns for semantic model

    Args:
        source_path: Path to Job Architecture CSV file
        config: Pipeline configuration (defaults to PipelineConfig())
        table_name: Output table name (defaults to "job_architecture")

    Returns:
        Dict with gold_path, batch_id, and row counts
    """
    engine = PipelineEngine(config=config)

    # Bronze: clean column names
    bronze_schema = {
        "rename": {
            "JT_ID": "jt_id",
            "Job_Title": "job_title_en",
            "Titre_de_poste": "job_title_fr",
            "Job_Function": "job_function_en",
            "Fonction_professionnelle": "job_function_fr",
            "Job_Family": "job_family_en",
            "Famille_d'emplois": "job_family_fr",
            "Managerial_Level": "managerial_level_en",
            "Niveau_de_gestion": "managerial_level_fr",
            "2021_NOC_UID": "noc_2021_uid",
            "2021_NOC_Title": "noc_2021_title",
            "2016_NOC_UID": "noc_2016_uid",
            "2016_NOC_Title": "noc_2016_title",
            "Match_Key": "match_key",
        },
    }

    # Silver transforms
    def derive_unit_group_id(df: pl.LazyFrame) -> pl.LazyFrame:
        """Derive unit_group_id from 2021_NOC_UID.

        NOC 2021 codes should be 5 digits. Zero-pad if needed.
        Handle null/missing NOC codes gracefully.
        """
        return df.with_columns(
            pl.when(pl.col("noc_2021_uid").is_not_null())
            .then(
                pl.col("noc_2021_uid")
                .cast(pl.Utf8)
                .str.replace_all(r"[^\d]", "")  # Remove non-numeric chars
                .str.zfill(5)
            )
            .otherwise(pl.lit(None))
            .alias("unit_group_id")
        )

    silver_transforms = [derive_unit_group_id]

    # Gold: select final columns
    def select_gold_columns(df: pl.LazyFrame) -> pl.LazyFrame:
        """Select and order columns for Job Architecture gold table."""
        cols = df.collect_schema().names()
        priority = [
            "jt_id",
            "unit_group_id",
            "job_title_en",
            "job_title_fr",
            "job_function_en",
            "job_function_fr",
            "job_family_en",
            "job_family_fr",
            "managerial_level_en",
            "managerial_level_fr",
            "noc_2021_uid",
            "noc_2021_title",
        ]
        provenance = ["_source_file", "_ingested_at", "_batch_id", "_layer"]
        available = [c for c in priority if c in cols]
        return df.select(available + provenance)

    gold_transforms = [select_gold_columns]

    result = engine.run_full_pipeline(
        source_path=source_path,
        table_name=table_name,
        domain="gc",
        bronze_schema=bronze_schema,
        silver_transforms=silver_transforms,
        gold_transforms=gold_transforms,
    )

    return result


def extract_dim_occupations(
    job_arch_gold_path: Path,
    config: Optional[PipelineConfig] = None,
) -> dict:
    """Extract DIM Occupations (Occupational Groups) from Job Architecture.

    Creates a dimension table of unique job families and functions
    from the Job Architecture table. This is the L6 level hierarchy.

    Args:
        job_arch_gold_path: Path to Job Architecture gold parquet
        config: Pipeline configuration

    Returns:
        Dict with gold_path and row count
    """
    config = config or PipelineConfig()

    # Read Job Architecture gold table
    job_arch = pl.scan_parquet(job_arch_gold_path)

    # Extract unique job families with their functions
    dim_occupations = (
        job_arch
        .select([
            "job_family_en",
            "job_family_fr",
            "job_function_en",
            "job_function_fr",
        ])
        .unique()
        .with_columns([
            # Create a family_id based on hash of family name
            pl.concat_str([pl.col("job_family_en"), pl.col("job_function_en")])
            .hash()
            .cast(pl.Utf8)
            .str.slice(0, 8)
            .alias("occupation_group_id"),
        ])
        .sort("job_function_en", "job_family_en")
    )

    # Add provenance columns
    from datetime import datetime, timezone
    import uuid

    batch_id = str(uuid.uuid4())
    ingested_at = datetime.now(timezone.utc).isoformat()

    dim_occupations = dim_occupations.with_columns([
        pl.lit(str(job_arch_gold_path)).alias("_source_file"),
        pl.lit(ingested_at).alias("_ingested_at"),
        pl.lit(batch_id).alias("_batch_id"),
        pl.lit("gold").alias("_layer"),
    ])

    # Write to gold
    gold_dir = config.gold_path()
    gold_dir.mkdir(parents=True, exist_ok=True)
    output_path = gold_dir / "dim_occupations.parquet"

    result_df = dim_occupations.collect()
    result_df.write_parquet(output_path, compression="zstd")

    return {
        "gold_path": output_path,
        "row_count": len(result_df),
        "batch_id": batch_id,
    }


def ingest_job_architecture_with_dim_occupations(
    source_path: Path,
    config: Optional[PipelineConfig] = None,
) -> dict:
    """Ingest Job Architecture and extract DIM Occupations in one call.

    Args:
        source_path: Path to Job Architecture CSV file
        config: Pipeline configuration

    Returns:
        Dict with job_architecture_result and dim_occupations_result
    """
    config = config or PipelineConfig()

    # First, ingest Job Architecture
    job_arch_result = ingest_job_architecture(source_path, config)

    # Then, extract DIM Occupations from it
    dim_occ_result = extract_dim_occupations(job_arch_result["gold_path"], config)

    return {
        "job_architecture": job_arch_result,
        "dim_occupations": dim_occ_result,
    }
```

**Create sample Job Architecture CSV for testing:**

**data/source/job_architecture_sample.csv:**
```csv
JT_ID,Job_Title,Titre_de_poste,Job_Function,Fonction_professionnelle,Job_Family,Famille_d'emplois,Managerial_Level,Niveau_de_gestion,2021_NOC_UID,2021_NOC_Title,2016_NOC_UID,2016_NOC_Title,Match_Key
1,Senior Policy Analyst,Analyste principal des politiques,Policy,Politiques,Policy Analysis,Analyse des politiques,Employee,Employe,41400,Policy analysts,4164,Policy analysts,1001
2,Director of Finance,Directeur des finances,Finance,Finance,Financial Management,Gestion financiere,Director,Directeur,10010,Financial managers,0111,Financial managers,1002
3,HR Advisor,Conseiller en RH,Human Resources,Ressources humaines,HR Advisory,Conseil en RH,Employee,Employe,11200,Human resources professionals,1121,Human resources professionals,1003
4,IT Project Manager,Gestionnaire de projet TI,Information Technology,Technologie de l'information,Project Management,Gestion de projet,Manager,Gestionnaire,21231,Computer systems developers,2171,Computer systems developers,1004
5,Administrative Assistant,Adjoint administratif,Administration,Administration,Administrative Support,Soutien administratif,Employee,Employe,12011,Administrative assistants,1241,Administrative assistants,1005
```
  </action>
  <verify>
1. `python -c "from jobforge.ingestion.job_architecture import ingest_job_architecture, extract_dim_occupations; print('Import OK')"`
2. Verify sample CSV: `ls data/source/job_architecture_sample.csv`
3. Quick test: `python -c "from jobforge.ingestion.job_architecture import ingest_job_architecture; from pathlib import Path; r = ingest_job_architecture(Path('data/source/job_architecture_sample.csv')); print(f'Gold: {r[\"gold_path\"]}')"` (if sample exists)
  </verify>
  <done>
ingest_job_architecture() and extract_dim_occupations() functions created. Job Architecture derives unit_group_id from NOC 2021 code. DIM Occupations extracts unique job families/functions.
  </done>
</task>

<task type="auto">
  <name>Task 3: Create Tests and Update Ingestion Module Exports</name>
  <files>
    tests/test_cops_ingestion.py
    tests/test_job_architecture_ingestion.py
    src/jobforge/ingestion/__init__.py
  </files>
  <action>
Create tests for COPS and Job Architecture ingestion and update module exports.

**tests/test_cops_ingestion.py:**
```python
"""Tests for COPS forecasting table ingestion."""
from pathlib import Path

import polars as pl
import pytest

from jobforge.pipeline.config import PipelineConfig
from jobforge.ingestion.cops import (
    ingest_cops_table,
    COPS_SUPPLY_TABLES,
    COPS_DEMAND_TABLES,
)


@pytest.fixture
def test_config(tmp_path: Path) -> PipelineConfig:
    """Create a PipelineConfig using temporary directory."""
    return PipelineConfig(data_root=tmp_path / "data")


@pytest.fixture
def sample_cops_csv(tmp_path: Path) -> Path:
    """Create a sample COPS employment CSV for testing."""
    csv_path = tmp_path / "employment.csv"
    csv_content = '''Code,Occupation Name,Nom de la profession,2023,2024,2025
00000,All occupations,Toutes les professions,20000000,20100000,20200000
TEER_0,TEER 0,Professions FEER 0,500000,510000,520000
00010,Legislators,Legislateurs,5000,5100,5150
00011,Senior managers,Cadres superieurs,12000,12100,12200
10010,Financial managers,Directeurs financiers,45000,45500,46000'''
    csv_path.write_text(csv_content, encoding="utf-8")
    return csv_path


class TestCopsIngestion:
    """Tests for COPS ingestion."""

    def test_cops_tables_lists_complete(self) -> None:
        """Verify COPS table lists contain expected tables."""
        assert "employment" in COPS_DEMAND_TABLES
        assert "job_openings" in COPS_DEMAND_TABLES
        assert "school_leavers" in COPS_SUPPLY_TABLES
        assert "immigration" in COPS_SUPPLY_TABLES

    def test_ingest_cops_creates_gold_file(
        self,
        sample_cops_csv: Path,
        test_config: PipelineConfig,
    ) -> None:
        """Verify ingest_cops_table creates gold parquet file."""
        result = ingest_cops_table(
            source_path=sample_cops_csv,
            table_name="cops_employment",
            config=test_config,
        )

        assert result["gold_path"].exists()

    def test_cops_filters_aggregate_rows(
        self,
        sample_cops_csv: Path,
        test_config: PipelineConfig,
    ) -> None:
        """Verify aggregate rows (00000, TEER_*) are filtered out."""
        result = ingest_cops_table(
            source_path=sample_cops_csv,
            table_name="cops_employment",
            config=test_config,
            filter_to_unit_groups=True,
        )

        gold_df = pl.read_parquet(result["gold_path"])

        # Sample has 5 rows, but only 3 are unit groups
        assert len(gold_df) == 3
        # All should have unit_group_id
        assert gold_df["unit_group_id"].null_count() == 0

    def test_cops_preserves_aggregates_when_requested(
        self,
        sample_cops_csv: Path,
        test_config: PipelineConfig,
    ) -> None:
        """Verify aggregates are preserved when filter_to_unit_groups=False."""
        result = ingest_cops_table(
            source_path=sample_cops_csv,
            table_name="cops_employment_all",
            config=test_config,
            filter_to_unit_groups=False,
        )

        gold_df = pl.read_parquet(result["gold_path"])

        # All 5 rows preserved
        assert len(gold_df) == 5
        # Aggregates have null unit_group_id
        assert gold_df["unit_group_id"].null_count() == 2

    def test_cops_has_projection_columns(
        self,
        sample_cops_csv: Path,
        test_config: PipelineConfig,
    ) -> None:
        """Verify projection year columns are preserved."""
        result = ingest_cops_table(
            source_path=sample_cops_csv,
            table_name="cops_employment",
            config=test_config,
        )

        gold_df = pl.read_parquet(result["gold_path"])

        assert "2023" in gold_df.columns
        assert "2024" in gold_df.columns
        assert "2025" in gold_df.columns

    def test_cops_has_bilingual_names(
        self,
        sample_cops_csv: Path,
        test_config: PipelineConfig,
    ) -> None:
        """Verify bilingual occupation names are preserved."""
        result = ingest_cops_table(
            source_path=sample_cops_csv,
            table_name="cops_employment",
            config=test_config,
        )

        gold_df = pl.read_parquet(result["gold_path"])

        assert "occupation_name_en" in gold_df.columns
        assert "occupation_name_fr" in gold_df.columns
```

**tests/test_job_architecture_ingestion.py:**
```python
"""Tests for Job Architecture and DIM Occupations ingestion."""
from pathlib import Path

import polars as pl
import pytest

from jobforge.pipeline.config import PipelineConfig
from jobforge.ingestion.job_architecture import (
    ingest_job_architecture,
    extract_dim_occupations,
    ingest_job_architecture_with_dim_occupations,
)


@pytest.fixture
def test_config(tmp_path: Path) -> PipelineConfig:
    """Create a PipelineConfig using temporary directory."""
    return PipelineConfig(data_root=tmp_path / "data")


@pytest.fixture
def sample_job_arch_csv(tmp_path: Path) -> Path:
    """Create a sample Job Architecture CSV for testing."""
    csv_path = tmp_path / "job_architecture.csv"
    csv_content = '''JT_ID,Job_Title,Titre_de_poste,Job_Function,Fonction_professionnelle,Job_Family,Famille_d'emplois,Managerial_Level,Niveau_de_gestion,2021_NOC_UID,2021_NOC_Title
1,Senior Analyst,Analyste principal,Policy,Politiques,Policy Analysis,Analyse des politiques,Employee,Employe,41400,Policy analysts
2,Director Finance,Directeur finances,Finance,Finance,Financial Mgmt,Gestion financiere,Director,Directeur,10010,Financial managers
3,HR Advisor,Conseiller RH,HR,RH,HR Advisory,Conseil RH,Employee,Employe,11200,HR professionals
4,Junior Analyst,Analyste junior,Policy,Politiques,Policy Analysis,Analyse des politiques,Employee,Employe,41400,Policy analysts'''
    csv_path.write_text(csv_content, encoding="utf-8")
    return csv_path


class TestJobArchitectureIngestion:
    """Tests for Job Architecture ingestion."""

    def test_ingest_creates_gold_file(
        self,
        sample_job_arch_csv: Path,
        test_config: PipelineConfig,
    ) -> None:
        """Verify ingest_job_architecture creates gold parquet file."""
        result = ingest_job_architecture(
            source_path=sample_job_arch_csv,
            config=test_config,
        )

        assert result["gold_path"].exists()

    def test_job_arch_has_unit_group_id(
        self,
        sample_job_arch_csv: Path,
        test_config: PipelineConfig,
    ) -> None:
        """Verify unit_group_id is derived from NOC 2021 code."""
        result = ingest_job_architecture(
            source_path=sample_job_arch_csv,
            config=test_config,
        )

        gold_df = pl.read_parquet(result["gold_path"])

        assert "unit_group_id" in gold_df.columns
        # Check derivation: 10010 -> 10010
        row = gold_df.filter(pl.col("jt_id") == "2")
        assert row["unit_group_id"][0] == "10010"

    def test_job_arch_has_bilingual_columns(
        self,
        sample_job_arch_csv: Path,
        test_config: PipelineConfig,
    ) -> None:
        """Verify bilingual columns are preserved."""
        result = ingest_job_architecture(
            source_path=sample_job_arch_csv,
            config=test_config,
        )

        gold_df = pl.read_parquet(result["gold_path"])

        assert "job_title_en" in gold_df.columns
        assert "job_title_fr" in gold_df.columns
        assert "job_function_en" in gold_df.columns
        assert "job_function_fr" in gold_df.columns

    def test_job_arch_preserves_all_rows(
        self,
        sample_job_arch_csv: Path,
        test_config: PipelineConfig,
    ) -> None:
        """Verify all job title rows are preserved."""
        result = ingest_job_architecture(
            source_path=sample_job_arch_csv,
            config=test_config,
        )

        gold_df = pl.read_parquet(result["gold_path"])

        assert len(gold_df) == 4


class TestDimOccupationsExtraction:
    """Tests for DIM Occupations extraction."""

    def test_extract_dim_occupations_creates_file(
        self,
        sample_job_arch_csv: Path,
        test_config: PipelineConfig,
    ) -> None:
        """Verify extract_dim_occupations creates gold parquet file."""
        # First ingest Job Architecture
        job_arch_result = ingest_job_architecture(
            source_path=sample_job_arch_csv,
            config=test_config,
        )

        # Then extract DIM Occupations
        result = extract_dim_occupations(
            job_arch_gold_path=job_arch_result["gold_path"],
            config=test_config,
        )

        assert result["gold_path"].exists()

    def test_dim_occupations_unique_families(
        self,
        sample_job_arch_csv: Path,
        test_config: PipelineConfig,
    ) -> None:
        """Verify DIM Occupations contains unique job families."""
        job_arch_result = ingest_job_architecture(
            source_path=sample_job_arch_csv,
            config=test_config,
        )

        result = extract_dim_occupations(
            job_arch_gold_path=job_arch_result["gold_path"],
            config=test_config,
        )

        gold_df = pl.read_parquet(result["gold_path"])

        # Sample has 3 unique families (Policy Analysis x2 deduped)
        assert len(gold_df) == 3
        assert "job_family_en" in gold_df.columns
        assert "job_function_en" in gold_df.columns

    def test_combined_ingestion(
        self,
        sample_job_arch_csv: Path,
        test_config: PipelineConfig,
    ) -> None:
        """Verify combined ingestion creates both tables."""
        result = ingest_job_architecture_with_dim_occupations(
            source_path=sample_job_arch_csv,
            config=test_config,
        )

        assert result["job_architecture"]["gold_path"].exists()
        assert result["dim_occupations"]["gold_path"].exists()


class TestJobArchForeignKeys:
    """Tests for foreign key relationships."""

    def test_unit_group_ids_valid_format(
        self,
        sample_job_arch_csv: Path,
        test_config: PipelineConfig,
    ) -> None:
        """Verify all unit_group_id values are 5-digit format."""
        result = ingest_job_architecture(
            source_path=sample_job_arch_csv,
            config=test_config,
        )

        gold_df = pl.read_parquet(result["gold_path"])

        for uid in gold_df["unit_group_id"]:
            if uid is not None:
                assert len(uid) == 5, f"unit_group_id {uid} is not 5 digits"
```

**Update src/jobforge/ingestion/__init__.py:**
```python
from jobforge.ingestion.noc import ingest_dim_noc
from jobforge.ingestion.oasis import ingest_oasis_table, ingest_all_oasis_tables, OASIS_TABLES
from jobforge.ingestion.element import ingest_element_table, ingest_all_element_tables, ELEMENT_TABLES
from jobforge.ingestion.cops import (
    ingest_cops_table,
    ingest_all_cops_tables,
    COPS_SUPPLY_TABLES,
    COPS_DEMAND_TABLES,
    COPS_SUMMARY_TABLES,
    COPS_TABLES,
)
from jobforge.ingestion.job_architecture import (
    ingest_job_architecture,
    extract_dim_occupations,
    ingest_job_architecture_with_dim_occupations,
)
from jobforge.ingestion.transforms import (
    filter_unit_groups,
    derive_unit_group_id,
    normalize_noc_code,
    derive_noc_element_code,
    derive_unit_group_from_oasis,
)

__all__ = [
    # NOC ingestion
    "ingest_dim_noc",
    # OASIS ingestion
    "ingest_oasis_table",
    "ingest_all_oasis_tables",
    "OASIS_TABLES",
    # Element ingestion
    "ingest_element_table",
    "ingest_all_element_tables",
    "ELEMENT_TABLES",
    # COPS ingestion
    "ingest_cops_table",
    "ingest_all_cops_tables",
    "COPS_SUPPLY_TABLES",
    "COPS_DEMAND_TABLES",
    "COPS_SUMMARY_TABLES",
    "COPS_TABLES",
    # Job Architecture ingestion
    "ingest_job_architecture",
    "extract_dim_occupations",
    "ingest_job_architecture_with_dim_occupations",
    # Transform functions
    "filter_unit_groups",
    "derive_unit_group_id",
    "normalize_noc_code",
    "derive_noc_element_code",
    "derive_unit_group_from_oasis",
]
```
  </action>
  <verify>
Run both test files:
1. `pytest tests/test_cops_ingestion.py -v` - all tests pass
2. `pytest tests/test_job_architecture_ingestion.py -v` - all tests pass

Expected results:
- COPS: 6+ tests pass (table lists, create file, filter aggregates, preserve aggregates, projection columns, bilingual names)
- Job Architecture: 8+ tests pass (create file, unit_group_id, bilingual columns, preserve rows, extract dim_occupations, unique families, combined ingestion, FK format)
  </verify>
  <done>
All tests pass. COPS, Job Architecture, and DIM Occupations ingestion complete:
- COPS filters aggregates and preserves projection years
- Job Architecture derives unit_group_id from NOC 2021 code
- DIM Occupations extracts unique job families/functions
- All tables have valid FK format for relationship to DIM NOC
  </done>
</task>

</tasks>

<verification>
## Phase 2 Plan 03 Verification

### Automated Checks
1. **Import verification:**
   ```bash
   python -c "from jobforge.ingestion import ingest_cops_table, ingest_job_architecture, extract_dim_occupations; print('OK')"
   ```
2. **Test suites:**
   - `pytest tests/test_cops_ingestion.py -v` - all tests pass
   - `pytest tests/test_job_architecture_ingestion.py -v` - all tests pass
3. **Full test suite:** `pytest tests/ -v` - all Phase 2 tests pass

### Gold Table Verification (with real data)
```python
import polars as pl

# Verify all gold tables exist
tables = ["dim_noc", "cops_employment", "job_architecture", "dim_occupations"]
for table in tables:
    path = f"data/gold/{table}.parquet"
    df = pl.read_parquet(path)
    print(f"{table}: {len(df)} rows, columns: {df.columns[:5]}...")

# Verify FK relationships
dim_noc = pl.read_parquet("data/gold/dim_noc.parquet")
noc_ids = set(dim_noc["unit_group_id"])

cops = pl.read_parquet("data/gold/cops_employment.parquet")
cops_ids = set(cops["unit_group_id"])
print(f"COPS orphans: {len(cops_ids - noc_ids)}")

job_arch = pl.read_parquet("data/gold/job_architecture.parquet")
job_ids = set(job_arch["unit_group_id"].drop_nulls())
print(f"Job Arch orphans: {len(job_ids - noc_ids)}")
```
</verification>

<success_criteria>
1. ingest_cops_table() ingests forecasting tables with year projections
2. COPS tables filter aggregate rows (00000, TEER_*, NOC1_*) by default
3. COPS tables have unit_group_id for valid 5-digit NOC codes
4. ingest_job_architecture() ingests job titles with NOC mappings
5. extract_dim_occupations() derives unique job families/functions
6. Job Architecture has unit_group_id derived from 2021_NOC_UID
7. All gold tables have provenance columns
8. All tests pass
</success_criteria>

<output>
After completion, create `.planning/phases/02-data-ingestion/02-03-SUMMARY.md`
</output>
