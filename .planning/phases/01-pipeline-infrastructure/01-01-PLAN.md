---
phase: 01-pipeline-infrastructure
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - pyproject.toml
  - src/jobforge/__init__.py
  - src/jobforge/pipeline/__init__.py
  - src/jobforge/pipeline/models.py
  - src/jobforge/pipeline/config.py
  - data/.gitkeep
autonomous: true

must_haves:
  truths:
    - "Python project is installable with pip install -e ."
    - "All required dependencies (polars, duckdb, pydantic, structlog) are available"
    - "Data directory structure matches medallion architecture"
    - "Pydantic models validate provenance and metadata correctly"
  artifacts:
    - path: "pyproject.toml"
      provides: "Project configuration with dependencies"
      contains: "polars"
    - path: "src/jobforge/pipeline/models.py"
      provides: "Pydantic models for provenance tracking"
      exports: ["ProvenanceColumns", "LayerTransitionLog", "TableMetadata"]
    - path: "src/jobforge/pipeline/config.py"
      provides: "Pipeline configuration and paths"
      exports: ["PipelineConfig", "Layer"]
  key_links:
    - from: "pyproject.toml"
      to: "src/jobforge"
      via: "package discovery"
      pattern: "packages.*jobforge"
---

<objective>
Set up Python project structure and create foundational data models for medallion pipeline.

Purpose: Establish the project skeleton and core data structures that all pipeline operations will use. Without this foundation, no pipeline work can proceed.

Output: Installable Python package with Pydantic models for provenance tracking and medallion layer configuration.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/01-pipeline-infrastructure/01-RESEARCH.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create Python Project Structure</name>
  <files>
    pyproject.toml
    src/jobforge/__init__.py
    src/jobforge/pipeline/__init__.py
  </files>
  <action>
Create a modern Python project using pyproject.toml (NOT setup.py - deprecated).

pyproject.toml must include:
- name: "jobforge"
- version: "2.0.0"
- requires-python: ">=3.11"
- dependencies:
  - polars>=1.37.0 (DataFrame operations, parquet I/O)
  - duckdb>=1.4.0 (SQL analytics on parquet)
  - pydantic>=2.12.0 (schema validation, metadata models)
  - structlog (structured logging for audit trail)
  - pyarrow (Polars and DuckDB both need this)
- Use src layout (src/jobforge/)
- Include dev dependencies: pytest, ruff

Create __init__.py files that expose package version.
  </action>
  <verify>
Run: pip install -e .
Run: python -c "import jobforge; print(jobforge.__version__)"
Both commands succeed without errors.
  </verify>
  <done>Package installs successfully and imports return version "2.0.0"</done>
</task>

<task type="auto">
  <name>Task 2: Create Data Directory Structure</name>
  <files>
    data/staged/.gitkeep
    data/bronze/.gitkeep
    data/silver/.gitkeep
    data/gold/.gitkeep
    data/quarantine/.gitkeep
    data/catalog/tables/.gitkeep
    data/catalog/lineage/.gitkeep
    data/catalog/glossary/.gitkeep
    data/catalog/schemas/.gitkeep
  </files>
  <action>
Create the medallion data directory structure per DAMA DMBOK architecture:

data/
  staged/           # Layer 0: Raw ingestion, preserve source fidelity
  bronze/           # Layer 1: Type-enforced, schema standardized
  silver/           # Layer 2: Cleaned, harmonized, deduplicated
  gold/             # Layer 3: Business-ready dimensional model
  quarantine/       # Error isolation with context
  catalog/          # External metadata (DAMA Chapter 5)
    tables/         # Table-level metadata JSON files
    lineage/        # Layer transition logs
    glossary/       # Business terms (future use)
    schemas/        # Validation schemas (future use)

Use .gitkeep files to preserve empty directories in git.
Add data/ to .gitignore EXCEPT for .gitkeep files and catalog/ (metadata is code).
  </action>
  <verify>
Run: python -c "from pathlib import Path; dirs = ['data/staged', 'data/bronze', 'data/silver', 'data/gold', 'data/quarantine', 'data/catalog/tables', 'data/catalog/lineage']; assert all(Path(d).exists() for d in dirs)"
  </verify>
  <done>All medallion layer directories exist and catalog directories are in place</done>
</task>

<task type="auto">
  <name>Task 3: Create Pipeline Configuration and Models</name>
  <files>
    src/jobforge/pipeline/config.py
    src/jobforge/pipeline/models.py
  </files>
  <action>
Create config.py with:
- Layer enum: STAGED, BRONZE, SILVER, GOLD (use Python Enum)
- PipelineConfig class containing:
  - data_root: Path (default: Path("data"))
  - Methods to get path for each layer: staged_path(), bronze_path(), etc.
  - Method to get catalog paths: catalog_tables_path(), catalog_lineage_path()

Create models.py with Pydantic BaseModel classes per RESEARCH.md:

1. ProvenanceColumns - row-level provenance added to every parquet:
   - _source_file: str
   - _ingested_at: datetime (UTC)
   - _batch_id: str (UUID)
   - _layer: str (layer name)

2. LayerTransitionLog - captures each layer movement:
   - transition_id: str (UUID)
   - batch_id: str (links to row-level)
   - source_layer: Literal["staged", "bronze", "silver"]
   - target_layer: Literal["bronze", "silver", "gold"]
   - source_files: list[str]
   - target_file: str
   - row_count_in: int
   - row_count_out: int
   - transforms_applied: list[str]
   - started_at: datetime
   - completed_at: datetime
   - status: Literal["success", "partial", "failed"]
   - errors: Optional[list[str]] = None

3. ColumnMetadata - column-level metadata:
   - name: str
   - data_type: str
   - nullable: bool
   - description: str
   - glossary_term_id: Optional[str]
   - source_columns: list[str]
   - pii_classification: Optional[str]
   - example_values: list[str] = []

4. TableMetadata - table-level metadata per DAMA Chapter 5:
   - table_name: str
   - layer: str
   - domain: str
   - file_path: str
   - row_count: int
   - column_count: int
   - file_size_bytes: int
   - schema_version: str
   - created_at: datetime
   - updated_at: datetime
   - description: str
   - business_purpose: str
   - data_owner: str
   - data_steward: Optional[str]
   - upstream_tables: list[str]
   - downstream_tables: list[str]
   - transform_script: Optional[str]
   - retention_days: Optional[int]
   - classification: str
   - columns: list[ColumnMetadata]

All datetime fields should use timezone-aware UTC. Add model_config for JSON serialization of datetime.
  </action>
  <verify>
Run: python -c "from jobforge.pipeline.models import ProvenanceColumns, LayerTransitionLog, TableMetadata; from jobforge.pipeline.config import PipelineConfig, Layer; print('Models imported successfully')"
Run: python -c "from jobforge.pipeline.config import PipelineConfig; c = PipelineConfig(); print(c.staged_path())"
  </verify>
  <done>All Pydantic models import and validate correctly; PipelineConfig returns correct paths</done>
</task>

</tasks>

<verification>
1. pip install -e . succeeds
2. python -c "import jobforge" succeeds
3. All data directories exist
4. All Pydantic models can be instantiated with valid data
5. PipelineConfig returns expected paths
</verification>

<success_criteria>
- Python project is installable and importable
- Dependencies (polars, duckdb, pydantic, structlog) are available
- Medallion directory structure exists
- Pydantic models for provenance, transitions, and metadata are defined
- Configuration provides typed access to all pipeline paths
</success_criteria>

<output>
After completion, create `.planning/phases/01-pipeline-infrastructure/01-01-SUMMARY.md`
</output>
