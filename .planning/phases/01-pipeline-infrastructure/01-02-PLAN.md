---
phase: 01-pipeline-infrastructure
plan: 02
type: execute
wave: 2
depends_on: ["01-01"]
files_modified:
  - src/jobforge/pipeline/provenance.py
  - src/jobforge/pipeline/layers.py
  - src/jobforge/pipeline/engine.py
autonomous: true

must_haves:
  truths:
    - "Provenance columns are automatically added when data enters each layer"
    - "Layer transitions enforce allowed operations (staged cannot modify source data)"
    - "Pipeline can move data from staged through bronze, silver, to gold"
    - "Each layer transition creates a log record"
  artifacts:
    - path: "src/jobforge/pipeline/provenance.py"
      provides: "Provenance column helper functions"
      exports: ["add_provenance_columns", "generate_batch_id"]
    - path: "src/jobforge/pipeline/layers.py"
      provides: "Layer-specific transformation logic"
      exports: ["StagedLayer", "BronzeLayer", "SilverLayer", "GoldLayer"]
    - path: "src/jobforge/pipeline/engine.py"
      provides: "Pipeline orchestrator"
      exports: ["PipelineEngine"]
  key_links:
    - from: "src/jobforge/pipeline/engine.py"
      to: "src/jobforge/pipeline/layers.py"
      via: "imports layer classes"
      pattern: "from.*layers import"
    - from: "src/jobforge/pipeline/layers.py"
      to: "src/jobforge/pipeline/provenance.py"
      via: "uses provenance helpers"
      pattern: "from.*provenance import"
    - from: "src/jobforge/pipeline/engine.py"
      to: "src/jobforge/pipeline/models.py"
      via: "creates LayerTransitionLog"
      pattern: "LayerTransitionLog"
---

<objective>
Build the core pipeline engine that moves data through medallion layers with provenance tracking.

Purpose: This is the heart of the pipeline infrastructure - the ability to ingest source data and transform it through staged -> bronze -> silver -> gold while maintaining full provenance. Without this, no data can flow through the system.

Output: Working pipeline engine that can process a source file through all four layers with provenance columns and transition logs.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/01-pipeline-infrastructure/01-RESEARCH.md
@.planning/phases/01-pipeline-infrastructure/01-01-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create Provenance Helpers</name>
  <files>src/jobforge/pipeline/provenance.py</files>
  <action>
Create provenance.py with functions for adding provenance columns to DataFrames.

Functions to implement:

1. generate_batch_id() -> str
   - Returns UUID4 string for batch identification
   - Used to link records across layers

2. add_provenance_columns(df: pl.LazyFrame, source_file: str, batch_id: str, layer: str) -> pl.LazyFrame
   - Adds standard provenance columns per DAMA DMBOK:
     - _source_file: str (the source filename)
     - _ingested_at: datetime (UTC now)
     - _batch_id: str (the batch ID)
     - _layer: str (current layer name)
   - Uses pl.lit() to add literal columns
   - Returns LazyFrame (preserves lazy evaluation)

3. update_layer_column(df: pl.LazyFrame, new_layer: str) -> pl.LazyFrame
   - Updates _layer column when data moves to new layer
   - Preserves other provenance columns
   - Updates _ingested_at to current timestamp

Use Polars (not Pandas). Use datetime.now(timezone.utc) for timestamps.
Import uuid for batch ID generation.
  </action>
  <verify>
Run: python -c "
import polars as pl
from jobforge.pipeline.provenance import add_provenance_columns, generate_batch_id

df = pl.LazyFrame({'a': [1,2,3]})
batch_id = generate_batch_id()
df = add_provenance_columns(df, 'test.csv', batch_id, 'staged')
result = df.collect()
assert '_source_file' in result.columns
assert '_batch_id' in result.columns
assert '_layer' in result.columns
print('Provenance columns work correctly')
"
  </verify>
  <done>Provenance helpers add correct columns to LazyFrames</done>
</task>

<task type="auto">
  <name>Task 2: Create Layer Classes</name>
  <files>src/jobforge/pipeline/layers.py</files>
  <action>
Create layers.py with classes representing each medallion layer.

Each layer class should:
- Know its name (STAGED, BRONZE, SILVER, GOLD)
- Know allowed operations per DAMA architecture
- Have a process() method that transforms data appropriately

Layer responsibilities (from RESEARCH.md):

1. StagedLayer:
   - Only adds provenance columns
   - MUST NOT modify source data in any way
   - Converts input to parquet format
   - Method: ingest(source_path: Path, config: PipelineConfig) -> Path
     - Reads source file (CSV, Excel, JSON, or Parquet)
     - Adds provenance columns
     - Writes to staged/ directory
     - Returns path to staged parquet file

2. BronzeLayer:
   - Type standardization
   - Schema enforcement (can rename columns, cast types)
   - NO business logic
   - Method: process(staged_path: Path, config: PipelineConfig, schema: Optional[dict] = None) -> Path
     - Reads from staged
     - Applies schema if provided (column renames, type casts)
     - Updates layer provenance
     - Writes to bronze/
     - Returns path to bronze parquet file

3. SilverLayer:
   - Data cleaning and harmonization
   - Deduplication
   - Validation
   - Method: process(bronze_path: Path, config: PipelineConfig, transforms: list[Callable] = None) -> Path
     - Reads from bronze
     - Applies transform functions if provided
     - Updates layer provenance
     - Writes to silver/
     - Returns path to silver parquet file

4. GoldLayer:
   - Business model creation
   - Derived fields
   - Ready for analytics
   - Method: process(silver_path: Path, config: PipelineConfig, transforms: list[Callable] = None) -> Path
     - Reads from silver
     - Applies business transforms if provided
     - Updates layer provenance
     - Writes to gold/
     - Returns path to gold parquet file

Use Polars scan_parquet() for lazy reads (NOT read_parquet().lazy()).
Use write_parquet() with compression="zstd" for output.
  </action>
  <verify>
Run: python -c "
from jobforge.pipeline.layers import StagedLayer, BronzeLayer, SilverLayer, GoldLayer
from jobforge.pipeline.config import PipelineConfig
print('All layer classes import successfully')
"
  </verify>
  <done>All four layer classes are defined with correct responsibilities</done>
</task>

<task type="auto">
  <name>Task 3: Create Pipeline Engine</name>
  <files>src/jobforge/pipeline/engine.py</files>
  <action>
Create engine.py with PipelineEngine class that orchestrates data flow through all layers.

PipelineEngine class:

Constructor:
- __init__(self, config: PipelineConfig = None)
- Stores config (default to PipelineConfig())
- Initializes layer instances
- Creates structlog logger for audit trail

Methods:

1. ingest(self, source_path: Path, table_name: str, domain: str = "default") -> dict
   - Entry point for new data
   - Runs source through staged layer
   - Creates LayerTransitionLog for the ingestion
   - Returns dict with batch_id, staged_path, log

2. promote_to_bronze(self, staged_path: Path, schema: dict = None) -> dict
   - Moves data from staged to bronze
   - Creates LayerTransitionLog
   - Returns dict with batch_id, bronze_path, log

3. promote_to_silver(self, bronze_path: Path, transforms: list[Callable] = None) -> dict
   - Moves data from bronze to silver
   - Creates LayerTransitionLog
   - Returns dict with batch_id, silver_path, log

4. promote_to_gold(self, silver_path: Path, transforms: list[Callable] = None) -> dict
   - Moves data from silver to gold
   - Creates LayerTransitionLog
   - Returns dict with batch_id, gold_path, log

5. run_full_pipeline(self, source_path: Path, table_name: str, domain: str = "default", bronze_schema: dict = None, silver_transforms: list = None, gold_transforms: list = None) -> dict
   - Convenience method: runs all four stages in sequence
   - Returns dict with all paths and all logs
   - Useful for simple cases; individual promote_* methods for complex pipelines

Each method must:
- Generate or propagate batch_id
- Record start/end timestamps
- Count rows before and after
- Create LayerTransitionLog
- Save log to catalog/lineage/{batch_id}.json using Pydantic's model_dump_json()

Use structlog for logging each transition.
  </action>
  <verify>
Run: python -c "
from jobforge.pipeline.engine import PipelineEngine
from jobforge.pipeline.config import PipelineConfig

engine = PipelineEngine()
print('PipelineEngine initialized successfully')
print(f'Config data root: {engine.config.data_root}')
"
  </verify>
  <done>PipelineEngine orchestrates full medallion flow with transition logging</done>
</task>

</tasks>

<verification>
1. All pipeline modules import without errors
2. Provenance columns are correctly added to DataFrames
3. Layer classes enforce their responsibilities
4. PipelineEngine can be instantiated with default config
5. LayerTransitionLog is created for each transition
</verification>

<success_criteria>
- Provenance helpers work with Polars LazyFrames
- Four layer classes exist with correct responsibilities
- PipelineEngine orchestrates data flow
- Transition logs are saved to catalog/lineage/
- All code uses lazy evaluation (scan_parquet not read_parquet)
</success_criteria>

<output>
After completion, create `.planning/phases/01-pipeline-infrastructure/01-02-SUMMARY.md`
</output>
