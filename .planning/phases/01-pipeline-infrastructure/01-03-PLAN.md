---
phase: 01-pipeline-infrastructure
plan: 03
type: execute
wave: 3
depends_on: ["01-02"]
files_modified:
  - src/jobforge/pipeline/catalog.py
  - src/jobforge/pipeline/query.py
  - tests/test_pipeline_e2e.py
  - tests/fixtures/sample_noc.csv
autonomous: true

must_haves:
  truths:
    - "Table metadata is automatically generated and saved after each layer transition"
    - "Lineage logs are queryable (can find all transitions for a batch)"
    - "Gold layer is queryable via DuckDB SQL"
    - "End-to-end pipeline works: source file goes in, gold parquet comes out with provenance"
  artifacts:
    - path: "src/jobforge/pipeline/catalog.py"
      provides: "Metadata catalog management"
      exports: ["CatalogManager", "generate_table_metadata"]
    - path: "src/jobforge/pipeline/query.py"
      provides: "DuckDB query interface for gold layer"
      exports: ["GoldQueryEngine"]
    - path: "tests/test_pipeline_e2e.py"
      provides: "End-to-end pipeline verification"
      min_lines: 50
  key_links:
    - from: "src/jobforge/pipeline/catalog.py"
      to: "data/catalog/tables/"
      via: "writes TableMetadata JSON"
      pattern: "catalog/tables"
    - from: "src/jobforge/pipeline/query.py"
      to: "data/gold/"
      via: "DuckDB queries parquet files"
      pattern: "duckdb.*gold"
    - from: "tests/test_pipeline_e2e.py"
      to: "src/jobforge/pipeline/engine.py"
      via: "exercises full pipeline"
      pattern: "PipelineEngine.*run_full_pipeline"
---

<objective>
Create metadata catalog management, DuckDB query interface, and verify end-to-end pipeline with real test data.

Purpose: Complete the pipeline infrastructure by adding queryability (DuckDB) and observability (catalog). Then prove it all works with an end-to-end test. This plan delivers the Phase 1 success criteria: "Gold layer output is queryable via DuckDB SQL" and "Layer transitions are logged and queryable."

Output: Working catalog system, DuckDB query interface, and passing end-to-end tests that verify a source file flows through all four layers with provenance.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/01-pipeline-infrastructure/01-RESEARCH.md
@.planning/phases/01-pipeline-infrastructure/01-02-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create Catalog Manager</name>
  <files>src/jobforge/pipeline/catalog.py</files>
  <action>
Create catalog.py with functions for managing the metadata catalog per DAMA DMBOK Chapter 5.

Functions/classes to implement:

1. generate_table_metadata(parquet_path: Path, layer: str, domain: str, description: str = "", business_purpose: str = "", upstream_tables: list[str] = None) -> TableMetadata
   - Reads parquet file to extract schema info
   - Computes row_count, column_count, file_size_bytes
   - Generates ColumnMetadata for each column (data_type from parquet, nullable from schema)
   - Returns populated TableMetadata model
   - Uses pl.scan_parquet() for lazy schema inspection

2. CatalogManager class:
   - __init__(self, config: PipelineConfig)
   - save_table_metadata(self, metadata: TableMetadata) -> Path
     - Writes to catalog/tables/{table_name}.json
     - Uses metadata.model_dump_json(indent=2)
     - Returns path to saved file

   - load_table_metadata(self, table_name: str) -> TableMetadata
     - Reads from catalog/tables/{table_name}.json
     - Returns TableMetadata model

   - list_tables(self, layer: str = None) -> list[str]
     - Lists all tables in catalog
     - Optional filter by layer

   - get_lineage_logs(self, batch_id: str = None) -> list[LayerTransitionLog]
     - If batch_id provided: returns logs for that batch
     - If None: returns all lineage logs
     - Reads from catalog/lineage/*.json

3. Utility function: update_catalog_on_transition(catalog: CatalogManager, transition_log: LayerTransitionLog, target_path: Path, domain: str)
   - Called after each layer transition
   - Generates metadata for the new file
   - Saves to catalog
   - Links to upstream via transition_log.source_files

Use Pydantic model_validate_json() for loading.
  </action>
  <verify>
Run: python -c "
from jobforge.pipeline.catalog import CatalogManager, generate_table_metadata
from jobforge.pipeline.config import PipelineConfig
import polars as pl
from pathlib import Path

# Create a test parquet file
test_df = pl.DataFrame({'id': [1,2,3], 'name': ['a','b','c']})
test_path = Path('data/gold/test_table.parquet')
test_path.parent.mkdir(parents=True, exist_ok=True)
test_df.write_parquet(test_path)

# Test metadata generation
metadata = generate_table_metadata(test_path, 'gold', 'test', 'Test table', 'Testing')
print(f'Table: {metadata.table_name}, Rows: {metadata.row_count}, Cols: {metadata.column_count}')

# Cleanup
test_path.unlink()
print('Catalog functions work correctly')
"
  </verify>
  <done>CatalogManager saves and loads table metadata; lineage logs are queryable</done>
</task>

<task type="auto">
  <name>Task 2: Create DuckDB Query Interface</name>
  <files>src/jobforge/pipeline/query.py</files>
  <action>
Create query.py with GoldQueryEngine class for SQL queries on gold layer data.

GoldQueryEngine class:

Constructor:
- __init__(self, config: PipelineConfig = None)
- Creates DuckDB connection (in-memory by default)
- Stores config for locating gold files
- Can optionally persist to disk: __init__(self, config, db_path: Path = None)

Methods:

1. register_gold_tables(self) -> list[str]
   - Scans gold/ directory for parquet files
   - Registers each as a DuckDB view (filename without extension becomes table name)
   - Uses: CREATE VIEW {name} AS SELECT * FROM '{path}'
   - Returns list of registered table names
   - Call this after pipeline runs to make data queryable

2. query(self, sql: str) -> pl.DataFrame
   - Executes SQL against registered tables
   - Returns result as Polars DataFrame
   - Uses conn.execute(sql).pl()

3. query_with_provenance(self, sql: str) -> pl.DataFrame
   - Ensures provenance columns (_source_file, _batch_id, _ingested_at) are included
   - Useful for audit queries

4. get_lineage(self, table_name: str) -> dict
   - Returns provenance summary for a gold table
   - Groups by _source_file, counts rows, finds min/max _ingested_at
   - Returns dict with lineage info

5. list_tables(self) -> list[str]
   - Returns list of registered table names

Important: Use a persistent connection within the class instance. Do NOT create new connections per query.
  </action>
  <verify>
Run: python -c "
from jobforge.pipeline.query import GoldQueryEngine
from jobforge.pipeline.config import PipelineConfig
import polars as pl
from pathlib import Path

# Create a test gold parquet
config = PipelineConfig()
gold_path = config.gold_path() / 'test_query.parquet'
gold_path.parent.mkdir(parents=True, exist_ok=True)
pl.DataFrame({'id': [1,2,3], 'value': [10,20,30], '_source_file': ['a.csv']*3, '_batch_id': ['123']*3, '_layer': ['gold']*3}).write_parquet(gold_path)

# Test query engine
engine = GoldQueryEngine(config)
tables = engine.register_gold_tables()
print(f'Registered tables: {tables}')

result = engine.query('SELECT * FROM test_query WHERE id > 1')
print(f'Query returned {len(result)} rows')

# Cleanup
gold_path.unlink()
print('GoldQueryEngine works correctly')
"
  </verify>
  <done>GoldQueryEngine registers gold tables and executes SQL queries returning Polars DataFrames</done>
</task>

<task type="auto">
  <name>Task 3: Create End-to-End Test</name>
  <files>
    tests/__init__.py
    tests/fixtures/sample_noc.csv
    tests/test_pipeline_e2e.py
  </files>
  <action>
Create end-to-end test that verifies ALL Phase 1 success criteria:

1. Create tests/__init__.py (empty file for test discovery)

2. Create tests/fixtures/sample_noc.csv with sample data:
```csv
noc_code,title,description
11100,Legislators,Members of legislative bodies
11101,Financial Managers,Plan direct and coordinate financial activities
11102,Human Resources Managers,Plan organize direct control and evaluate operations
```
(At least 5-10 rows of realistic NOC-like data)

3. Create tests/test_pipeline_e2e.py with pytest tests:

test_full_pipeline_flow():
- Create PipelineEngine
- Run run_full_pipeline() with sample_noc.csv
- Assert staged file exists with provenance columns
- Assert bronze file exists with provenance columns
- Assert silver file exists with provenance columns
- Assert gold file exists with provenance columns
- Assert all parquet files have _source_file, _ingested_at, _batch_id, _layer columns

test_layer_transitions_logged():
- Run pipeline
- Use CatalogManager to get lineage logs
- Assert log exists for each transition (staged->bronze, bronze->silver, silver->gold)
- Assert logs contain correct row counts

test_gold_queryable_via_duckdb():
- Run pipeline
- Create GoldQueryEngine
- Register gold tables
- Execute: SELECT COUNT(*) FROM {table}
- Assert count matches expected
- Execute: SELECT DISTINCT _source_file FROM {table}
- Assert returns the original source filename

test_provenance_columns_present():
- Run pipeline
- Read gold parquet with Polars
- Assert columns include: _source_file, _ingested_at, _batch_id, _layer
- Assert _layer value is 'gold'
- Assert _source_file contains original filename

Use pytest fixtures for config and temp directories.
Clean up test data after each test (use tmp_path fixture).
  </action>
  <verify>
Run: pytest tests/test_pipeline_e2e.py -v
All tests pass.
  </verify>
  <done>End-to-end tests verify all Phase 1 success criteria pass</done>
</task>

</tasks>

<verification>
Phase 1 Success Criteria Verification:

1. "Pipeline can accept a source file and write it through all four layers"
   - test_full_pipeline_flow() verifies this

2. "Each layer produces parquet files with provenance columns"
   - test_provenance_columns_present() verifies this

3. "Layer transitions are logged and queryable"
   - test_layer_transitions_logged() verifies this

4. "Gold layer output is queryable via DuckDB SQL"
   - test_gold_queryable_via_duckdb() verifies this

Run: pytest tests/ -v --tb=short
All tests must pass.
</verification>

<success_criteria>
- CatalogManager saves and retrieves table metadata
- GoldQueryEngine queries gold parquet via DuckDB SQL
- End-to-end test passes with sample data
- All four Phase 1 success criteria are verified by tests
- Pipeline is ready for Phase 2 (Data Ingestion)
</success_criteria>

<output>
After completion, create `.planning/phases/01-pipeline-infrastructure/01-03-SUMMARY.md`
</output>
