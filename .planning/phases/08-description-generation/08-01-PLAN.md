---
phase: 08-description-generation
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/jobforge/description/__init__.py
  - src/jobforge/description/models.py
  - src/jobforge/description/sources.py
  - tests/test_description_models.py
autonomous: true

must_haves:
  truths:
    - "DescriptionProvenance model captures source type, confidence, timestamp, and resolution method"
    - "GeneratedDescription model holds entity type, entity ID, description text, and provenance"
    - "Lead statement lookup returns text + provenance for OASIS profile codes"
    - "Source cascade checks authoritative first, returns LLM marker when no authoritative source"
  artifacts:
    - path: "src/jobforge/description/__init__.py"
      provides: "Package exports for description models and sources"
      min_lines: 10
    - path: "src/jobforge/description/models.py"
      provides: "DescriptionProvenance, GeneratedDescription, DescriptionSource enum"
      exports: ["DescriptionProvenance", "GeneratedDescription", "DescriptionSource"]
    - path: "src/jobforge/description/sources.py"
      provides: "Lead statement lookup, source cascade logic"
      exports: ["load_lead_statements", "get_lead_statement_for_oasis", "determine_source_type"]
    - path: "tests/test_description_models.py"
      provides: "Model validation and serialization tests"
      min_lines: 50
  key_links:
    - from: "src/jobforge/description/sources.py"
      to: "data/gold/element_lead_statement.parquet"
      via: "polars scan_parquet"
      pattern: "scan_parquet.*element_lead_statement"
    - from: "src/jobforge/description/models.py"
      to: "src/jobforge/external/models.py"
      via: "SourcePrecedence import"
      pattern: "from jobforge\\.external\\.models import SourcePrecedence"
---

<objective>
Create description data models and source cascade logic for Phase 8 description generation.

Purpose: Establish Pydantic models for description provenance and the foundational source lookup functions that the DescriptionGenerationService will orchestrate. This separates data modeling from service logic for cleaner architecture.

Output: `jobforge.description` package with models, source cascade logic, and comprehensive tests.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/08-description-generation/08-CONTEXT.md
@.planning/phases/08-description-generation/08-RESEARCH.md

# Existing infrastructure to reuse
@src/jobforge/external/models.py
@src/jobforge/external/llm/client.py
@src/jobforge/imputation/resolution.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create description models</name>
  <files>
    src/jobforge/description/__init__.py
    src/jobforge/description/models.py
  </files>
  <action>
Create `src/jobforge/description/` package with Pydantic models for description generation.

**models.py:**

1. Create `DescriptionSource` enum (str, Enum):
   - AUTHORITATIVE = "authoritative" - NOC lead statement
   - ONET = "onet" - O*NET occupation description (future)
   - LLM = "llm" - GPT-synthesized

2. Create `DescriptionProvenance` Pydantic model:
   ```python
   class DescriptionProvenance(BaseModel):
       source_type: DescriptionSource
       confidence: float = Field(ge=0.0, le=1.0)
       timestamp: datetime
       model_version: str | None = None  # For LLM (e.g., "gpt-4o-2024-08-06")
       input_context: str | None = None  # Boundary words/prompts used
       resolution_method: str | None = None  # How entity was resolved (e.g., "DIRECT_MATCH")
       matched_text: str | None = None  # What text was matched against

       @property
       def precedence(self) -> int:
           """Get source precedence for conflict resolution."""
           # Import SourcePrecedence from external.models
           # Map source_type to precedence value
   ```

3. Create `GeneratedDescription` Pydantic model:
   ```python
   class GeneratedDescription(BaseModel):
       entity_type: Literal["title", "family", "function"]
       entity_id: str  # jt_id for titles, family name, function name
       entity_name: str  # Human-readable name
       description: str
       provenance: DescriptionProvenance

       model_config = ConfigDict(
           json_encoders={datetime: lambda v: v.isoformat()},
       )
   ```

**__init__.py:**
Export all models: DescriptionSource, DescriptionProvenance, GeneratedDescription

Follow existing patterns from `jobforge.external.models` for datetime handling and model configuration.
  </action>
  <verify>
```bash
python -c "from jobforge.description import DescriptionSource, DescriptionProvenance, GeneratedDescription; print('Models import OK')"
python -c "from jobforge.description.models import DescriptionSource; print(DescriptionSource.AUTHORITATIVE.value)"
```
  </verify>
  <done>
DescriptionSource enum has AUTHORITATIVE/ONET/LLM values, DescriptionProvenance captures all provenance fields with precedence property, GeneratedDescription holds entity + description + provenance with proper serialization.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create source cascade logic</name>
  <files>src/jobforge/description/sources.py</files>
  <action>
Create source lookup and cascade determination functions.

**sources.py:**

1. Create cached lead statement loader:
   ```python
   @lru_cache(maxsize=1)
   def load_lead_statements(gold_path: Path) -> dict[str, str]:
       """Load lead statements indexed by oasis_profile_code.

       Returns dict mapping oasis_profile_code -> lead statement text.
       Cached for efficiency across multiple lookups.
       """
       # Use polars scan_parquet for lazy loading
       # Column name is "Lead statement" (with space)
   ```

2. Create single lead statement lookup:
   ```python
   def get_lead_statement_for_oasis(
       oasis_profile_code: str,
       gold_path: Path | None = None,
   ) -> str | None:
       """Get lead statement text for an OASIS profile code.

       Args:
           oasis_profile_code: The OASIS profile code (e.g., "21231.00")
           gold_path: Path to gold layer. Defaults to PipelineConfig().gold_path()

       Returns:
           Lead statement text if found, None otherwise.
       """
   ```

3. Create source type determination:
   ```python
   def determine_source_type(
       oasis_profile_code: str | None,
       has_lead_statement: bool,
   ) -> DescriptionSource:
       """Determine which source should provide the description.

       Source cascade per CONTEXT.md:
       1. If oasis_profile_code exists and has lead statement -> AUTHORITATIVE
       2. Otherwise -> LLM (O*NET descriptions are future work)

       Args:
           oasis_profile_code: Resolved OASIS code from resolution (can be None)
           has_lead_statement: Whether authoritative lead statement was found

       Returns:
           DescriptionSource indicating which source to use.
       """
   ```

4. Create cache clear function:
   ```python
   def clear_lead_statement_cache() -> None:
       """Clear the lead statement cache. Call when source data changes."""
       load_lead_statements.cache_clear()
   ```

Update `__init__.py` to export: load_lead_statements, get_lead_statement_for_oasis, determine_source_type, clear_lead_statement_cache
  </action>
  <verify>
```bash
python -c "from jobforge.description import load_lead_statements, get_lead_statement_for_oasis, determine_source_type; print('Sources import OK')"
python -c "from jobforge.description.sources import load_lead_statements; from jobforge.pipeline.config import PipelineConfig; ls = load_lead_statements(PipelineConfig().gold_path()); print(f'Loaded {len(ls)} lead statements')"
```
  </verify>
  <done>
Lead statement loader returns ~900 statements indexed by OASIS code, single lookup returns text or None, source type determination follows cascade logic (authoritative first, then LLM).
  </done>
</task>

<task type="auto">
  <name>Task 3: Add model and source tests</name>
  <files>tests/test_description_models.py</files>
  <action>
Create comprehensive tests for description models and source logic.

**test_description_models.py:**

Model Tests:
1. Test DescriptionSource enum values (authoritative, onet, llm)
2. Test DescriptionProvenance creation with all fields
3. Test DescriptionProvenance.precedence property returns correct SourcePrecedence values
4. Test DescriptionProvenance with minimal fields (only required)
5. Test GeneratedDescription creation for title entity
6. Test GeneratedDescription creation for family entity
7. Test GeneratedDescription creation for function entity
8. Test GeneratedDescription JSON serialization (datetime handling)
9. Test confidence bounds validation (0.0-1.0)

Source Tests:
10. Test load_lead_statements returns dict with ~900 entries
11. Test get_lead_statement_for_oasis returns text for valid code
12. Test get_lead_statement_for_oasis returns None for invalid code
13. Test determine_source_type returns AUTHORITATIVE when has_lead_statement=True
14. Test determine_source_type returns LLM when has_lead_statement=False
15. Test determine_source_type returns LLM when oasis_profile_code=None
16. Test clear_lead_statement_cache clears the cache

Use pytest fixtures for common setup. Follow existing test patterns from tests/test_llm_imputation.py.
  </action>
  <verify>
```bash
pytest tests/test_description_models.py -v
pytest tests/ -v --tb=short
```
  </verify>
  <done>
All model tests pass (enum values, provenance creation, precedence mapping, description serialization), all source tests pass (lead statement loading, lookup, cascade determination), full test suite remains green.
  </done>
</task>

</tasks>

<verification>
After all tasks complete:

1. Package structure:
```bash
python -c "from jobforge.description import DescriptionSource, DescriptionProvenance, GeneratedDescription, load_lead_statements, get_lead_statement_for_oasis, determine_source_type"
```

2. Model validation:
```bash
python -c "
from jobforge.description import DescriptionSource, DescriptionProvenance, GeneratedDescription
from datetime import datetime, timezone
from jobforge.external.models import SourcePrecedence

# Test provenance precedence mapping
prov = DescriptionProvenance(
    source_type=DescriptionSource.AUTHORITATIVE,
    confidence=1.0,
    timestamp=datetime.now(timezone.utc),
)
assert prov.precedence == SourcePrecedence.AUTHORITATIVE, 'Precedence mapping failed'

# Test description creation
desc = GeneratedDescription(
    entity_type='title',
    entity_id='123',
    entity_name='Data Analyst',
    description='Test description',
    provenance=prov,
)
print(f'Created: {desc.entity_name} ({desc.entity_type})')
print(f'Source: {desc.provenance.source_type.value}, Confidence: {desc.provenance.confidence}')
"
```

3. Source cascade:
```bash
python -c "
from jobforge.description import determine_source_type, DescriptionSource
assert determine_source_type('21231.00', True) == DescriptionSource.AUTHORITATIVE
assert determine_source_type('21231.00', False) == DescriptionSource.LLM
assert determine_source_type(None, False) == DescriptionSource.LLM
print('Source cascade logic verified')
"
```

4. Full test suite:
```bash
pytest tests/ -v --tb=short
```
</verification>

<success_criteria>
- [ ] `jobforge.description` package exists with models.py and sources.py
- [ ] DescriptionSource enum has AUTHORITATIVE/ONET/LLM values
- [ ] DescriptionProvenance.precedence maps to SourcePrecedence correctly
- [ ] GeneratedDescription holds entity_type (title/family/function), entity_id, entity_name, description, provenance
- [ ] load_lead_statements returns ~900 entries from element_lead_statement.parquet
- [ ] get_lead_statement_for_oasis returns text or None appropriately
- [ ] determine_source_type follows cascade: authoritative first, then LLM
- [ ] All new tests pass
- [ ] Full test suite remains green
</success_criteria>

<output>
After completion, create `.planning/phases/08-description-generation/08-01-SUMMARY.md`
</output>
