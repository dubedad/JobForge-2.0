---
phase: 03-wiq-semantic-model
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/jobforge/semantic/__init__.py
  - src/jobforge/semantic/models.py
  - src/jobforge/semantic/introspect.py
autonomous: true

must_haves:
  truths:
    - "Semantic model definitions can represent Power BI relationship requirements"
    - "Parquet file schemas can be introspected programmatically"
    - "Column metadata includes data types needed for FK/PK validation"
  artifacts:
    - path: "src/jobforge/semantic/__init__.py"
      provides: "Module exports"
      min_lines: 5
    - path: "src/jobforge/semantic/models.py"
      provides: "Pydantic models for Table, Column, Relationship, Schema"
      exports: ["TableType", "Cardinality", "CrossFilterDirection", "Column", "Table", "Relationship", "SemanticSchema"]
      min_lines: 80
    - path: "src/jobforge/semantic/introspect.py"
      provides: "Functions to introspect parquet schemas via DuckDB"
      exports: ["introspect_parquet_schema", "introspect_all_gold_tables"]
      min_lines: 40
  key_links:
    - from: "src/jobforge/semantic/introspect.py"
      to: "duckdb"
      via: "DuckDB DESCRIBE query"
      pattern: "duckdb\\.connect|DESCRIBE"
    - from: "src/jobforge/semantic/models.py"
      to: "pydantic"
      via: "BaseModel inheritance"
      pattern: "class.*BaseModel"
---

<objective>
Create Pydantic models for semantic schema definition and DuckDB-based introspection utilities.

Purpose: Establish the data structures that will represent the WiQ semantic model and the tooling to extract actual column schemas from gold layer parquet files. This enables Plan 02 to define concrete relationships with validated column names.

Output: `src/jobforge/semantic/` module with models.py (Pydantic schema definitions) and introspect.py (parquet schema extraction via DuckDB).
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/ROADMAP.md
@.planning/phases/03-wiq-semantic-model/03-RESEARCH.md

# Existing codebase (in JobForge 2.0 directory)
@src/jobforge/pipeline/models.py       # Existing Pydantic patterns
@src/jobforge/pipeline/query.py        # Existing DuckDB patterns (GoldQueryEngine)
@src/jobforge/pipeline/config.py       # PipelineConfig for paths
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create Pydantic models for semantic schema</name>
  <files>src/jobforge/semantic/__init__.py, src/jobforge/semantic/models.py</files>
  <action>
Create the semantic module with Pydantic models for Power BI compatible schema definitions.

In `src/jobforge/semantic/models.py`:

1. Define enums:
   - `TableType(str, Enum)`: DIMENSION, FACT, ATTRIBUTE
   - `Cardinality(str, Enum)`: ONE_TO_MANY = "1:*", MANY_TO_ONE = "*:1", ONE_TO_ONE = "1:1", MANY_TO_MANY = "*:*"
   - `CrossFilterDirection(str, Enum)`: SINGLE = "Single", BOTH = "Both"

2. Define Column model:
   - name: str
   - data_type: str (from DuckDB DESCRIBE)
   - is_primary_key: bool = False
   - is_foreign_key: bool = False
   - references_table: str | None = None
   - references_column: str | None = None

3. Define Table model:
   - name: str
   - table_type: TableType
   - columns: list[Column]
   - primary_key: str | None = None
   - description: str = ""

4. Define Relationship model:
   - from_table: str
   - from_column: str
   - to_table: str
   - to_column: str
   - cardinality: Cardinality
   - cross_filter_direction: CrossFilterDirection = CrossFilterDirection.SINGLE
   - is_active: bool = True
   - Use model_config with use_enum_values=True for JSON serialization

5. Define SemanticSchema model (root):
   - name: str
   - tables: list[Table]
   - relationships: list[Relationship]
   - validated: bool = False
   - validation_date: datetime | None = None
   - Add helper methods: get_dimension_tables(), get_relationships_for_table(table_name)
   - Add to_json() method using model_dump_json(indent=2)

In `src/jobforge/semantic/__init__.py`:
- Export all public classes: TableType, Cardinality, CrossFilterDirection, Column, Table, Relationship, SemanticSchema

Follow existing Pydantic patterns from pipeline/models.py (ConfigDict, Field descriptions).
  </action>
  <verify>
    Run: `python -c "from jobforge.semantic import SemanticSchema, Table, Relationship, Cardinality; print('Imports OK')"`
    Expected: "Imports OK" printed without errors
  </verify>
  <done>
    All Pydantic models import cleanly and can be instantiated with valid data
  </done>
</task>

<task type="auto">
  <name>Task 2: Create parquet introspection utilities</name>
  <files>src/jobforge/semantic/introspect.py</files>
  <action>
Create DuckDB-based utilities to introspect parquet file schemas.

In `src/jobforge/semantic/introspect.py`:

1. Import dependencies:
   - duckdb
   - pathlib.Path
   - from jobforge.semantic.models import Column, Table, TableType
   - from jobforge.pipeline.config import PipelineConfig

2. Define `introspect_parquet_schema(parquet_path: Path) -> list[Column]`:
   - Connect to DuckDB in-memory
   - Execute: `DESCRIBE SELECT * FROM '{parquet_path}'`
   - Parse result rows: (column_name, column_type, null, key, default, extra)
   - Return list of Column objects with name and data_type populated
   - Handle file not found gracefully (raise ValueError with clear message)

3. Define `classify_table_type(table_name: str) -> TableType`:
   - If name starts with "dim_": return DIMENSION
   - If name starts with "cops_": return FACT
   - If name starts with "element_" or "oasis_": return ATTRIBUTE
   - If name == "job_architecture": return ATTRIBUTE
   - Default: return ATTRIBUTE

4. Define `introspect_all_gold_tables(config: PipelineConfig | None = None) -> list[Table]`:
   - Use config or create default PipelineConfig()
   - Get all *.parquet files from config.gold_path()
   - For each file:
     - table_name = file.stem (lowercase normalized)
     - columns = introspect_parquet_schema(file)
     - table_type = classify_table_type(table_name)
     - Create Table object
   - Return sorted list of Table objects by name
   - Skip provenance columns (_source_file, _batch_id, etc.) from schema - they're system columns

Do NOT include any main block or test code. This is a library module.
  </action>
  <verify>
    Run: `python -c "from jobforge.semantic.introspect import introspect_all_gold_tables; tables = introspect_all_gold_tables(); print(f'{len(tables)} tables found')"`
    Expected: "25 tables found" (or actual count from gold directory)
  </verify>
  <done>
    Introspection utilities return Table objects with correct column schemas from all gold parquet files
  </done>
</task>

</tasks>

<verification>
1. Module imports work: `python -c "from jobforge.semantic import *"`
2. Pydantic models serialize to JSON: `python -c "from jobforge.semantic import SemanticSchema; s = SemanticSchema(name='test', tables=[], relationships=[]); print(s.to_json())"`
3. Introspection finds all gold tables: `python -c "from jobforge.semantic.introspect import introspect_all_gold_tables; print([t.name for t in introspect_all_gold_tables()])"`
</verification>

<success_criteria>
1. `src/jobforge/semantic/` module exists with models.py and introspect.py
2. All Pydantic models (Table, Column, Relationship, SemanticSchema) import without error
3. SemanticSchema.to_json() produces valid JSON with enum values as strings
4. introspect_all_gold_tables() returns Table objects for all 25 gold parquet files
5. Each Table has correct columns with data types from DuckDB introspection
</success_criteria>

<output>
After completion, create `.planning/phases/03-wiq-semantic-model/03-01-SUMMARY.md`
</output>
