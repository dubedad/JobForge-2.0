---
phase: 18-data-quality-dashboard
plan: 02
type: tdd
wave: 1
depends_on: []
files_modified:
  - src/jobforge/quality/metrics.py
  - src/jobforge/quality/validators.py
  - tests/quality/test_metrics.py
  - tests/quality/test_validators.py
autonomous: true

must_haves:
  truths:
    - "Completeness metric returns NULL rate percentage per column"
    - "Timeliness metric returns score based on days since refresh"
    - "Coherence metric validates FK integrity for bridge tables"
    - "Accuracy metric validates NOC code patterns"
  artifacts:
    - path: "src/jobforge/quality/metrics.py"
      provides: "Completeness, timeliness, coherence, consistency calculations"
      exports: ["calculate_completeness", "calculate_timeliness", "calculate_coherence", "calculate_consistency"]
    - path: "src/jobforge/quality/validators.py"
      provides: "Accuracy validation rules for NOC codes and standard columns"
      exports: ["validate_noc_codes", "validate_patterns", "AccuracyValidator"]
  key_links:
    - from: "src/jobforge/quality/metrics.py"
      to: "data/gold/*.parquet"
      via: "DuckDB queries via polars"
      pattern: "duckdb\\.connect|pl\\.read_parquet"
    - from: "src/jobforge/quality/validators.py"
      to: "data/catalog/tables/*.json"
      via: "Catalog metadata for FK references"
      pattern: "read_json|catalog"
---

<objective>
Implement core quality metric calculations: completeness (DQ-02), timeliness (DQ-03), coherence (DQ-07), and accuracy validation (DQ-08).

Purpose: These are the automatable GC DQMF dimensions that form the foundation of quality scoring. Each metric returns a 0-100 score that feeds into the weighted overall calculation.

Output: Metrics module with 4 dimension calculators, validators for accuracy patterns, and 25+ tests.
</objective>

<execution_context>
@C:\Users\Administrator\.claude/get-shit-done/workflows/execute-plan.md
@C:\Users\Administrator\.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md

@.planning/phases/18-data-quality-dashboard/18-CONTEXT.md
@.planning/phases/18-data-quality-dashboard/18-RESEARCH.md

# Existing data layer patterns
@src/jobforge/pipeline/config.py
@data/catalog/tables/dim_noc.json
</context>

<feature>
  <name>Core Quality Metrics (TDD)</name>
  <files>
    src/jobforge/quality/metrics.py
    src/jobforge/quality/validators.py
    tests/quality/test_metrics.py
    tests/quality/test_validators.py
  </files>
  <behavior>
    COMPLETENESS (DQ-02):
    - Input: table_name, gold_path
    - Process: Count NULLs per column, calculate fill rate
    - Output: 0-100 score (percentage of non-NULL cells)
    - Test cases:
      - Table with no NULLs -> 100.0
      - Table with all NULLs -> 0.0
      - Table with 50% NULLs -> 50.0
      - Empty table -> 100.0 (no cells to be NULL)

    TIMELINESS (DQ-03):
    - Input: table_name, catalog_path
    - Process: Read updated_at from catalog, calculate days since refresh
    - Output: 0-100 score using decay function
    - Thresholds (from RESEARCH.md):
      - <=7 days: 100.0
      - 8-30 days: Linear decay 100->50
      - 31-90 days: Linear decay 50->0
      - >90 days: 0.0
    - Test cases:
      - Updated today -> 100.0
      - Updated 7 days ago -> 100.0
      - Updated 30 days ago -> 50.0
      - Updated 90 days ago -> 0.0
      - Table not in catalog -> 0.0

    COHERENCE (DQ-07):
    - Input: table_name, schema_info, gold_path
    - Process: LEFT JOIN FK columns to referenced tables, count orphans
    - Output: 0-100 score (percentage of valid FK references)
    - Test cases:
      - All FKs valid -> 100.0
      - 50% orphaned FKs -> 50.0
      - Table with no FKs -> 100.0 (nothing to validate)
      - All FKs orphaned -> 0.0

    CONSISTENCY (part of automatable):
    - Input: table_name, gold_path
    - Process: Check for duplicate PKs, contradictory values
    - Output: 0-100 score
    - Test cases:
      - No duplicates -> 100.0
      - 10% duplicate PKs -> 90.0

    ACCURACY (DQ-08):
    - Input: table_name, gold_path, validation_rules
    - Process: Apply pattern validation (NOC codes = 5 digits)
    - Output: 0-100 score (percentage passing pattern validation)
    - NOC code pattern: ^\d{5}$ (NOC 2026: 5-digit codes)
    - Test cases:
      - All valid NOC codes -> 100.0
      - 80% valid patterns -> 80.0
      - No NOC columns -> 100.0 (nothing to validate)
  </behavior>
  <implementation>
    RED: Write failing tests for each metric calculation
    GREEN: Implement metrics.py with calculate_* functions using Polars/DuckDB
    GREEN: Implement validators.py with AccuracyValidator class
    REFACTOR: Extract common DuckDB connection handling, add type hints
  </implementation>
</feature>

<verification>
```bash
# Run metrics tests
pytest tests/quality/test_metrics.py tests/quality/test_validators.py -v

# Test completeness on real table
python -c "
from jobforge.quality.metrics import calculate_completeness
score = calculate_completeness('dim_noc', 'data/gold')
print(f'dim_noc completeness: {score:.1f}%')
"

# Test timeliness (requires catalog)
python -c "
from jobforge.quality.metrics import calculate_timeliness
score = calculate_timeliness('dim_noc', 'data/catalog/tables')
print(f'dim_noc timeliness: {score:.1f}%')
"
```
</verification>

<success_criteria>
- [ ] calculate_completeness returns NULL rate percentage (0-100)
- [ ] calculate_timeliness uses catalog updated_at with decay function
- [ ] calculate_coherence validates FK integrity via LEFT JOIN
- [ ] calculate_consistency checks for duplicate PKs
- [ ] AccuracyValidator validates NOC code patterns (5 digits)
- [ ] All functions handle edge cases (empty tables, missing catalog, no FKs)
- [ ] 25+ tests passing for metrics and validators
</success_criteria>

<output>
After completion, create `.planning/phases/18-data-quality-dashboard/18-02-SUMMARY.md`
</output>
